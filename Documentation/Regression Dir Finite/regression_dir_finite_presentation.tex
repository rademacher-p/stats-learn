\documentclass[aspectratio=169]{beamer}

\usetheme{NRLpresentationGWU}
\usepackage{NRLcolors}

%\titlegraphic{\includegraphics[width=0.9\textwidth]{$HOME/Documents/TAVEX/gmt/TAVEX-overview-3}}% optional

\setbeamerfont{footnote}{size=\tiny}
%\beamertemplatenavigationsymbolsempty


\usepackage[style=authortitle-comp,maxnames=1]{biblatex} % PGR: backend=biber???
\addbibresource{../References/PhD_refs.bib}

\usepackage{graphicx}
\graphicspath{{../Figures/}}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{upgreek}
\usepackage[retainorgcmds]{IEEEtrantools}

%\usepackage{hyperref}
%\usepackage[capitalize]{cleveref}

\usepackage{relsize}

\usepackage{bm}
\usepackage{hhline}
\usepackage{xcolor}


\setlength{\fboxsep}{3pt}
\setlength{\fboxrule}{2pt}


\usepackage{mathfont_shortcuts}
\usepackage{PhDmath}


\DeclareMathOperator{\nbarrm}{\bar{\mathrm{n}}}


\newcommand{\indep}{\perp \!\!\! \perp}
%\DeclareMathOperator*{\indep}{\perp \!\!\! \perp}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}




\title{Bayesian Learning for Regression using Dirichlet Prior Distributions of Varying Localization}
%\subtitle{Work Supported by the U.S. Office of Naval Research}% optional

\author[Rademacher \& Doroslova\v{c}ki]{Paul Rademacher\inst{1} \and Milo\v{s} Doroslova\v{c}ki\inst{2}}
\institute[NRL,~GWU] 
{
  \inst{1}
  U.S. Naval Research Laboratory\\Radar Division
  \and
  \inst{2}
  The George Washington University\\Department of Electrical and Computer Engineering
}


\date{July 11, 2021}


%\NRLcredit{Work Supported by the U.S. Office of Naval Research}% optional
%\NRLmark{FOR OFFICIAL USE ONLY}% optional
%\NRLpatents{Example Patents\\7,749,438 and 7,754,145}% optional

\NRLdist{DISTRIBUTION A. Approved for public release: distribution unlimited.}
%\NRLfoot{DISTRIBUTION A. Approved for public release: distribution unlimited.}


\begin{document}


\begin{frame}
\titlepage
\end{frame}


\section{Introduction} 

\begin{frame}
\frametitle{Bayesian Learning}
%\frametitle{Introduction}
%\framesubtitle{Part I}

Bayesian approaches to statistical learning attempt to make better decisions by exploiting \alert{prior knowledge} regarding the data-generating distribution:

\vspace{1em}

\begin{columns}[T]

\begin{column}{.5\linewidth}

\centering
\large \textbf{\underline{Informative}} \normalsize
\vspace{0.5em}
\begin{itemize}
\item If the prior is localized around the true data-generating model, low-risk decisions can be made even with limited training data
\item Priors that assign low weighting to the true model may not be able to realize satisfactory performance 
\end{itemize}


\end{column}

\vrule

\begin{column}{.5\linewidth}

\centering
\large \textbf{\underline{Non-Informative}} \normalsize
\vspace{0.5em}
\begin{itemize}
\item Learners designed with minimally localized priors respond strongly to training data, avoiding the drawbacks of misinformed prior knowledge
\item If the data volume is limited, high variance "overfit" solutions can occur
%\item Learners designed with approximately uniform priors will not perform as well as those made with well-selected informative priors
%\item Avoid high risk inherent to learners made by mismatched informative priors
\end{itemize}



\end{column}

\end{columns}

\end{frame}




\begin{frame}
\frametitle{The Dirichlet Prior}
%\frametitle{Introduction}
%\framesubtitle{Part II}

Dirichlet prior distributions have a number of desirable properties:
\begin{itemize}
%\item Often, priors are termed non-informative as long as they are approximately uniform over their \underline{limited support}. For example, a parametric regression function might use a high covariance Gaussian vector prior to characterize a subset of probability distributions
\vspace{0.5em}
\item \alert{Full support} over the space of data-generating distributions, guaranteeing \emph{consistent estimation} of the true data model
\vspace{0.5em}
\item They are \alert{conjugate priors} for independent, identically distributed observations \footfullcite{ferguson}, leading to \emph{closed-form} posterior distributions
\vspace{0.5em}
\item \alert{Flexible parameterization} enabling \emph{both} maximally and minimally informative priors and thus a wide range of learning solutions
\end{itemize}

\end{frame}



\section{Data Model and Regression Objective}


\begin{frame}
\frametitle{Data Representation}

\begin{description}
\item[Observable random variable:] $\xrm \in \Xcal \subset \Rbb$
\item[Unobservable random variable:] $\yrm \in \Ycal \subset \Rbb$
\item[Observable training data:] $\Drm \in \Dcal = \{\Ycal \times \Xcal\}^N$
\end{description}

\vspace{0.5em}

Independently, identically distributed according to an \alert{unknown} probability mass function (PMF) 
\begin{equation*}
\uptheta \in \Theta = \left\{ \theta \in {\Rbb_{\geq 0}}^{\Ycal \times \Xcal}: \sum_{y \in \Ycal} \sum_{x \in \Xcal} \theta(y,x) = 1 \right\} \ ,
\end{equation*}
such that $\Prm_{\yrm,\xrm | \uptheta}(y,x | \theta) = \Prm_{\Drm_n | \uptheta}(y,x | \theta) = \theta(y,x)$.

\hrulefill

\vspace{0.5em}
\textit{Alternate Notation}: $\uptheta \Leftrightarrow \big( \upthetam,\upthetac \big)$
\begin{itemize}
\item Marginal model $\upthetam \equiv \sum_{y \in \Ycal} \uptheta(y,\cdot) = \Prm_{\xrm | \uptheta} \equiv \Prm_{\xrm | \upthetam}$
\item Conditional models $\upthetac(x) \equiv \uptheta(\cdot,x) / \upthetam(x) = \Prm_{\yrm | \xrm,\uptheta} \equiv \Prm_{\yrm | \xrm,\upthetac}$ 
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Sufficient Statistic}
\framesubtitle{Transform}

\begin{columns}[c]

\begin{column}{.5\linewidth}

Using the i.i.d. assumption,
\begin{IEEEeqnarray}{C}
\Prm_{\Drm | \uptheta}\big( D | \theta \big) = \left( \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\Psi(y,x;D)} \right)^N \nonumber 
\end{IEEEeqnarray}
where data is represented using $\Psi : \Dcal \mapsto \Uppsi \subset \Theta$, defined as
\begin{equation*}
\Psi(y,x;D) = N^{-1} \sum_{n=1}^N \delta \big[ (y,x),D_n \big] \;.
\end{equation*}
%and the range is
%\begin{equation*}
%\Uppsi = \left\{ \frac{n}{N} \in {\Zbb_{\geq 0}}^{\Ycal \times \Xcal}: \sum_{y \in \Ycal} \sum_{x \in \Xcal} n(y,x) = N \right\}
%\end{equation*}

\end{column}

\vrule
\hspace{0.5ex}
\begin{column}{.5\linewidth}

\begin{itemize}
\item Empirical distribution $\Psi(\Drm)$ is a \alert{sufficient statistic}\footnotemark ~for the model $\uptheta$
\vspace{0.5em}
\item Efficient: $|\Uppsi| = \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} \leq |\Dcal|$
\vspace{1.5em}
\item [$\Rightarrow$] \textbf{Represent data using new random process $\uppsi \equiv \Psi(\Drm) \in \Uppsi$}

%\vspace{0.5em}
%\item \textit{Alternate Notation}: $\uppsi \Leftrightarrow \big( \uppsim,\uppsic \big)$
%\begin{itemize}
%\item Marginal $\uppsim \equiv \sum_{y \in \Ycal} \uppsi(y,\cdot)$
%\item Conditional $\uppsic(\xrm) \equiv \uppsi(\cdot,\xrm) / \uppsim(x)$
%\end{itemize}
\end{itemize}

\end{column}

\end{columns}

\footcitetext{bernardo}
%\footcitetext{feller}

\end{frame}



\begin{frame}
\frametitle{Sufficient Statistic}
\framesubtitle{Distribution}

\begin{itemize}
\item Conditioned on the true model, the data statistic is an "Empirical" random process $\uppsi | \uptheta \sim \Emp(N,\uptheta)$
\begin{itemize}
\item Equivalent to a normalized multinomial random process \footfullcite{minka-multi}
\end{itemize}
\item As $N \to \infty$, the random process converges to $\uppsi | \uptheta \inprob \uptheta$
\begin{itemize}
\item [$\Rightarrow$] Use enables \alert{consistent} estimation of model
\end{itemize}

\end{itemize}



\hrulefill

\begin{columns}[c]

\begin{column}{.5\linewidth}
\textit{Alternate Notation}: $\uppsi \Leftrightarrow \big( \uppsim,\uppsic \big)$
\begin{itemize}
\item Marginal $\uppsim \equiv \sum_{y \in \Ycal} \uppsi(y,\cdot)$
\item Conditional $\uppsic(x) \equiv \uppsi(\cdot,x) / \uppsim(x)$
\end{itemize}
\end{column}

\begin{column}{.5\linewidth}
By the aggregation property \footnotemark,
\begin{itemize}
\item $\uppsim | \upthetam \sim \Emp(N,\upthetam)$
\item $\uppsic(x) | \uppsim(x),\upthetac(x) \sim \Emp\big( N \uppsim(x),\upthetac(x) \big)$ are mutually independent
\end{itemize}
\end{column}

\end{columns}

\footcitetext{johnson}

\end{frame}



\begin{frame}
\frametitle{Objective}

\begin{itemize}
\item Design a regression function $f: \Uppsi \mapsto \Rbb^{\Xcal}$ to minimize the expected squared-error with respect to $\uptheta$:
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_SE}
\Rcal_{\Theta}(f ; \uptheta) = \Erm_{\yrm,\xrm,\uppsi | \uptheta} \Big[ \big( f(\xrm;\uppsi)-\yrm \big)^2 \Big] & \equiv & \underbrace{\Erm_{\xrm | \upthetam} \left[ \Sigma_{\yrm | \xrm,\upthetac} \right]}_{\mathlarger{\Rcal_{\Theta}^*(\uptheta)}} + \underbrace{\Erm_{\xrm,\uppsi | \uptheta} \Big[ \big( f(\xrm;\uppsi) - \mu_{\yrm | \xrm,\upthetac} \big)^2 \Big]}_{\mathlarger{\Rcal_{\Theta, \mathrm{ex}}(f ; \uptheta)}} \nonumber
\end{IEEEeqnarray}

\item Clairvoyant \footfullcite{kay-det} regressor $f_{\Theta}(\xrm;\upthetac) = \mu_{\yrm | \xrm,\upthetac}$ achieves irreducible squared-error $\Rcal_{\Theta}^*(\uptheta)$

\item Excess squared-error can be decomposed into \alert{bias} and \alert{variance} terms:
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_ex_SE}
\Rcal_{\Theta, \mathrm{ex}}(f ; \uptheta) & \equiv & \Erm_{\xrm | \upthetam} \left[ \Big( \Erm_{\uppsi | \uptheta}\big[ f(\xrm;\uppsi) \big] - f_{\Theta}(\xrm;\upthetac) \Big)^2 + \Crm_{\uppsi | \uptheta}\big[ f(\xrm;\uppsi) \big] \right] \nonumber 
\end{IEEEeqnarray}

\end{itemize}

\end{frame}

%\begin{frame}
%\frametitle{Objective}
%
%\begin{itemize}
%\item Design a regression function $f: \Uppsi \mapsto \Rbb^{\Xcal}$ to minimize the expected squared-error with respect to $\uptheta$:
%\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_SE}
%\Rcal_{\Theta}(f ; \uptheta) = \Erm_{\yrm,\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm)-\yrm \big)^2 \Big] & = & \underbrace{\Erm_{\xrm | \uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right]}_{\mathlarger{\Rcal_{\Theta}^*(\uptheta)}} + \underbrace{\Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big]}_{\mathlarger{\Rcal_{\Theta, \mathrm{ex}}(f ; \uptheta)}} \nonumber
%\end{IEEEeqnarray}
%
%\item Clairvoyant \footfullcite{kay-det} regressor $f_{\Theta}(\xrm;\uptheta) = \mu_{\yrm | \xrm,\uptheta}$ achieves irreducible squared-error $\Rcal_{\Theta}^*(\uptheta)$
%
%\item Excess squared-error can be decomposed into \alert{bias} and \alert{variance} terms:
%\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_ex_SE}
%\Rcal_{\Theta, \mathrm{ex}}(f ; \uptheta) & = & \Erm_{\xrm | \uptheta} \left[ \Big( \Erm_{\Drm | \uptheta}\big[ f(\xrm;\Drm) \big] - f_{\Theta}(\xrm;\uptheta) \Big)^2 + \Crm_{\Drm | \uptheta}\big[ f(\xrm;\Drm) \big] \right] \nonumber 
%\end{IEEEeqnarray}
%
%\end{itemize}
%
%\end{frame}




\begin{frame}
\frametitle{Bayesian Inference}

\textbf{Model unknown. Select prior $\prm_\uptheta$ and formulate Bayesian risk:}
\begin{IEEEeqnarray}{rCl} \label{eq:risk}
\Rcal(f) & = & \Erm_{\uptheta}\big[ \Rcal_{\Theta}(f ; \uptheta) \big] = \Erm_{\yrm,\xrm,\uppsi} \Big[ \big( f(\xrm;\uppsi)-\yrm \big)^2 \Big] \nonumber
\end{IEEEeqnarray}

%\large
%\begin{equation*} 
%\Downarrow \quad \Downarrow \quad \textbf{\textit{Model Unknown. Select Prior }} \bm{\mathrm{p}_\uptheta} \quad \Downarrow \quad \Downarrow 
%\end{equation*}
%\normalsize
%\begin{IEEEeqnarray}{rCl} \label{eq:risk}
%\Rcal(f) & = & \Erm_{\uptheta}\big[ \Rcal_{\Theta}(f ; \uptheta) \big] = \Erm_{\yrm,\xrm,\uppsi} \Big[ \big( f(\xrm;\uppsi)-\yrm \big)^2 \Big] \nonumber
%\end{IEEEeqnarray}

\vspace{-3em}
\huge
\begin{equation*} 
\Downarrow \quad \Downarrow \quad \Downarrow \quad \Downarrow 
\end{equation*}
\normalsize

\textbf{Bayes optimal regressor:}
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_xD}
f^*(\xrm;\uppsi) & = & \argmin_{y' \in \Rbb} \Erm_{\yrm | \xrm,\uppsi}\big[ (y'-\yrm)^2 \big] = \mu_{\yrm | \xrm,\uppsi} \nonumber
\end{IEEEeqnarray}

\begin{itemize}
\item[$*$] Observe that $\Prm_{\yrm | \xrm,\uppsi} = \Erm_{\uptheta | \xrm,\uppsi}\big[ \Prm_{\yrm | \xrm,\uptheta} \big] \equiv \mu_{\upthetac(\xrm) | \xrm,\uppsi}$
\end{itemize}

%\vspace{1em}

\centering
\fcolorbox{NRL_blue}{NRL_blue}{\color{white}
\parbox{37em}{
\centering
\large
\textbf{Bayesian distribution is the posterior mean \footfullcite{murphy} of the predictive model $\upthetac$}
}
}

\vspace{1em}

%Note: the Bayesian distribution $\Prm_{\yrm | \xrm,\uppsi} = \Erm_{\uptheta | \xrm,\uppsi}\big[ \Prm_{\yrm | \xrm,\uptheta} \big] \equiv \mu_{\upthetac(\xrm) | \xrm,\uppsi}$ is the posterior mean \footfullcite{murphy} of the true predictive model $\upthetac$

%\begin{itemize}
%\item Bayes optimal regressor = $\mu_{\yrm | \xrm,\uppsi}$
%\begin{itemize}
%\item[$*$] Note: Bayesian predictive distribution is the expectation of the true predictive distribution, $\Prm_{\yrm | \xrm,\uppsi} = \Erm_{\uptheta | \xrm,\uppsi}\big[ \Prm_{\yrm | \xrm,\uptheta} \big]$
%\end{itemize}
%\end{itemize}

\end{frame}









\section{Distributions: Prior to Predictive}


\begin{frame}
\frametitle{Dirichlet Prior}

\begin{columns}[c]

\begin{column}{.8\linewidth}

The probability density function (PDF) of the model $\uptheta \in \Theta$ is Dirichlet:
\begin{IEEEeqnarray*}{L}
\prm_{\uptheta}(\theta) = \Dir(\theta; \alpha_0,\alpha) \equiv \beta(\alpha_0 \alpha)^{-1} \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\alpha_0 \alpha(y,x) - 1}
\end{IEEEeqnarray*}

%\footcitetext{bishop}

\begin{itemize}
\item Parameter $\alpha_0$ controls localization around mean $\alpha$
\end{itemize}

\vspace{1em}

\textit{Alternate Notation}: 
\begin{itemize}
\item Marginal $\alpham \equiv \sum_{y \in \Ycal} \alpha(y,\cdot)$ and conditional $\alphac(x) \equiv \alpha(\cdot,x) / \alpham(x)$

\item By the aggregation property\footnotemark, $\upthetam \sim \Dir(\alpha_0,\alpham)$ and $\upthetac(x) \sim \Dir\big(\alpha_0 \alpham(x),\alphac(x)\big)$, are \alert{independent} of one another and of $\upthetam$
\end{itemize}

\end{column}


\begin{column}{.2\linewidth}

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{SSP_2021/presentation/dir_loc.png}
%\caption{}
\end{figure}

\end{column}

\end{columns}

\footcitetext{ferguson}


\end{frame}




\begin{frame}
\frametitle{Predictive Model Posterior}
\framesubtitle{Closed-Form}

Since $\indep_x \upthetac(x)$ and $\upthetac \indep \upthetam$, and since the Empirical process $\upthetac(x) | \uppsim(x),\uppsic(x)$ has exponential form, Dirichlet process $\upthetac(x)$ is conjugate \footfullcite{theodoridis-ML} and thus
\begin{IEEEeqnarray*}{L}
	\upthetac(x) | \uppsim(x),\uppsic(x) \sim \Dir\Big( \alpha_0 \alpham(x) + N \uppsim(x), \mu_{\upthetac(x) | \uppsim(x), \uppsic(x)} \Big) \;,
\end{IEEEeqnarray*}
with mean functions
\begin{IEEEeqnarray*}{rCl} \label{eq:pred_Bayes_psi}
	\mu_{\upthetac(x) | \uppsim(x), \uppsic(x)} & = & \gamma(x; \uppsim) \alphac(x) + \big(1 - \gamma(x; \uppsim)\big) \uppsic(x) \equiv \textcolor[rgb]{1,0,0}{\Prm_{\yrm | \xrm,\uppsi}} \;,
\end{IEEEeqnarray*}
where $\gamma(x; \uppsim) = \Big( 1 + N \uppsim(x) / \big(\alpha_0 \alpham(x)\big) \Big)^{-1} \in (0,1]$.

\vspace{1em}

\centering
\fcolorbox{NRL_blue}{NRL_blue}{\color{white}
\parbox{37em}{
\centering
\large
\textbf{Bayesian predictions mix prior mean $\alphac$ with empirical distribution $\uppsic$}
}
}

\end{frame}



\begin{frame}
\frametitle{Predictive Model Posterior}
\framesubtitle{Trends}

\begin{columns}[c]

\begin{column}{.5\linewidth}

\begin{itemize}
\item As localization $\alpha_0$ increases, $\Prm_{\yrm | \xrm,\uppsi} \to \alphac(\xrm)$ and the prior is emphasized
\vspace{1em}
\item As training volume $N$ increases, $\Prm_{\yrm | \xrm,\uppsi} \to \uppsic(\xrm)$ and data is emphasized
\begin{itemize}
\item[$*$] Since $\uppsic | \upthetac \inprob \upthetac$, the true predictive model is \alert{identified}
\end{itemize}
\end{itemize}


\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{SSP_2021/presentation/prior_post_h.png}
%\caption{}
\label{fig:P_theta_post_uni}
\end{figure}

\end{column}

\end{columns}


\vspace{1em}

\centering
\fcolorbox{NRL_blue}{NRL_blue}{\color{white}
\parbox{37em}{
\centering
\large
\textbf{Full support prior ensures consistent estimation of model}
}
}

\end{frame}








\section{Bayesian Classifier and Error Trends}


\begin{frame}
\frametitle{Bayesian Classifier and Risk}

\textbf{Optimal Hypothesis}: the \emph{conditional majority decision}
\begin{IEEEeqnarray}{rCl} 
f^*(x;\bar{n}) & = & \argmax_{y \in \Ycal} \Prm_{\yrm | \xrm,\nbarrm}\big( y | x,\bar{n} \big) = \argmax_{y \in \Ycal} \bar{n}(y,x) \nonumber
\end{IEEEeqnarray}

\hrulefill
\vspace{0.5em}

\textbf{Minimum Expected Probability of Error}:
\begin{IEEEeqnarray*}{L}
\Rcal^* = 1 - \Erm_{\xrm,\Drm} \left[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \right] = 1 - \sum_{x \in \Xcal} \frac{\Erm_{\nbarrm} \big[ \max_{y \in \Ycal} \bar{\nrm}(y,x) \big] + 1}{|\Ycal||\Xcal| + N} \nonumber \\
= 1 - \frac{\sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big)}{|\Ycal| + N/|\Xcal|} 
\end{IEEEeqnarray*}
\vspace{0.5em}
\begin{itemize}
\item[$*$] \emph{Efficient formula derived using Inclusion-Exclusion principle}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Probability of Error Trends}
\framesubtitle{with Class/Data Set Sizes}

\begin{columns}[c]

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_01_uni_N_leg_My.pdf}
%\caption{Minimum 0--1 Risk for different numbers of classes}
\label{fig:Risk_01_uni_N_leg_My}
\end{figure}

\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_01_uni_N_leg_Mx.pdf}
%\caption{Minimum 0--1 Risk for different numbers of possible observations}
\label{fig:Risk_01_uni_N_leg_Mx}
\end{figure}

\end{column}

\end{columns}

\begin{itemize}
\item Larger class sets $\Ycal$ raise the lower bound on probability of error
\item Larger observation set $\Xcal$ $\longrightarrow$ more data $N$ required to achieve the same level of performance
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Probability of Error Trends}
\framesubtitle{with Training Data Volume}

\begin{columns}[c]

\begin{column}{.5\linewidth}

\begin{itemize}
\item For binary classification, infinite training data only reduces the expected probability of error from 0.5 to 0.25
\item As $|\Ycal|$ increases, the probability of error tends to unity and any improvement due to training data becomes negligible
\end{itemize}

\begin{table}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{| c | c |}
\hline 
$N$ & $\Rcal^*$ \\
\hhline{|=|=|}
$0$ & $1 - |\Ycal|^{-1}$  \\ 
\hline
$\to \infty$ & $1 - |\Ycal|^{-1} \sum_{m=1}^{|\Ycal|} m^{-1}$ \\
\hline
\end{tabular}
%\caption{}
\end{table}

\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Risk_01_uni_N_bounds.pdf}
%\caption{Minimum 0--1 Risk for zero and infinite number of training data}
\label{fig:Risk_01_uni_N_bounds}
\end{figure}

\end{column}

\end{columns}

\end{frame}


\begin{frame}
\frametitle{Comparison to Informative Classifiers}
\framesubtitle{Bayes Risk}

Classifiers derived from informative Dirichlet priors with $\alpha_f'(x) \gg |\Ycal||\Xcal|$ prioritize the prior mean $\mu_{\upthetac} = \alpha_f(\cdot,x) / \alpha_f'(x)$ for prediction

\vspace{-0.5em}
\begin{columns}[T]

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_01_Dir_N_leg_f_a0.pdf}
%\caption{}
\label{fig:Risk_01_Dir_N_leg_f_a0}
\end{figure}
\vspace{-2.5em}
\centering
\footnotesize
%Various $\alpha'(x)$

\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_01_Dir_N_leg_f_mu.pdf}
%\caption{}
\label{fig:Risk_01_Dir_N_leg_f_mu}
\end{figure}
\vspace{-2.5em}
\centering
\footnotesize
%Various $\mu_{\upthetac}$

\end{column}

\end{columns}

\vspace{1.8em}
\centering
\fcolorbox{NRL_blue}{NRL_blue}{\color{white}
\parbox{37em}{
\centering
\large
\textbf{Slower adaptation $\Rightarrow$ Higher Bayes probability of error}
}
}

\end{frame}



\begin{frame}
\frametitle{Comparison to Informative Classifiers}
\framesubtitle{Excess Conditional Risk}

\vspace{-1em}
\begin{columns}[c]

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_cond_ex_01_Dir_theta__uni_clim.pdf}
%\caption{}
\label{fig:Risk_cond_ex_01_Dir_theta__uni}
\end{figure}

\end{column}

\begin{column}{.5\linewidth}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Risk_cond_ex_01_Dir_theta__subj_clim.pdf}
%\caption{}
\label{fig:Risk_cond_ex_01_Dir_theta__subj}
\end{figure}

\end{column}

\end{columns}

\vspace{-0.2em}
\begin{block}{Trade-Off}
\textbf{Uniform prior provides a robust classifier for all unknown models $\theta$, limiting the space of high error models. However, fewer models achieve the clairvoyant risk.}
\end{block}


\end{frame}




\begin{frame}
\frametitle{Conclusions}

\begin{itemize}
\item The majority decision classifier designed with a uniform Dirichlet prior minimizes the possibility of maximal error for applications where the data distribution can not be adequately modeled
\item Full support of Dirichlet priors guarantees minimal risk in the limit of training data volume
\item Efficient closed-form for Bayesian probability of error provides a lower-bound for general classifiers
\end{itemize}

\begin{columns}[T]

\begin{column}{.55\linewidth}

\begin{itemize}
\item Low localization Dirichlet priors result in the same classifier - these priors are sensible for recognition applications where humans perform with low-error
\end{itemize}

\end{column}

\begin{column}{.4\linewidth}

\vspace{-2em}
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{P_theta_highVar.pdf}
%\caption{}
\label{fig:P_theta_highVar}
\end{figure}

\end{column}

\end{columns}


\end{frame}







\end{document}
