
\documentclass[conference]{IEEEtran}
%\documentclass[12pt]{article}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{bm}
\usepackage{upgreek}

\usepackage{hyperref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\leftmark}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{2pt}
%\renewcommand{\footrulewidth}{1pt}

%\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage[nottoc]{tocbibind}


\linespread{1.3}

\setcounter{secnumdepth}{3}



%\graphicspath{ {C:/Users/Paul/Documents/PhD/Dissertation/Documentation/Figures/} }
\graphicspath{ {../Figures/} }


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator{\xrm}{\mathrm{x}}
\DeclareMathOperator{\Xrm}{\mathrm{X}}
\DeclareMathOperator{\yrm}{\mathrm{y}}
\DeclareMathOperator{\Yrm}{\mathrm{Y}}
\DeclareMathOperator{\Drm}{\mathrm{D}}
\DeclareMathOperator{\nrm}{\mathrm{n}}
\DeclareMathOperator{\nbarrm}{\bar{\mathrm{n}}}
\DeclareMathOperator{\zrm}{\mathrm{z}}

\DeclareMathOperator{\Prm}{\mathrm{P}}
\DeclareMathOperator{\prm}{\mathrm{p}}
\DeclareMathOperator{\Erm}{\mathrm{E}}
\DeclareMathOperator{\Crm}{\mathrm{C}}

\DeclareMathOperator{\Xcal}{\mathcal{X}}
\DeclareMathOperator{\Ycal}{\mathcal{Y}}
\DeclareMathOperator{\Dcal}{\mathcal{D}}
\DeclareMathOperator{\Ncal}{\mathcal{N}}
\DeclareMathOperator{\Zcal}{\mathcal{Z}}
\DeclareMathOperator{\Hcal}{\mathcal{H}}
\DeclareMathOperator{\Fcal}{\mathcal{F}}
\DeclareMathOperator{\Rcal}{\mathcal{R}}
\DeclareMathOperator{\Mcal}{\mathcal{M}}
\DeclareMathOperator{\Scal}{\mathcal{S}}
\DeclareMathOperator{\Pcal}{\mathcal{P}}
\DeclareMathOperator{\Lcal}{\mathcal{L}}

\DeclareMathOperator{\Rbb}{\mathbb{R}}
\DeclareMathOperator{\Nbb}{\mathbb{N}}
\DeclareMathOperator{\Zbb}{\mathbb{Z}}

\DeclareMathOperator{\Dir}{\mathrm{Dir}}
\DeclareMathOperator{\DM}{\mathrm{DM}}
\DeclareMathOperator{\Mult}{\mathrm{Mult}}
\DeclareMathOperator{\DP}{\mathrm{DP}}
\DeclareMathOperator{\DMP}{\mathrm{DMP}}






\title{Bayesian Learning for Classification using a Non-Informative Uniform Dirichlet Prior}

\author{
\IEEEauthorblockN{Paul Rademacher}
\IEEEauthorblockA{\textit{Radar Division}\\ \textit{U.S. Naval Research Laboratory}\\ Washington, DC, USA\\ paul.rademacher@nrl.navy.mil}
\and
\IEEEauthorblockN{Milo\v{s} Doroslova\v{c}ki}
\IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering}\\ \textit{The George Washington University}\\ Washington, DC, USA\\ doroslov@gwu.edu}
}







\begin{document}

\maketitle

\begin{abstract}
PGR
\end{abstract}

\begin{IEEEkeywords}
Bayesian learning, machine learning, classification, Dirichlet distribution, predictive distribution
\end{IEEEkeywords}


PGR: cite agg



\section{Introduction}

Bayesian learning methods attempt to make better decisions by exploiting knowledge regarding the data-generating distribution. A prior probability distribution weights the different data distributions and defines the mechanism for unknown quantities to be predicted using independent training data. When highly concentrated ``subjective'' priors are used, the performance of the learned functions will vary widely. If the prior is localized around the true data-generating model, low-risk decisions can be made even with limited training data; conversely, if the prior assigns low weighting to the true model, satisfactory performance may not be realized. 

If the designer does not have prior confidence in any specific model, a non-informative prior distribution can be used to weight the different models equally. Although learners designed with such a prior will not perform as well as those made with well-selected subjective priors, they are more robust against poor prior selection.

In the literature, priors for parametric learning are commonly referred to as non-informative if they are approximately uniform over their support, even if their support is limited; with this in mind, such priors can be viewed as highly subjective. The uniform Dirichlet distribution is unique in that it has full support over the space of data-generating distributions and is completely non-informative. Additionally, it is a conjugate prior \cite{theodoridis-ML} for indepdendent, identically distributed observations and leads to a closed-form posterior distribution.

This work assumes that the joint observed/unobserved random variables are drawn from finite sets, enabling the use of the uniform Dirichlet prior. After discussing the relevant probability distributions, they will be applied to Bayesian classification using the 0-1 loss function. The effects of the data set cardinalities and the volume of training data on the minimum probability of error will be analyzed.







\section{Objective}

Consider an observable random element $\xrm \in \Xcal$ and and unobservable random element $\yrm \in \Ycal$ which are jointly distributed according to an unknown probability distribution $\theta \in \Theta = \left\{ \theta \in {\Rbb_{\geq 0}}^{\Ycal \times \Xcal}: \sum_{y \in \Ycal} \sum_{x \in \Xcal} \theta(y,x) = 1 \right\}$, such that $\Prm_{\yrm,\xrm | \uptheta}(y,x | \theta) = \theta(y,x)$. 

Also observed is a random sequence of $N$ samples drawn from $\theta$, denoted $\Drm = ( \Yrm,\Xrm ) \in \Dcal = \{\Ycal \times \Xcal\}^N$. The $N$ data pairs are identically distributed as $\Prm_{\Drm_n | \uptheta}(y,x | \theta) = \theta(y,x)$ and are conditionally independent from one another and from the novel pair $(\yrm,\xrm)$.

The objective is to design a decision function $f: \Dcal \mapsto \Hcal^{\Xcal}$ which outputs a mapping from the space of the observed random element $\xrm$ to a decision space $\Hcal$. The metric guiding the design is a loss function $\Lcal: \Hcal \times \Ycal \mapsto \Rbb_{\geq 0}$ which penalizes the decision $h \in \Hcal$ based on the value of $\yrm$. The conditional expected loss, or conditional ``risk'', is defined as
\begin{equation} \label{eq:risk_cond}
\Rcal_{\Theta}(f ; \uptheta) = \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm,\xrm | \uptheta} \Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \bigg] \;.
\end{equation}

As the model $\theta$ is not observed, $\Rcal_{\Theta}$ is not yet a feasible objective function for optimization. If the designer selects a PDF $\prm_{\uptheta}$, the Bayes risk is formulated as
\begin{IEEEeqnarray}{rCl} \label{eq:risk}
\Rcal(f) & = & \Erm_{\uptheta}\big[ \Rcal_{\Theta}(f ; \uptheta) \big] \\
& = & \Erm_{\yrm,\xrm,\Drm}\big[ \Lcal(f(\xrm;\Drm),\yrm) \big] \nonumber
\end{IEEEeqnarray}
and $\yrm$, $\xrm$, and $\Drm$ are treated as jointly distributed random elements. The optimal learning function is expressed as
\begin{equation} \label{eq:f_opt_xD}
f^*(\xrm;\Drm) = \argmin_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm}\big[ \Lcal(h,\yrm) \big]
\end{equation}
and the corresponding minimum Bayes risk is
\begin{equation} \label{eq:risk_min}
\Rcal(f^*) = \Erm_{\xrm,\Drm} \left[ \min_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm}\big[ \Lcal(h,\yrm) \big] \right] \;.
\end{equation}














\section{Probability Distributions}

In this section, the joint PMF $\Prm_{\yrm,\xrm,\Drm}$ is determined using the uniform Dirichlet prior $\prm_{\uptheta}$. Other distributions of interest will be provided, including the training data PMF $\Prm_{\Drm}$ and the predictive distribution $\Prm_{\yrm | \xrm,\Drm}$.




\subsection{Model PDF, $\prm_{\uptheta}$} \label{sec:P_theta}

The PDF for the model random process $\uptheta \in \Theta$ is Dirichlet \cite{bishop} with parameters $\alpha(y,x) = 1$, such that
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta}(\theta) & = & \beta(\alpha)^{-1} \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\alpha(y,x) - 1} \bigg|_{\alpha = 1} \\
& = & \big( |\Ycal||\Xcal|-1 \big)! \nonumber 
\end{IEEEeqnarray}
is uniform over $\Theta$. Note that $\beta$ is the generalized beta function. 


%Figure \ref{fig:P_theta_uniform} shows the uniform distribution amplitude for $|\Ycal| = 3$ and $|\Xcal| = 1$.
%
%\begin{figure}
%\centering
%\includegraphics[width=0.8\linewidth]{P_theta_uniform.pdf}
%\caption{Uniform model prior PDF, $|\Ycal| = 3$, $|\Xcal| = 1$}
%\label{fig:P_theta_uniform}
%\end{figure}

For convenience, the Dirichlet concentration parameter is defined as $\alpha_0 \equiv \sum_{y \in \Ycal} \sum_{x \in \Xcal} \alpha(y,x)$. The mean function of the model is 
\begin{equation}
\mu_{\uptheta}(y,x) = \frac{\alpha(y,x)}{\alpha_0}\bigg|_{\alpha = 1} = \big( |\Ycal||\Xcal| \big)^{-1} \;.
\end{equation}
Observe that $\Prm_{\yrm,\xrm} = \mu_{\uptheta}$ and thus the novel data marginal PMF is uniform over $\Ycal \times \Xcal$. 













\subsection{Training Set PMF, $\Prm_{\Drm}$}

Next, the conditional distribution $\Prm(\Drm | \uptheta)$ will be used to determine the marginal PMF, $\Prm(\Drm)$. The distribution of $\Drm$ conditioned on the model can be formulated as
\begin{IEEEeqnarray}{rCl}
\Prm\big( \Drm | \uptheta \big) & = & \prod_{y \in \Ycal} \prod_{x \in \Xcal} \uptheta(y,x)^{\bar{N}(y,x;\Drm)} \;,
\end{IEEEeqnarray}
where the dependency on the training data $\Drm$ is expressed though a transform function $\bar{N} : \Dcal \mapsto \bar{\Ncal}$, defined as $\bar{N}(y,x;D) = \sum_{n=1}^N \delta \left[ y,Y_n \right] \delta \left[ x,X_n \right]$, which counts the number of occurences of the pair $(y,x)$ in the training set $D$. The range of the transform is the function space $\bar{\Ncal} = \left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal \times \Xcal}: \sum_{y \in \Ycal} \sum_{x \in \Xcal} \bar{n}(y,x) = N \right\}$. The cardinality of the set is $|\bar{\Ncal}| = \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1}$; this can be shown using the stars-and-bars method \cite{feller}.

Note that $\Prm(\Drm | \uptheta)$ depends on the training data $\Drm$ only through the transform $\bar{N}$; $\bar{N}(\Drm)$ is thus a sufficient statistic \cite{bernardo} for the model $\uptheta$. Consequently, other distributions of interest $\Prm(\Drm)$ and $\Prm(\yrm | \xrm,\Drm)$ will also depend on $\Drm$ via $\bar{N}(\Drm)$. As such, it is useful to define a new random process $\nbarrm \equiv \bar{N}(\Drm) \in \bar{\Ncal}$. 



The conditional PMF $\Prm(\nrm | \uptheta)$ is easily shown to be Multinomial. As a Dirichlet distribution $\prm_{\uptheta}$ characterizes the parameters of this distribution, the marginal PMF of $\nbarrm$ is a Dirichlet-Multinomial distribution \cite{johnson} parameterized by $\alpha = 1$,
\begin{IEEEeqnarray}{rCl}
\Prm(\nbarrm) & = & \Mcal(\nbarrm) \beta(\alpha)^{-1} \beta(\alpha + \nbarrm) \big|_{\alpha = 1} \\
& = & |\bar{\Ncal}|^{-1} = \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1}^{-1} \nonumber \;,
\end{IEEEeqnarray}
which is uniform over the set $\bar{\Ncal}$. The operator $\Mcal$ represents the multinomial coefficient.












\subsection{Predictive PMF, $\Prm_{\yrm | \xrm,\Drm}$}

As shown in Equation \eqref{eq:f_opt_xD}, the decision selected by the optimally designed function depends on the Bayesian predictive PMF $\Prm(\yrm | \xrm,\Drm)$. First, note that as $\Prm(\Drm | \uptheta)$ is of exponential form, the Dirichlet prior $\prm(\uptheta)$ is its conjugate prior \cite{theodoridis-ML}; thus, the posterior PDF $\prm_{\uptheta | \Drm}$ is a Dirichlet distribution with parameter function $\alpha(y,x) = \bar{N}(y,x;\Drm) + 1$.
%\begin{IEEEeqnarray}{rCl}
%\prm(\uptheta | \Drm) & = & \big(N+|\Ycal||\Xcal|-1\big)! \prod_{y \in \Ycal} \prod_{x \in \Xcal} \frac{\uptheta(y,x)^{\bar{N}(y,x;\Drm)}}{\bar{N}(y,x;\Drm)!} \;.
%\end{IEEEeqnarray}

%The posterior concentration parameter is $\alpha_0 = N+|\Ycal||\Xcal|$; Figure \ref{fig:P_theta_post_uni} shows how the posteror tends toward $\prm_{\uptheta} \to \delta\big( \cdot - \mu_{\uptheta} \big) = \delta\big( \cdot - \bar{N}(\Drm)/N \big)$ with infinite training data.
%
%\begin{figure}
%\centering
%\includegraphics[width=0.9\linewidth]{P_theta_post_uni.pdf}
%\caption{Model Posterior for different training sets}
%\label{fig:P_theta_post_uni}
%\end{figure}


The joint PMF of $\yrm$ and $\xrm$ conditioned on the training data is expressed as $\Prm(\yrm,\xrm | \Drm) = \mu_{\uptheta | \Drm}(\yrm,\xrm)$ \cite{murphy}. Finally, the predictive distribution of interest is generated via Bayes rule as
\begin{IEEEeqnarray}{rCl} \label{P_y_xD_uniform}
\Prm(\yrm | \xrm,\Drm) & = & \frac{\bar{N}(\yrm,\xrm;\Drm)+1}{N'(\xrm;\Drm) + |\Ycal|} \\
& = & \left( \frac{|\Ycal|}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{1}{|\Ycal|} + \nonumber \\
&& \quad \left( \frac{N'(\xrm;\Drm)}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{\bar{N}(\yrm,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \;.
\end{IEEEeqnarray}
where $N'(x;D) = \sum_{n=1}^N \delta \left[ x,X_n \right]$.

The last representation views the distribution as a convex combination of two conditional distributions. The first PMF $\Prm_{\yrm | \xrm} = |\Ycal|^{-1}$ is uniform and independent of the training data; the second distribution is the conditional empirical PMF. As the number of observations matching $\xrm = x$, $N'(x;\Drm)$, increases relative to the number of classes $|\Ycal|$, the predictive PMF tends toward the empirical PMF.









\section{Classification using the 0-1 Loss}

In this section, the developed framework is applied to classification. The 0-1 loss function is the most widely used for these problems; it is represented as $\Lcal(h,y) = 1 - \delta[h,y]$ with hypothesis space $\Hcal = \Ycal$.


\subsection{Optimal Hypothesis: Conditional Maximum \emph{a posteriori}}

To determine the optimal learning function, the 0-1 loss is substituted into \eqref{eq:f_opt_xD} to find
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_01}
f^*(\xrm;\Drm) & = & \argmin_{h \in \Ycal} \Erm_{\yrm | \xrm,\Drm}\big[ 1 - \delta[h,\yrm] \big] \\
& = & \argmax_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \nonumber \\
& = & \argmax_{y \in \Ycal} \bar{N}(y,\xrm;\Drm) \nonumber \;.
\end{IEEEeqnarray}
The optimal classifier chooses the value $y \in \Ycal$ that maximizes the conditional PMF for the observed values of $\xrm$ and $\Drm$. It is a conditional majority decision which chooses the class from $\Ycal$ most often represented among training set samples $\Drm$ with a matching input value $\xrm$. 







\subsection{Minimum Risk: Probability of Error}

Substituting the 0-1 loss into \eqref{eq:risk_min}, the minimum Bayes 0-1 risk is 
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & 1 - \Erm_{\xrm,\Drm} \left[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \right] \\
& = & 1 - \sum_{x \in \Xcal} \frac{\Erm_{\nbarrm} \big[ \max_{y \in \Ycal} \bar{\nrm}(y,x) \big] + 1}{|\Ycal||\Xcal| + N} \nonumber \;.
\end{IEEEeqnarray}
The expectation operates on the maximum value from a subset of the random process $\nbarrm$. Via the Dirichlet-Multinomial aggregation property \cite{johnson}, a consequence of the the uniform PMF $\Prm(\nbarrm)$ is that the individual segments $\nbarrm(\cdot,x)$ are identically distributed; thus, the expectation will be same for every value $x$.

The expectation is found in Appendix \ref{app:n_max}; substituting, the minimum 0-1 risk is
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & 1 - \frac{\sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big)}{|\Ycal| + N/|\Xcal|} \; .
\end{IEEEeqnarray}




It is informative to express the risk for minimal and maximal volumes of training data. Using an identity for alternating binomial sums of polynomials \cite{graham}, it can be shown that for $N = 0$, the minimum risk is $\Rcal^*  = 1 - |\Ycal|^{-1}$. This is sensible, as the classes are equiprobable with $\Prm_{\yrm} = |\Ycal|^{-1}$.


To find the risk for $N \to \infty$, note that
\begin{IEEEeqnarray}{L}
\lim_{N \to \infty} \big( |\Ycal| + N/|\Xcal| \big)^{-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big) \\
\qquad = \lim_{N/m \to \infty} \frac{|\Xcal|}{N} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \left( 1 - \frac{mn}{N} \right)^{|\Ycal||\Xcal|-1} \nonumber \\
\qquad = \frac{|\Xcal|}{m} \int_0^1 (1-t)^{|\Ycal||\Xcal|-1} \mathrm{d} t  = \frac{1}{m|\Ycal|} \nonumber \;.
\end{IEEEeqnarray}
The irreducible 0-1 risk for the uniform prior tends toward
\begin{IEEEeqnarray}{rCl}
\Rcal^* & \to & 1 - |\Ycal|^{-1} \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} m^{-1} \\
& = & 1 - |\Ycal|^{-1} \sum_{m=1}^{|\Ycal|} m^{-1} \nonumber \;,
\end{IEEEeqnarray}
providing a lower bound for the achievable 0-1 Bayes risk. The above formulation has made use of the alternating summation identity from \cite{roman} to display the risk with a form including the $|\Ycal|^\mathrm{th}$ harmonic number $H_{|\Ycal|} = \sum_{m=1}^{|\Ycal|} m^{-1}$. Observe that the irreducible risk does not depend on the cardinality $|\Xcal|$.

PGR: harmonic reference?



The trends for the minimum 0-1 risk can be illustrated with examples.

Figure \ref{fig:Risk_01_uni_N_leg_My} demonstrates how the minimum 0-1 risk decreases with training volume $N$; observe that the risk is more severe for sequences corresponding to higher $|\Ycal|$. It is sensible that the probability of error should increase when more classes have to be considered. Figure \ref{fig:Risk_01_uni_N_leg_Mx} illustrates the minimum risk with multiple sequences for different cardinalities $|\Xcal|$. Note that risk increases with $|\Xcal|$. Considering $\Erm\big[N'(\Drm)\big] = \Erm[\nrm'] = N/|\Xcal|$, this should be intuitive - each conditional empirical distribution $\bar{N}(\cdot,x;D) / N'(x;D)$ is forced to approximate $\tilde{\uptheta}(\cdot;x)$ with less data.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{Risk_01_uni_N_leg_My.pdf}
\caption{Optimal 0-1 Risk vs training set size for various input set sizes}
\label{fig:Risk_01_uni_N_leg_My}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{Risk_01_uni_N_leg_Mx.pdf}
\caption{Optimal 0-1 Risk vs training set size for various input set sizes}
\label{fig:Risk_01_uni_N_leg_Mx}
\end{figure}

%Further insight into how $|\Xcal|$ affects the risk can be acquired by plotting the risk as a function of $N/|\Xcal|$. In Figure \ref{fig:Risk_01_uni_N-Mx}, it is shown that the optimal risk can be approximated by a function dependent only on $N/|\Xcal|$; of the series plotted, only the series for $|\Xcal| = 1$ shows notable non-negligible from the others.
%
%\begin{figure}
%\centering
%\includegraphics[width=0.9\linewidth]{Risk_01_uni_N-Mx.pdf}
%\caption{Optimal 0-1 Risk vs training set size for various input set sizes}
%\label{fig:Risk_01_uni_N-Mx}
%\end{figure}


It is also informative to graph the $N=0$ and $N \to \infty$ minimum risk as a function of $|\Ycal|$; both formulas are independent of $|\Xcal|$. Figure \ref{fig:Risk_01_uni_N_bounds} displays these bounds; note the margin in the probability of error between the optimal $N=0$ and $N \to \infty$ classifiers. For $|\Ycal| = 2$ binary classification, both sequences are at their minimum and infinite training data provides a reduction in expected probability of error from 0.5 to 0.25. As $|\Ycal|$ increases, the classification risk for both the $N=0$ and $N \to \infty$ cases tend to unity and the error reduction for $N \to \infty$ decreases. 

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{Risk_01_uni_N_bounds.pdf}
\caption{Optimal 0-1 Risk vs training set size for various input set sizes}
\label{fig:Risk_01_uni_N_bounds}
\end{figure}






\section{Conclusion}

This work has used the non-informative uniform Dirichlet prior for Bayesian learning, discussed the Bayes predictive distribution, and applied the results to classification. The optimal majority decision learner and the minimum 0-1 risk have been analyzed. Graphical examples illustrate how the probability of error increases with the number of classes and the number of possible observations. Additionally, the asympotic Bayes risk for infinite training data has been found and its modest deviation from the risk without training data has been discussed. 

Future work will expand the results presented for a general Dirichlet prior. The effect on the subjectivity of the prior on the conditional risk \eqref{eq:risk_cond} will be of specific interest. Additionally, the work will be generalized for an infinite number of possible observations using Dirichlet processes








\appendices

\section{} \label{app:n_max}

To evaluate this expectation, new random variables $\nbarrm_{\max}(x) \equiv \max_{y \in \Ycal} \nbarrm(y,x)$ are introduced and characterized by their identical PMF. To this end, the probability of the event $\Prm\big( \nbarrm_{\max}(x) \geq n \big) = \Prm\big( \cup_{y \in \Ycal} \{ \nbarrm(y,x) \geq n \} \big)$ will be determined. As the distribution of $\nbarrm$ is uniform, the event probability is proportionate to the cardinality of the set $\cup_{y \in \Ycal} \{ \bar{n}: \bar{n}(y,x) \geq n \}$. Using the inclusion-exclusion principle \cite{brualdi}, the cardinality is represented as
\begin{IEEEeqnarray}{L}
\big| \cup_{y \in \Ycal} \{ \bar{n} : \bar{n}(y,x) \geq n \} \big| \\
\quad = \begin{cases} \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} & \mathrm{if} \ n < 0, \\ \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \\ \quad \binom{N-mn+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} H\Big( \big\lfloor\frac{N}{m}\big\rfloor - n \Big) & \mathrm{if} \ 0 \leq n \leq N, \\ 0 & \mathrm{if} \ n > N, \end{cases} \nonumber
\end{IEEEeqnarray}
where $H: \Zbb \mapsto \{0,1\}$ is the discrete Heaviside step function. For $n < 0$, the cardinality is equivalent to $|\bar{\Ncal}|$. 

For $0 \leq n < N$, the cardinality is an alternating binomial summation where the $m^\mathrm{th}$ term accounts for the different intersections of $m$ of the $|\Ycal|$ individual sets $\{ \bar{n} : \bar{n}(y,x) \geq n \}$. Observe that the cardinality of the intersections is only dependent on the number of contributing sets $m$ and not on which sets intersect. Furthermore, note the dependency of the intersection cardinalities on the argument $n$. The step function contributes such that if $n > \big\lfloor\frac{N}{m}\big\rfloor$, only up to $m-1$ individual sets will intersect. The binomial coefficient calculates the intersection cardinality for a given $m$; note the similarity to the cardinality $|\bar{\Ncal}|$ - the only difference is the number of points characterizing the $|\Ycal||\Xcal|-1$ dimensional region.

The probability of interest can thus be expressed as $\Prm\big( \nbarrm_{\max}(x) \geq n \big) = \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1}^{-1} \big| \cup_{y \in \Ycal} \{ \bar{n} : \bar{n}(y,x) \geq n \} \big|$.

As the PMF of $\nbarrm_{\max}(x)$ has support on $n \in [0,\ldots,N]$, the expectation over $\nbarrm$ is evaluated as
\begin{IEEEeqnarray}{L}
\Erm_{\nbarrm}\big[ \nbarrm_{\max}(x) \big] = \sum_{n=0}^N n \Prm\big( \nbarrm_{\max}(x) = n \big) \\
\quad = -1 + \sum_{n=0}^N \Prm\big( \nbarrm_{\max}(x) \geq n \big) \nonumber \\
\quad = -1 + \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \binom{N-mn+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} \nonumber 
\end{IEEEeqnarray}




\bibliographystyle{IEEEtran}
\bibliography{{../References/phd_bib}}


\end{document}


























