
\documentclass[11pt]{article}
% \documentclass{report}
% \documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{bm}

\usepackage[retainorgcmds]{IEEEtrantools}


\title{Bayesian Learning using a Non-Informative Prior over Finite-Dimensional Spaces}
\author{Paul Rademacher}
\date{October 20, 2017}


\begin{document}

\maketitle


\section{Introduction}

This report details a Bayesian perspective on stastical learning theory when both the input and outputs exist in finite dimensional spaces and a uniform prior distribution is used. To simplify the presentation, the first sections will assume that the predictions are not driven by any observable variable; subsequently, the results will be extended to the more practical case where we want to generate an estimate given observed data.

While the validity of Bayesian methods for statistical signal processing and machine learning has long been contended, the author believes it to be a justified approach that does not necessarily imply that the generative model is `random'; rather, it simply reflects the desire of the user to formulate risk as a weighted sum of learner performance across the space of models. 

The uniform, or `non-informative', prior is of specific interest because it reflects the user's lack of confidence that his/her data was generated using any specific distribution. Integrating a learner's risk with such a prior provides a Bayesian analogy to the ``No Free Lunch'' theorem; however, it will be shown that for a general loss metric, all learning functions \emph{do not} provide the same performance.

After examining the joint and conditional probability mass functions for the unobserved outputs and the training data, the results will be applied to two of the most common loss functions in machine learning: the squared error loss function (common for regression), and the 0-1 loss function (common for classification). Optimal learners will be presented and the loss as a function of space dimensions and volume of training data will be provided. Additionally, asymptotic results for these values will be discussed to provide insight into how well these learners perform for infinite input-output spaces and as the number of training examples increases. 



\section{Basic model: scalar estimate, no observations}

In this simplified treatment, we want to design a learner to provide an estimate of unobserved variable $y \in \mathcal{H} = \{ \mathcal{H}_1, \ldots, \mathcal{H}_M \}$ using $N$ observed training examples $\mathcal{D} \in \mathcal{H}^N$. It is assumed that the output and training data are independently and identically distributed according to an unknown probability mass function (PMF), $\text{P}_{y} \left(\mathcal{H}_{m}\right) = \bm{\theta}_m$, where $\bm{\theta} \in \bm{\Theta}_M = \left\{ \bm{\theta} \in {\mathbb{R}^+}^M: \bm{1}^\text{T}\bm{\theta} = 1 \right\}$. 

% $\bm{\theta} \in \bm{\Theta}_M = \left\{ \bm{\theta} \in \mathbb{R}^M: \bm{\theta}_m \geq 0, \bm{1}^\text{T}\bm{\theta} = 1, m = 1,\ldots,M \right\}$

Our objective is to create a learning function $\hat{y}: \mathcal{H}^N \mapsto \mathcal{H}$ that minimizes a chosen risk function $\mathcal{R}(\hat{y}) \in \mathbb{R}^+$.  The algorithm designer makes two decisions regarding how risk is calculated. First, he/she chooses a loss function $\mathcal{L}: \mathcal{H} \times \mathcal{H} \mapsto \mathbb{R}^+$ that assigns a penalty dependent on both the unobserved output and the estimated value. Using this function, we calculate the conditional risk for a given model $\bm{\theta}$,

\begin{equation}
\mathcal{R}_{\bm{\theta}}(\hat{y}) = \text{E}_{\mathcal{D}|\bm{\theta}} \left[ \text{E}_{y|\bm{\theta}} \left[ \mathcal{L}(\bm{y},\hat{\bm{y}}(\bm{D})) \right] \right] \;.
\end{equation}

The second choice the designer has is how to formulate a scalar risk based on the set of conditional risks $\mathcal{R}_{\bm{\theta}}: \bm{\Theta} \mapsto \mathbb{R}^+$.



\begin{equation}
R(\hat{\bm{y}}) = \text{E}_{\bm{y},\bm{D}} \left[ \mathcal{L}(\bm{y},\hat{\bm{y}}(\bm{D})) \right]
\end{equation}




\appendix

\section{Proof A}

\end{document}