\documentclass[12pt]{report}

\linespread{1.5}
\usepackage[margin=1.0in]{geometry}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{bm}
\usepackage{upgreek}

\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage{hyperref}
\usepackage[colorinlistoftodos,backgroundcolor=yellow,linecolor=yellow]{todonotes}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\leftmark}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{2pt}
%\renewcommand{\footrulewidth}{1pt}

\usepackage[nottoc]{tocbibind}
\setcounter{secnumdepth}{3}



\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator{\xrm}{\mathrm{x}}
\DeclareMathOperator{\Xrm}{\mathrm{X}}
\DeclareMathOperator{\yrm}{\mathrm{y}}
\DeclareMathOperator{\Yrm}{\mathrm{Y}}
\DeclareMathOperator{\Drm}{\mathrm{D}}
\DeclareMathOperator{\nrm}{\mathrm{n}}
\DeclareMathOperator{\nbarrm}{\bar{\mathrm{n}}}
\DeclareMathOperator{\zrm}{\mathrm{z}}

\DeclareMathOperator{\Prm}{\mathrm{P}}
\DeclareMathOperator{\prm}{\mathrm{p}}
\DeclareMathOperator{\Erm}{\mathrm{E}}
\DeclareMathOperator{\Crm}{\mathrm{C}}

\DeclareMathOperator{\Xcal}{\mathcal{X}}
\DeclareMathOperator{\Ycal}{\mathcal{Y}}
\DeclareMathOperator{\Dcal}{\mathcal{D}}
\DeclareMathOperator{\Ncal}{\mathcal{N}}
\DeclareMathOperator{\Zcal}{\mathcal{Z}}
\DeclareMathOperator{\Hcal}{\mathcal{H}}
\DeclareMathOperator{\Fcal}{\mathcal{F}}
\DeclareMathOperator{\Rcal}{\mathcal{R}}
\DeclareMathOperator{\Mcal}{\mathcal{M}}
\DeclareMathOperator{\Scal}{\mathcal{S}}
\DeclareMathOperator{\Pcal}{\mathcal{P}}
\DeclareMathOperator{\Lcal}{\mathcal{L}}

\DeclareMathOperator{\Rbb}{\mathbb{R}}
\DeclareMathOperator{\Nbb}{\mathbb{N}}
\DeclareMathOperator{\Zbb}{\mathbb{Z}}

\DeclareMathOperator{\Dir}{\mathrm{Dir}}
\DeclareMathOperator{\DM}{\mathrm{DM}}
\DeclareMathOperator{\Multi}{\mathrm{Multi}}
\DeclareMathOperator{\Bi}{\mathrm{Bi}}
\DeclareMathOperator{\DP}{\mathrm{DP}}
\DeclareMathOperator{\DMP}{\mathrm{DMP}}



%\usepackage{cite}
\usepackage[style=numeric]{biblatex}
\addbibresource{{../References/phd_bib.bib}}


\graphicspath{ {../Figures/} }


\title{Bayesian Learning using a Dirichlet Prior for Regression and Classification}
\author{Paul Rademacher}
%\date{}


\begin{document}

\maketitle
\tableofcontents


\newpage

\listoftodos

PGR: equation numbers to final line!

\todo[inline]{PGR: use todonotes package instead of my initials?}



PGR: multinomial random process?

PGR: change from nbar to emprical RV???

PGR: investigate N lim for nbar given theta, bayes risk vs model support

PGR: SEMI-SUPERVISED - generalize to joint decisions!!! training/test!

PGR: priors = sparse conditionals; w/ sufficient statistics

PGR: NFLT investigation? try matlab examples



PGR: fix arguments for theta, nbar dists.

PGR: suppress arguments where sensible? eg Pd = theta

PGR: introduce tilde alpha to match tilde theta?

PGR: use x,y subs instead of prime/tilde?

PGR: is $\Theta$ redundant given $\Pcal$?



PGR: ALL figure notation: theta font + Ycal indexing. use R,f opt?

PGR: use clairvoyant/irreducible terms and symbols 

PGR: check notation throughout for expectation/variance operators

PGR: check for remaining random function terminology

PGR: empirical risk terms/discussion?


PGR: learners (full) or decision functions (range)?

PGR: likelihood function terminology

PGR: Dirichlet localization or concentration?

PGR: Informative or Subjective?

PGR: brackets for expectation ops?

PGR: line break symbol format?



PGR: generalize y,x,h from scalars to functions!!!

PGR: jeffrey prior, fisher info?




PGR: bibliography

Theo: (mult moments), DP agg, Dir moments

Bishop: (dir eq), dir posterior, moments, mode

Ferguson: (agg Dir), agg DP - via theo, DP posterior, moments

Gershman: agg DP - ref ferg, discrete draws

Johnson GET PDF: mult moments, (mult agg, DM agg, DM moments)

Add Theo-PR???

\newpage





\chapter{Introduction}


\section{Background}

PGR: complete rework

This report details a Bayesian perspective on stastical learning theory for when both the observations and unobserved quantities are jointly distributed according to an unknown probability distribution function. While the validity of Bayesian methods for statistical signal processing and machine learning has long been contended, the author believes it to be a justified approach that does not necessarily imply that the distribution model is `random'; rather, it simply reflects the desire of the user to formulate risk as a weighted sum of learner performance across the space of distributions. 

The success or failure of Bayesian learning methods hinge on how well the prior knowledge imparted by the designer matches reality. The chosen prior distribution over the set of data-generating probability distributions reflects the users confidence that different distributions are responsible for generating the observed/unobserved random elements. If a highly subjective prior \cite{box} is chosen that is localized around the actual data probability distribution, low risk learning functions are possible even with limited training data; however, if the subjective prior is poorly designed, a good solution may not be achieved. Conversely, a non-informative prior that weights the different distributions without preference provides a more robust solution for all models, but may underperform relative to learners based on well-selected subjective priors.

will always be able to adapt will enough training data; if data is limited, however, the learning function may not deliver the required performance.

This work assumes that the prior distribution is Dirichlet. The class of Dirichlet probability density functions (PDF) and processes have the desirable properties of full support over the set of possible data-generating distributions and an analytic posterior distribution for independently and identically distributed data \cite{ferguson}. Furthermore, control of the Dirichlet parameters can enable both objective and subjective prior knowledge. Special cases including the uniform prior will be given specific attention.

After introducing the problem and discussing the relevant data probability distributions, the Bayesian framework will be applied to two of the most common loss functions in machine learning: the squared error loss function (common for regression), and the 0-1 loss function \cite{berger} (common for classification). Optimal estimators/classifiers and their corresponding minimum risk will be presented for different Dirichlet prior distributions. Specific attention will be given to various asymptotic cases to show the differing performance for objective and subjective Dirichlet priors.




\section{Notation}

This section details the mathematical notation and typesetting conventions used throughout. Note that many variable scalars and functions including $x$, $y$, $g$, etc. are repeatedly redefined and reused to avoid introducing an excessive volume of symbols; unless explicitly stated, none of these variable definitions will hold in subsequent sections.


\subsection*{Sets and Function Arguments}

Sets will typically be typeset with a calligraphic font, such as $\Xcal$. Exceptions include common number sets such as the real numbers, which are typeset using blackboard bold $\Rbb$. Function spaces such as the set of functions $\Xcal \mapsto \Ycal$ are compactly represented as $\Ycal^{\Xcal}$.

Various mappings will be defined for which the domain and/or the range are function spaces. The set of functions $\Xcal \mapsto \Ycal$ is denoted $\Ycal^{\Xcal}$. For a mapping $g : \Zcal \mapsto \Ycal^{\Xcal}$, the argument notation $g(z) \in \Ycal^{\Xcal}$ denotes a function, while $g(x;z) \in \Ycal$ is a specific value of that function. Semicolons are used to distinguish between the different groups of arguments. A set of finite sequences $\{1,\ldots,N\} \mapsto \Scal$ will be represented as $\Scal^N$ for brevity.

The convention adopted for natural numbers is $\Nbb = \{1,2,\ldots\}$; the set of non-negative integers is denoted $\Zbb_{\geq 0} = \Nbb \cup \{0\}$. The set of positive real numbers $\Rbb^+$ excludes zero, while non-negative real numbers are represented as $\Rbb_{\geq 0} = \Rbb^+ \cup \{0\}$. The cardinality of countably infinite sets, including the set of natural numbers, is denoted $\aleph_0 = |\Nbb|$; the cardinality of uncountable sets such as $\Rbb$ is at least $\aleph_1$.

Numerous probability distribution functions will be defined over different domains. As such, for a given set $\Xcal$, define a set function $\Pcal$ such that $\Pcal(\Xcal)$ is the set of distributions over $\Xcal$. If $\Xcal$ is countable, the set is defined as $\Pcal(\Xcal) = \left\{ p \in {\Rbb_{\geq 0}}^{\Xcal}: \sum_{x \in \Xcal} p(x) = 1 \right\}$; if $\Xcal$ is a Euclidean space, the set is defined as $\Pcal(\Xcal) = \left\{ p \in {\Rbb_{\geq 0}}^{\Xcal}: \int_{\Xcal} p(x) \mathrm{d}x = 1 \right\}$.

PGR: aleph reference?

PGR: ref Rudin for range?



\subsection*{Random elements, variables, and processes}

Random elements are denoted with roman font (e.g. $\xrm$), while specific values are denoted with italics (e.g. $x$). Random elements that assume numerical scalars/functions are referred to as random variables/processes, respectively.

Consider a random element $\xrm \in \Xcal$. If $\Xcal$ is countable, either finite with $|\Xcal| \in \Nbb$ or countably infinite with $|\Xcal| = \aleph_0$, then $\xrm$ is a discrete random element and is characterized by a probability mass function (PMF) \cite {papoulis}, denoted $\Prm_{\xrm} \in \Pcal(\Xcal)$. If $\Xcal$ is a Euclidean space and is thus uncountable with $|\Xcal| \geq \aleph_1$, then $\xrm$ is a continuous random variable/process characterized by a probability density function (PDF), denoted $\prm_{\xrm} \in \Pcal(\Xcal)$.

PGR: explicit PMF/PDF formula with P of events?

For notational simplicity, probability distributions are occasionally represented as $\Prm(\xrm)$; in such instances, the formal notation can be recovered by replacing the distribution with $\Prm_{\xrm}(x)$ and all instances of the roman symbol $\xrm$ with the italic symbol $x$. 

Consider $\xrm$ conditioned on another random element $\zrm \in \Zcal$. The conditional distribution is represented as $\Prm_{\xrm | \zrm} : \Zcal \mapsto \Pcal(\Xcal)$, such that $\Prm_{\xrm | \zrm}(z)$ is a PMF over $\Xcal$ and $\Prm_{\xrm | \zrm}(x|z)$ is a specific value of that PMF. These distributions may be compactly represented as $\Prm(\xrm|\zrm)$. 

Often, the dependency on the conditional variable $\zrm$ will not be expressed in terms of a specific value $z$, but will be left in terms of the random element itself; in this case, the more compact notation $\Prm_{\xrm | \zrm}$ is used and the distribution is implied to be a function of $\zrm$.

PGR: reconsider conditional argument suppression, etc. ALWAYS random function? Can't do both preceeding paragraph styles. Fixed?

Many distributions will be repeatedly used and thus special functions will be defined for the PDF's and PMF's of interest. For example, consider a random process $\xrm \in \Xcal$ characterized by a Dirichlet distribution with parameters $\alpha \in \mathcal{A}$; the PDF will be notated as $\Dir : \mathcal{A} \mapsto \Pcal(\Xcal)$, where the range is the set of valid PDF's. More compactly, the notation $\xrm \sim \Dir(\alpha)$ implies that $\Prm_{\xrm} = \Dir(\cdot ; \alpha)$. Other distribution functions repeatedly used include $\Multi$, $\DM$, $\DP$, and $\DMP$, representing the multinomial distribution, the Dirichlet-multinomial distribution, the Dirichlet process, and the Dirichlet-Multinomial process.



\subsection*{Expectation Operators}

For a discrete random element $\xrm$, the expectation operator $\Erm_{\xrm}$ is defined as
\begin{equation}
\Erm_{\xrm}\big[ g(\xrm) \big] = \sum_{x} \Prm_{\xrm}(x) g(x) \;,
\end{equation}
where the argument $g$ is an arbitrary scalar function of $\xrm$ with range $\Rbb$. Additionally, define the variance operator $\Crm_{\xrm}$ as
\begin{equation}
\Crm_{\xrm}\big[g(\xrm)\big] = \Erm_{\xrm} \bigg[ \Big( g(\xrm) - \Erm_{\xrm}\big[g(\xrm)\big] \Big)^2 \bigg] \;.
\end{equation}
When $\xrm$ is a random variable and the function $g$ is the identity operator, such that $g(\xrm) = \xrm$, the mean and variance are represented by $\mu_{\xrm}$ and $\Sigma_{\xrm}$, respectively.

These operations can be performed with respect to a condtional distribution as well. In this case, the expectation operator is a function of the observed value of $\zrm$, such that
\begin{equation}
\Erm_{\xrm | \zrm}\big[ g(\xrm) \big](z) = \sum_{x} \Prm_{\xrm | \zrm}(x | z) g(x) \;.
\end{equation}
Similarly, the conditional variance is notated $\Crm_{\xrm | \zrm}\big[ g(\xrm) \big](z)$. When $g$ is the identity operator, the conditional mean and variance as represented by $\mu_{\xrm | \zrm}(z)$ and $\Sigma_{\xrm | \zrm}(z)$, respectively.

As with conditional distributions, it is common that an explicit value $z$ of the conditional random element will not be used, but rather the expectation will be left as a function of the random element $\zrm$. In these cases, the argument is suppressed and the notation $\Erm_{\xrm | \zrm}\big[ g(\xrm) \big]$ implies the dependency on $\zrm$. This convention also holds for the conditional variance operator $\Crm_{\xrm | \zrm}$, as well as for the $\mu_{\xrm | \zrm}$ and $\Sigma_{\xrm | \zrm}$ operators.

If the range of $g$ is a Hilbert space, such that $g(\xrm)$ is itself a function with a domain $\Ycal$, then the notation for these operators is expanded. The output of the expectation operator is a function over $\Ycal$ represented by
\begin{equation}
\Erm_{\xrm}\big[ g(\xrm) \big](y) = \sum_{x} \Prm_{\xrm}(x) g(y;x) \;.
\end{equation}
Similarly, the covariance operator notation is modified and the output is a function over $\Ycal \times \Ycal$, 
\begin{IEEEeqnarray}{L}
\Crm_{\xrm}\big[g(\xrm)\big](y,y') \\
\quad = \Erm_{\xrm} \bigg[ \Big( g(y;\xrm) - \Erm_{\xrm}\big[g(y;\xrm)\big] \Big) \Big( g(y';\xrm) - \Erm_{\xrm}\big[g(y';\xrm)\big] \Big) \bigg] \nonumber \;.
\end{IEEEeqnarray}
As before, the notation is simplified when the function $g$ is the identity operator. If $\xrm$ is a random process over a domain $\Ycal$, then the mean and covariance functions are $\mu_{\xrm}(y)$ and $\Sigma_{\xrm}(y,y')$. 

If the expectations are evaluated with respect to a conditional distribution $\Prm_{\xrm | \zrm}$, the additional argument for the observed random element is added and the notation for the above operators extends to $\Erm_{\xrm|\zrm}\big[ g(\xrm) \big](y|z)$ and $\Crm_{\xrm|\zrm}\big[g(\xrm)\big](y,y'|z)$ for non-scalar outputs. When $g$ is the identity operator, the notation $\mu_{\xrm|\zrm}(y|z)$ and $\Sigma_{\xrm|\zrm}(y,y'|z)$ is used.

Again, it is common for the conditional random element $\zrm$ to be left as a random quantity instead of being explicitly defined. In such cases, the italic $z$ is dropped from the arguments and the formulae $\Erm_{\xrm|\zrm}\big[ g(\xrm) \big](y)$, $\Crm_{\xrm|\zrm}\big[g(\xrm)\big](y,y')$, $\mu_{\xrm|\zrm}(y)$, and $\Sigma_{\xrm|\zrm}(y,y')$ imply dependence on $\zrm$.

PGR: ABOVE NOTATION CREATES AMBIGUITY !!!!!!!

As for probability distributions, the subscript notation of these operators may be suppressed. In such cases, the expectations are to be performed with respect to the joint distribution of all random elements (roman font) found in the argument. For example, $\Erm\big[f(\yrm,\xrm) | \zrm \big]$ compactly represents $\Erm_{\yrm,\xrm | \zrm}\big[f(\yrm,\xrm)\big]$.



\subsection*{Special Functions}

Certain specialized functions are detailed next. Both the Dirac and Kronecker delta functions will be used throughout. The Dirac delta function over a Euclidean domain $\Xcal$ is represented as $\delta(\cdot)$; it has support only at the point $x=0$ and satisfies
\begin{equation}
\int_{\Xcal} \delta(x) \mathrm{d}x = 1 \;.
\end{equation}
Consequently, it also satisfies
\begin{equation}
\int_{\Xcal} g(x) \delta(x) \mathrm{d}x = g(0) \;.
\end{equation}
Consider a countable set $\Xcal$; the Kronecker delta function has domain $\Xcal \times \Xcal$ and is defined as
\begin{equation}
\delta[x,x'] = \begin{cases} 1 & \mathrm{if} \ x = x', \\ 0 & \mathrm{if} \ x \neq x'.  \end{cases}
\end{equation}

PGR: reference Dirac/Kronecker?


The multinomial coefficient and multivariate beta function, which typically operate on sequences, are defined more generally for function inputs. The multinomial operator $\Mcal$ is used for functions $g : \Xcal \mapsto \Zbb_{\geq 0}$ that map to non-negative integers from an arbitrary countable domain $\Xcal$. The output of the operator is
\begin{equation}
\Mcal(g) = \frac{\big( \sum_{x \in \Xcal} g(x) \big)!}{\prod_{x \in \Xcal} g(x)!} \;.
\end{equation}
Similarly, the beta function $\beta$ operates on functions $g : \Xcal \mapsto \Rbb^+$ that map to positive real numbers from an arbitrary countable domain $\Xcal$, such that
\begin{equation}
\beta(g) = \frac{\prod_{x \in \Xcal} \Gamma\big( g(x) \big)}{\Gamma \left( \sum_{x \in \Xcal} g(x) \right)} \;.
\end{equation}
Note that the countable domains of the input functions may have an infinite number of elements. These functions will also be used to operate on a subset of a functions' domain $\Scal \subset \Xcal$ and its corresponding image. Set notation for a function is used to express the argument, so that $\Mcal\Big( \big\{ g(x) : x \in S \big\} \Big)$ and $\beta\Big( \big\{ g(x) : x \in S \big\} \Big)$.

PGR: define subset functions, ditch set inputs? appendix?








\newpage

\chapter{Problem Statement}


\section{Data Model and Objective}

PGR: italic theta font before Bayes?

Consider an observable random element $\xrm \in \Xcal$ and and unobservable random element $\yrm \in \Ycal$ which are jointly distributed according to an unknown probability distribution $\uptheta \in \Theta = \Pcal(\Ycal \times \Xcal)$, such that $\Prm_{\yrm,\xrm | \uptheta} = \uptheta$. Note that the uppercase PMF notation used throughout this section implies that the random elements are discrete; PDF's are used when $\xrm$ and/or $\yrm$ are continuous random variables/processes.

PGR: DIM operator? in notation section?

PGR: consider definition/equivalence of D and Y,X. Think indexing.

Also observed is a random sequence of $N$ samples from $\uptheta$, denoted $\Drm \in \Dcal = \{\Ycal \times \Xcal\}^N$; an alternative representation that can be used is $\Drm \equiv ( \Yrm,\Xrm )$. The $N$ data pairs are conditionally independent from one another and are identically distributed as $\Prm_{\Drm_n | \uptheta} = \uptheta$. The samples are also conditionally independent from $(\yrm,\xrm)$. Thus,
\begin{equation}
\Prm_{\yrm,\xrm,\Drm | \uptheta}(y,x,D | \theta) = \Prm_{\yrm,\xrm | \uptheta}(y,x | \theta) \prod_{n=1}^N \Prm_{\Drm_n | \uptheta}\big(Y_n,X_n | \theta\big) \;.
\end{equation}

The task in supervised machine learning is to design a decision function $f: \Dcal \mapsto \Hcal^{\Xcal}$ which produces a mapping from the space of the observed random elements to a decision space $\Hcal$. Define the function space $\Fcal = \left\{ {\Hcal^{\Xcal}} \right\}^{\Dcal}$, such that $f \in \Fcal$. The learning functions are non-parametric and there are no restrictions on the set of achievable functions $\Fcal$.

The metric guiding the design is a loss function $\Lcal: \Hcal \times \Ycal \mapsto \Rbb_{\geq 0}$ which penalizes the decision $h \in \Hcal$ based on the value of $\yrm$. The objective is to minimize the conditional expected loss, or conditional ``risk'',
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond}
\Rcal_{\Theta}(f ; \uptheta) & = &  \Erm_{\yrm,\xrm,\Drm | \uptheta} \Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \\
& = & \Erm_{\xrm,\Drm | \uptheta} \bigg[ \Erm_{\yrm | \xrm,\uptheta} \Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \bigg] \nonumber \\
& = & \Erm_{\Drm | \uptheta}\Bigg[ \Erm_{\xrm | \uptheta}\bigg[ \Erm_{\yrm | \xrm,\uptheta}\Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \bigg] \Bigg] \nonumber \;.
\end{IEEEeqnarray}
where the conditional independence of random element $\yrm$ from the training data $\Drm$ given the model $\uptheta$ is used. As the model $\uptheta$ is not observed, $\Rcal_{\Theta}: \Theta \mapsto {\Rbb_{\geq 0}}^{\Fcal}$ is not a feasible objective function for optimization. This is the fundamental challenge of supervised learning: the true risk objective is unknown and the designer can never be precisely sure how well any decision function performs. 







\subsection{Clairvoyant Decision}

PGR: subscript Theta? Use theta sub and remove argument like a cond dist?

PGR: use marginal/conditional thetas?

It is informative to formulate the optimal decision function assuming the model $\uptheta$ was in fact observed; it will be referred to as the ``clairvoyant'' function, following terminology used in \cite{kay-det}. This clairvoyant decision function $f_{\Theta}: \Theta \mapsto \Fcal$ is represented by
\begin{equation}
f_{\Theta}(\uptheta) = \argmin_{f \in \Fcal} \Rcal_{\Theta}(f ; \uptheta) \;.
\end{equation}
For a given set of observations $\xrm$ and $\Drm$, the function $f_{\Theta}(\uptheta) \in \Fcal$ selects the decision $h = \argmin_{h \in \Hcal} \Erm_{\yrm | \xrm,\uptheta}\big[ \Lcal(h,\yrm) \big]$. Note the conditional independence of $\yrm$ from $\Drm$ in \eqref{eq:risk_cond} - the knowledge of $\uptheta$ renders the training data $\Drm$ uninformative. As such, the range of the clairvoyant function is recast as $f_{\Theta} : \Theta \mapsto \Hcal^{\Xcal}$ and the decisions are
\begin{equation} \label{eq:f_clv_x}
f_{\Theta}(\xrm;\uptheta) = \argmin_{h \in \Hcal} \Erm_{\yrm | \xrm,\uptheta}\big[ \Lcal(h,\yrm) \big] \;.
\end{equation}
The corresponding clairvoyant risk for a given model $\uptheta$ is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_clv}
\Rcal_{\Theta}^*(\uptheta) & \equiv & \Rcal_{\Theta}\big( f_{\Theta}(\uptheta) ; \uptheta \big) \\
& = & \min_{f \in \Fcal} \Rcal_{\Theta}(f ; \uptheta) \nonumber \\
& = & \Erm_{\xrm | \uptheta} \left[ \min_{h \in \Hcal} \Erm_{\yrm | \xrm,\uptheta}\big[ \Lcal(h,\yrm) \big] \right] \nonumber \;.
\end{IEEEeqnarray}




\subsection{Bayes Decision}

To design an optimal decision function $f \in \Fcal$, an operator must be chosen to remove the dependency of the conditional risk on $\uptheta$ and form an objective function $\Fcal \mapsto \Rbb_{\geq 0}$. One choice is to integrate over $\Theta$; to ensure a non-negative objective value, the weighting function should be non-negative. Also, as scaling the objective function will not change its minimizing argument, the weighting function can be constrained to integrate to one. These are the requirements for a valid probability density function (PDF); as such, the model $\uptheta$ is treated as a random process and a Bayesian approach can be adopted. 

Define the PDF $\prm_{\uptheta} \in \Pcal(\Theta)$. Now the Bayes risk can be formulated as
\begin{IEEEeqnarray}{rCl} \label{eq:risk}
\Rcal(f) & = & \Erm_{\uptheta}\big[ \Rcal_{\Theta}(f ; \uptheta) \big] \\
& = & \Erm_{\yrm,\xrm,\Drm}\big[ \Lcal(f(\xrm;\Drm),\yrm) \big] \nonumber \\
& = & \Erm_{\xrm,\Drm}\bigg[ \Erm_{\yrm | \xrm,\Drm} \Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \bigg] \nonumber \\
& = & \Erm_{\Drm}\Bigg[ \Erm_{\xrm | \Drm}\bigg[ \Erm_{\yrm | \xrm,\Drm} \Big[ \Lcal\big( f(\xrm;\Drm),\yrm \big) \Big] \bigg] \Bigg] \nonumber
\end{IEEEeqnarray}
and $\yrm$, $\xrm$, and $\Drm$ are treated as jointly distributed random elements. Observe that the Bayesian predictive distributions can be represented as $\Prm_{\xrm | \Drm} = \Erm_{\uptheta | \Drm}\big[ \Prm_{\xrm | \uptheta} \big]$ and $\Prm_{\yrm | \xrm,\Drm} = \Erm_{\uptheta | \xrm,\Drm}\big[ \Prm_{\yrm | \xrm,\uptheta} \big]$, the expected values of the corresponding clairvoyant distributions with respect to the model posteriors $\prm_{\uptheta | \Drm}$ and $\prm_{\uptheta | \xrm,\Drm}$, respectively.

PGR: BELOW, add forumla for f(D)?

Finally, express the optimal learning function
\begin{equation} 
f^* = \argmin_{f \in \Fcal} \Rcal(f) \;.
\end{equation}
The decision expressed by the learning function $f^*$ given observed values of $\xrm$ and $\Drm$ is
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_xD}
f^*(\xrm;\Drm) & = & \argmin_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm}\big[ \Lcal(h,\yrm) \big] \\
& = & \argmin_{h \in \Hcal} \Erm_{\uptheta | \xrm,\Drm}\bigg[ \Erm_{\yrm | \xrm,\uptheta}\big[ \Lcal(h,\yrm) \big] \bigg] \nonumber \;.
\end{IEEEeqnarray}
Thus, the Bayesian approach uses the model posterior $\prm_{\uptheta | \xrm,\Drm}$ to integrate out the dependency on the model given the observable random elements. The minimum Bayes risk is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_min}
\Rcal^* & \equiv & \Rcal(f^*) \\
 & = & \min_{f \in \Fcal} \Rcal(f) \nonumber \\
& = & \Erm_{\xrm,\Drm} \left[ \min_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm}\big[ \Lcal(h,\yrm) \big] \right] \nonumber \\
& = & \Erm_{\Drm} \Bigg[ \Erm_{\xrm | \Drm} \bigg[ \min_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm}\big[ \Lcal(h,\yrm) \big] \bigg] \Bigg] \nonumber \;.
\end{IEEEeqnarray}



\subsubsection{Irreducible Risk}

PGR: RECONSIDER IRREDUCIBLE TERMINOLOGY!!!!!!

PGR: bound met in limit of N? full support? bounded prior? change/move to clairvoyant discussion?

The clairvoyant risk \eqref{eq:risk_clv} for a given model satisfies $\Rcal_{\Theta}^*(\theta) \leq \Rcal_{\Theta}(f;\theta) \quad \forall f \in \Fcal, \ \theta \in \Theta$. Consequently, the Bayes risk satisfies $\Erm_{\uptheta} \big[ \Rcal_{\Theta}^*(\uptheta) \big] \leq \Rcal(f) \quad \forall f \in \Fcal$; the expected value of the clairvoyant risk will thus be referred to as the ``irreducible'' risk. 

It is important to note that this inequality holds for any number of training samples $N$ and that the irreducible risk does not depend on $N$. Thus, even with unlimited training data, no learning function can provide a Bayes risk lower than this value.








\section{Sufficient Statistic: the Empirical PMF}

PGR: MOVE SECTION before Bayesian approach??

PGR: continuous? DMP?

PGR: change to emp PMF RP $\psi$?

PGR: trends in limit of N, use mean/cov?? EMPIRICAL RISK, bounded prior

For countable sets $\Ycal$ and $\Xcal$, the distribution of $\Drm$ conditioned on the model can be formulated as
\begin{IEEEeqnarray}{rCl}
\Prm_{\Drm | \uptheta}\big( D | \theta \big) & = & \prod_{n=1}^N \Prm_{\Drm_n | \uptheta}\big( D_n | \theta \big) \\
& = & \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\bar{N}(y,x;D)} \nonumber \;,
\end{IEEEeqnarray}
where the dependency on the training data $\Drm$ is expressed though a transform function $\bar{N} : \Dcal \mapsto \bar{\Ncal}$, where the range is 
\begin{IEEEeqnarray}{rCl}
\bar{\Ncal} & = & \left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal \times \Xcal}: \sum_{y \in \Ycal} \sum_{x \in \Xcal} \bar{n}(y,x) = N \right\} 
\end{IEEEeqnarray}
and the function is defined as
\begin{IEEEeqnarray}{rCl}
\bar{N}(y,x;D) & = & \sum_{n=1}^N \delta \big[ (y,x),D_n \big] \\
& = & \sum_{n=1}^N \delta \left[ y,Y_n \right] \delta \left[ x,X_n \right] \nonumber \;.
\end{IEEEeqnarray}
This function counts the number of occurrences of the pair $(y,x)$ in the training set $D$. 

PGR: show SS via Kay, data likelihood??!!

Note that $\Prm_{\Drm | \uptheta}$ depends on the training data $\Drm$ only through the transform $\bar{N}$; $\bar{N}(\Drm)$ is thus a sufficient statistic \cite{bernardo} for the model $\uptheta$. Consequently, other distributions of interest $\Prm_{\Drm}$, $\Prm_{\xrm | \Drm}$, and $\Prm_{\yrm | \xrm,\Drm}$ will also depend on $\Drm$ via $\bar{N}(\Drm)$. As such, it is useful to define a new random process $\nbarrm \equiv \bar{N}(\Drm) \in \bar{\Ncal}$. 

Frequently, the corresponding distributions $\Prm_{\nbarrm}$, $\Prm_{\xrm | \nbarrm}$, and $\Prm_{\yrm | \xrm,\nbarrm}$ will be used to find the optimal decision functions and the minimum risk. Note that $\Mcal\big( \bar{N}(D) \big) \Prm_{\Drm | \uptheta}(D | \theta) = \Prm_{\nbarrm | \uptheta}\big( \bar{N}(D) | \theta \big)$, where $\Mcal$ is the multinomial operator. Also note that $\Prm_{\xrm | \Drm}(D) = \Prm_{\xrm | \nbarrm}\big( \bar{N}(D) \big)$ and $\Prm_{\yrm | \xrm,\Drm}(x,D) = \Prm_{\yrm | \xrm,\nbarrm}\big( x,\bar{N}(D) \big)$.

The cardinality of the random process' domain is $|\bar{\Ncal}| = \Mcal\big( \{N,|\Ycal||\Xcal|-1\} \big)$; this can be shown using the stars-and-bars method \cite{feller}. The cardinality of original set is $|\Dcal| = \big( |\Ycal| |\Xcal| \big)^N$; thus $|\bar{\Ncal}| \leq |\Dcal|$ and the sufficient statistic compactly represents the valuable information in the training data. Also, observe that the set $\{ \bar{n}/N : \bar{n} \in \bar{\Ncal} \} \subset \Theta$ and thus that the empirical distribution $\bar{N}(\Drm)/N$ assumes one of a finite number of the elements from $\Theta$.

PGR: sufficient statistic savings in memory bits???

Conditioned on the model $\uptheta$, the PMF of $\nbarrm$ is a multinomial distribution
\begin{IEEEeqnarray}{rCl}
\Prm_{\nbarrm | \uptheta}(\bar{n} | \theta) & = & \sum_{D : \bar{N}(D) = \bar{n}} \Prm_{\Drm | \uptheta}(D | \theta) \\
& = & \big|\{ D : \bar{N}(D) = \bar{n} \}\big| \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\bar{n}(y,x)} \nonumber \\
& = & \Mcal(\bar{n}) \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\bar{n}(y,x)} \nonumber \\
& = & \Multi\big( \bar{n};N,\theta \big) \nonumber \;,
\end{IEEEeqnarray}
where the multinomial operator $\Mcal$ is used.  

The first and second joint moments of this multinomial distribution are \cite{theodoridis-ML}
\begin{IEEEeqnarray}{rCl}
\mu_{\bar{\nrm} | \uptheta} & = & N \uptheta
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{L}
\Erm_{\bar{\nrm} | \uptheta}\big[ \bar{\nrm}(y,x) \bar{\nrm}(y',x') \big] \\
\quad = N \big( \uptheta(y,x) \delta[y,y'] \delta[x,x'] + (N-1) \uptheta(y,x) \uptheta(y',x') \big) \nonumber
\end{IEEEeqnarray}
and the covariance function is
\begin{IEEEeqnarray}{rCl}
\Sigma_{\nbarrm | \uptheta}(y,x,y',x')  & = & N \big( \uptheta(y,x) \delta[y,y'] \delta[x,x'] - \uptheta(y,x) \uptheta(y',x') \big) \;.
\end{IEEEeqnarray}

PGR: trends with N? delta? Figures??

Also, observe that the maximum likelihood estimate of $\theta$ given the training statistic is \cite{rao},
\begin{IEEEeqnarray}{rCl}
\theta_\mathrm{ML}\big( \bar{n} \big) & = & \argmax_{\theta \in \Theta} \Prm_{\nbarrm | \uptheta}(\bar{n} | \theta) \\ \nonumber
& = & \frac{\bar{n}}{N} \;,
\end{IEEEeqnarray}
the empirical distribution.



\section{Marginal/Conditional Distributions for Observed/Unobserved Elements}

PGR: move section?

PGR: clean up

\subsection{Marginal and Conditional Distributions of $\uptheta$}

PGR: are they independent??? No?!

PGR: bijection!!

As only $\yrm$ is unobservable, it will be useful to decompose the model distribution as $\uptheta \equiv (\uptheta',\tilde{\uptheta})$. First, introduce the marginal distribution $\uptheta' \equiv \sum_{y \in \Ycal} \uptheta(y,\cdot) \in \Pcal(\Xcal)$; note that the summation is replaced by an integral when $\yrm$ is a continuous random variable. Next, introduce the conditional distributions $\tilde{\uptheta} \in \Pcal(\Ycal)^{\Xcal}$ defined as $\tilde{\uptheta}(x) \equiv \uptheta(\cdot,x) / \uptheta'(x)$. 

This decomposition enables the clairvoyant distributions to be represented as $\Prm_{\xrm | \uptheta} = \Prm_{\xrm | \uptheta'} = \uptheta'$ and $\Prm_{\yrm | \xrm,\uptheta} = \Prm_{\yrm | \xrm,\tilde{\uptheta}} = \tilde{\uptheta}(\xrm)$; these distributions will be of recurring importance. 

PGR: conditional theta condition on x necessary above? Yes?!

PGR: marginal theta conditional on X, not full D above???? No!?



\subsection{Marginal and Conditional Distributions of Training Data}

Also of interest are the marginal and conditional distributions of the joint training data partitions $\Yrm$ and $\Xrm$. The marginal distribution given $\uptheta$ for the observations $\Xrm$ alone is
\begin{IEEEeqnarray}{rCl}
\Prm_{\Xrm | \uptheta}\big( X | \theta \big) & = & \prod_{n=1}^N \Prm_{\Xrm_n | \uptheta}\big( X_n | \theta \big) \\
& = & \prod_{x \in \Xcal} \theta'(x)^{N'(x;X)} \nonumber \;,
\end{IEEEeqnarray}
where the dependency on $\uptheta$ is only through the marginal model $\uptheta'$. Additionally, note that the dependency on the training observations $\Xrm$ is expressed though a ``marginal'' counting function $N' : \Xcal \mapsto \Ncal'$ with range
\begin{IEEEeqnarray}{rCl}
\Ncal' & = & \left\{ n' \in {\Zbb_{\geq 0}}^{\Xcal}: \sum_{x \in \Xcal} n'(x) = N \right\} \;,
\end{IEEEeqnarray}
defined as
\begin{IEEEeqnarray}{rCl}
N'(X) & = & \sum_{n=1}^N \delta\big[ \cdot,X_n \big] \equiv \sum_{y \in \Ycal} \bar{N}(y,\cdot;D) \;.
\end{IEEEeqnarray}
The conditional distribution of the values $\Yrm$ given the corresponding $\Xrm$ can be found using Bayes theorem as
\begin{IEEEeqnarray}{rCl}
\Prm_{\Yrm | \Xrm,\uptheta}\big( Y | X,\theta \big) & = & \prod_{n=1}^N \frac{\Prm_{\Yrm_n,\Xrm_n | \uptheta}\big( Y_n,X_n | \theta \big)}{\Prm_{\Xrm_n | \uptheta}\big( X_n | \theta \big)} \\
& = & \prod_{x \in \Xcal} \left[ \prod_{y \in \Ycal} \tilde{\theta}(y;x)^{\bar{N}(y,x;Y,X)} \right] \nonumber \;,
\end{IEEEeqnarray}
which is dependent on the model $\uptheta$ only through the conditional models $\tilde{\uptheta}(x)$.


As before, the dependency on the training data can be simplified using a sufficient statistic. Introduce the ``marginalized'' random process $\nrm'$ over the set $\Xcal$, defined as $\nrm' \equiv \sum_{y \in \Ycal} \nbarrm(y,\cdot) \equiv N'(\Xrm) \in \Ncal'$. By the aggregation property of Multinomial random processes \cite{johnson}, the aggregation conditioned on the model $\uptheta$ is distributed as $\nrm' | \uptheta \sim \Multi(N,\uptheta')$. 

Also of interest is the distribution of $\nbarrm$ conditioned on its aggregation $\nrm'$. Using the multinomial distribution properties proven in Appendix \ref{app:mult}, it can be shown that when conditioned on the model $\uptheta$ as well, the PMF of $\nbarrm$ is
\begin{IEEEeqnarray}{rCl}
\Prm_{\bar{\nrm} | \nrm' , \uptheta}(\bar{n} | n' , \theta) & = & \prod_{x \in \Xcal} \Bigg[ \Mcal\big( \bar{n}(\cdot,x) \big) \prod_{y \in \Ycal} \tilde{\theta}(y;x)^{\bar{n}(y,x)} \Bigg] \\
& = & \prod_{x \in \Xcal} \Multi\Big( \bar{n}(\cdot,x) ; n'(x) , \tilde{\theta}(x) \Big) \nonumber \;,
\end{IEEEeqnarray}
over the domain $\left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal \times \Xcal} : \sum_{y \in \Ycal} \bar{n}(y,\cdot) = n' \right\}$. Observe that conditioning on the aggregation renders the function segments $\nbarrm(\cdot,x)$ independent of one another and that they are also Multinomial, such that $\nbarrm(\cdot,x) | \nrm'(x),\uptheta \sim \Multi\big( \nrm'(x),\tilde{\uptheta}(x) \big)$. Furthermore, the dependency on $\uptheta$ is expressed through the conditional model $\tilde{\uptheta}$.





\subsection{Model Posteriors}

PGR: clean, discuss

The Bayesian predicitive distributions analagous to the clairvoyant distributions can now be simplified as $\Prm_{\xrm | \Drm} = \mu_{\uptheta' | \Drm}$ and $\Prm_{\yrm | \xrm,\Drm} = \mu_{\tilde{\uptheta}(\xrm) | \xrm,\Drm}$.

PGR: use $\Prm_{\yrm | \xrm,\Drm} = \Erm_{\tilde{\uptheta} | \xrm,\Drm}\big[ \tilde{\uptheta}(\xrm) \big]$ instead???


\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta',\tilde{\uptheta} | \Yrm,\Xrm}(\theta',\tilde{\theta} | Y,X) & = & \frac{\Prm_{\Yrm | \Xrm,\tilde{\uptheta}}(Y | X,\tilde{\theta})}{\Prm_{\Yrm | \Xrm}(Y | X)} 
\frac{\Prm_{\Xrm | \uptheta'}(X | \theta')}{\Prm_{\Xrm}(X)} \prm_{\uptheta',\tilde{\uptheta}}(\theta',\tilde{\theta}) \\
& = & \prm_{\tilde{\uptheta} | \Yrm,\Xrm}(\tilde{\theta} | Y,X) \prm_{\uptheta' | \Xrm}(\theta' | X) 
\frac{\prm_{\tilde{\uptheta} | \uptheta'}(\tilde{\theta} | \theta')}{\prm_{\tilde{\uptheta} | \Xrm}(\tilde{\theta} | X)} \nonumber
\end{IEEEeqnarray}

%\begin{IEEEeqnarray}{rCl}
%\prm_{\uptheta' | \Yrm,\Xrm}(\theta' | Y,X) & = & \frac{\Erm_{\tilde{\uptheta} | \uptheta'} \left[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}}(Y | X,\tilde{\theta}) \right]}{\Prm_{\Yrm | \Xrm}(Y | X)} 
%\frac{\Prm_{\Xrm | \uptheta'}(X | \theta')}{\Prm_{\Xrm}(X)} \prm_{\uptheta'}(\theta') \\
%& = & \prm_{\uptheta' | \Xrm}(\theta' | X) 
%\Erm_{\tilde{\uptheta} | \uptheta'} \left[ \frac{\prm_{\tilde{\uptheta} | \Yrm,\Xrm}(\tilde{\theta} | Y,X)}{\prm_{\tilde{\uptheta} | \Xrm}(\tilde{\theta} | X)} \right] \nonumber
%\end{IEEEeqnarray}

\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta',\tilde{\uptheta} | \nbarrm}(\theta',\tilde{\theta} | \bar{n}) & = & \frac{\Prm_{\nbarrm | \nrm',\tilde{\uptheta}}\big( \bar{n} | \sum_y \bar{n}(y,\cdot),\tilde{\theta} \big)}{\Prm_{\nbarrm | \nrm'}\big( \bar{n} | \sum_y \bar{n}(y,\cdot) \big)} 
\frac{\Prm_{\nrm' | \uptheta'}\big( \sum_y \bar{n}(y,\cdot) | \theta' \big)}{\Prm_{\nrm'}\big( \sum_y \bar{n}(y,\cdot) \big)} \prm_{\uptheta',\tilde{\uptheta}}(\theta',\tilde{\theta}) \\
& = & \prm_{\tilde{\uptheta} | \nbarrm,\nrm'}\Big( \tilde{\theta} | \bar{n},\sum_y \bar{n}(y,\cdot) \Big) \prm_{\uptheta' | \nrm'}\Big( \theta' | \sum_y \bar{n}(y,\cdot) \Big) 
\frac{\prm_{\tilde{\uptheta} | \uptheta'}(\tilde{\theta} | \theta')}{\prm_{\tilde{\uptheta} | \nrm'}\big( \tilde{\theta} | \sum_y \bar{n}(y,\cdot) \big)} \nonumber
\end{IEEEeqnarray}

% Note that $\prm_{\tilde{\uptheta} | \Xrm}(X) = \prm_{\tilde{\uptheta} | \nrm'}\big( N'(X) \big) = \Erm_{\uptheta' | \Xrm}\big[ \prm_{\tilde{\uptheta} | \uptheta'} \big](X)$.

Note that $\prm_{\tilde{\uptheta} | \Xrm} = \Erm_{\uptheta' | \Xrm}\big[ \prm_{\tilde{\uptheta} | \uptheta'} \big]$ and $\prm_{\tilde{\uptheta} | \nrm'} = \Erm_{\uptheta' | \nrm'}\big[ \prm_{\tilde{\uptheta} | \uptheta'} \big]$. Also, note that $\Prm_{\Yrm | \Xrm} = \Erm_{\uptheta' | \Xrm} \Big[ \Erm_{\tilde{\uptheta} | \uptheta'} \big[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}} \big] \Big]$ and $\Prm_{\nbarrm | \nrm'} = \Erm_{\uptheta' | \nrm'} \Big[ \Erm_{\tilde{\uptheta} | \uptheta'} \big[ \Prm_{\nbarrm | \nrm',\tilde{\uptheta}} \big] \Big]$.


\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta',\tilde{\uptheta} | \Yrm,\Xrm,\xrm}(\theta',\tilde{\theta} | Y,X,x) & = & \frac{\Prm_{\Yrm | \Xrm,\tilde{\uptheta}}(Y | X,\tilde{\theta})}{\Prm_{\Yrm | \Xrm,\xrm}(Y | X,x)} 
\frac{\Prm_{\Xrm,\xrm | \uptheta'}(X,x | \theta')}{\Prm_{\Xrm,\xrm}(X,x)} \prm_{\uptheta',\tilde{\uptheta}}(\theta',\tilde{\theta}) \\
& = & \prm_{\tilde{\uptheta} | \Yrm,\Xrm}(\tilde{\theta} | Y,X) \prm_{\uptheta' | \Xrm,\xrm}(\theta' | X,x) 
\frac{\Prm_{\Yrm | \Xrm}(Y | X)}{\Prm_{\Yrm | \Xrm,\xrm}(Y | X,x)}
\frac{\prm_{\tilde{\uptheta} | \uptheta'}(\tilde{\theta} | \theta')}{\prm_{\tilde{\uptheta} | \Xrm}(\tilde{\theta} | X)} \nonumber
\end{IEEEeqnarray}

\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta',\tilde{\uptheta} | \nbarrm,\xrm}(\theta',\tilde{\theta} | \bar{n},x) & = & \frac{\Prm_{\nbarrm | \nrm',\tilde{\uptheta}}\big( \bar{n} | \sum_y \bar{n}(y,\cdot),\tilde{\theta} \big)}{\Prm_{\nbarrm | \nrm',\xrm}\big( \bar{n} | \sum_y \bar{n}(y,\cdot),x \big)} 
\frac{\Prm_{\nrm',\xrm | \uptheta'}\big( \sum_y \bar{n}(y,\cdot),x | \theta' \big)}{\Prm_{\nrm',\xrm}\big( \sum_y \bar{n}(y,\cdot),x \big)} \prm_{\uptheta',\tilde{\uptheta}}(\theta',\tilde{\theta}) \\
& = & \prm_{\tilde{\uptheta} | \nbarrm,\nrm'}\Big( \tilde{\theta} | \bar{n},\sum_y \bar{n}(y,\cdot) \Big) \prm_{\uptheta' | \nrm',\xrm}\Big( \theta' | \sum_y \bar{n}(y,\cdot),x \Big) \nonumber \\
&& \quad \frac{\Prm_{\nbarrm | \nrm'}\big( \bar{n} | \sum_y \bar{n}(y,\cdot) \big)}{\Prm_{\nbarrm | \nrm',\xrm}\big( \bar{n} | \sum_y \bar{n}(y,\cdot),x \big)} \frac{\prm_{\tilde{\uptheta} | \uptheta'}(\tilde{\theta} | \theta')}{\prm_{\tilde{\uptheta} | \nrm'}\big( \tilde{\theta} | \sum_y \bar{n}(y,\cdot) \big)} \nonumber
\end{IEEEeqnarray}

Note that $\Prm_{\Yrm | \Xrm,\xrm} = \Erm_{\uptheta' | \Xrm,\xrm} \Big[ \Erm_{\tilde{\uptheta} | \uptheta'} \big[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}} \big] \Big]$ and also that $\Prm_{\nbarrm | \nrm',\xrm} = \Erm_{\uptheta' | \nrm',\xrm} \Big[ \Erm_{\tilde{\uptheta} | \uptheta'} \big[ \Prm_{\nbarrm | \nrm',\tilde{\uptheta}} \big] \Big]$.









\section{Applications to Common Loss Functions}

PGR: equation label conflicts??

PGR: marginal/condtitional???

PGR: sum vs int???

PGR: move before nbar, keep D?



In this section, loss functions typical for classification and regression applications, specifically the 0-1 loss function and the squared-error loss function, are adopted. The conditional risk \eqref{eq:risk_cond} is assessed, clairvoyant learners \eqref{eq:f_clv_x} are found, and the clairvoyant risk \eqref{eq:risk_clv} is expressed.



\subsection{Regression: the Squared-Error Loss}

The squared-error (SE) loss function is arguably the most commonly used loss function for regression, or in fact for any estimation problem. This can be attributed to its quadratic form, which enables a closed-form expression of the minimizing estimation function.

It is assumed that the unobserved random element $\yrm$ is a scalar random variable; that is, $\Ycal \subseteq \Rbb$. Additionally, the learning function's estimate is allowed to assume real numbers; thus, $\Hcal = \Rbb \supseteq \Ycal$.

The loss function is defined as
\begin{equation}
\Lcal(h,y) = (h-y)^2 \;.
\end{equation}
Substituting the squared-error loss into \eqref{eq:risk_cond}, the conditional squared-error risk is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_SE}
\Rcal_{\Theta}(f ; \uptheta) & = & \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm,\xrm | \uptheta} \Big[ \big( f(\xrm;\Drm)-\yrm \big)^2 \Big] \bigg] \\
& = & \Erm_{\xrm | \uptheta}\Bigg[ \Erm_{\yrm| \xrm,\uptheta}\bigg[ \Erm_{\Drm | \uptheta}\Big[ \big( f(\xrm;\Drm)-\yrm \big)^2 \Big] \bigg] \Bigg] \nonumber \\
& = & \Erm_{\xrm | \uptheta} \Big[ \Erm_{\yrm | \xrm,\uptheta} \big[ (\yrm - \mu_{\yrm | \xrm,\uptheta})^2 \big] \Big] + \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \nonumber \\
& = & \Erm_{\xrm | \uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] + \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \nonumber \;,
\end{IEEEeqnarray}
a sum of two terms. The first term is the expected conditional variance of the true predictive distribution $\Prm_{\yrm | \xrm,\uptheta}$. The second term is the expected squared bias between the Bayesian estimate and the true mean $\mu_{\yrm | \xrm,\uptheta}$.





\subsubsection{Clairvoyant Estimation}

To find the clairvoyant estimator, the squared-error loss is substituted into \eqref{eq:f_clv_x}; note that the objective function is quadratic over the argument $h \in \Hcal = \Rbb$. It is easily shown that the function over $h$ is positive-definite; as such, the minimizing decision $h$ is the sole stationary point. Setting the first derivative of the function to zero, the clairvoyant estimate is the expected value of $\yrm$ given the model $\uptheta$ and the observed value $\xrm$, such that
\begin{IEEEeqnarray}{rCl} \label{eq:f_clv_SE}
f_{\Theta}(\xrm;\uptheta) & = & \argmin_{h \in \Rbb} \Erm_{\yrm | \xrm,\uptheta} \left[ (h-\yrm)^2 \right] \\
& = &  \mu_{\yrm | \xrm,\uptheta} = \sum_{y \in \Ycal} y \tilde{\uptheta}(y;\xrm) \nonumber \;.
\end{IEEEeqnarray}
Substituting the loss and clairvoyant function into \eqref{eq:risk_clv}, the resulting clairvoyant risk is
\begin{IEEEeqnarray}{rCl}
\Rcal_{\Theta}^*(\uptheta) & = & \Erm_{\xrm | \uptheta} \Big[ \Erm_{\yrm | \xrm,\uptheta} \big[ (\yrm - \mu_{\yrm | \xrm,\uptheta})^2 \big] \Big] \\
& = & \Erm_{\xrm | \uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] \nonumber \;.
\end{IEEEeqnarray}

Observe that the general conditional risk \eqref{eq:risk_cond_SE} can be represented as $\Rcal_{\Theta}(f ; \uptheta) = \Rcal_{\Theta}^*(\uptheta) + \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - f_{\Theta}(\xrm;\uptheta) \big)^2 \Big]$. The first summand is equal to the clairvoyant squared-error; the second term is dependent on the difference between the general estimate and the clairvoyant estimate. 


Figure \ref{fig:Risk_clv_SE_tilde} displays the clairvoyant risk for predictive models $\tilde{\theta}(x)$ independent of $x$. 
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_clv_SE_tilde.pdf}
\caption{Clairvoyant Risk for the Squared-Error Loss Function, constant $\tilde{\theta}(x)$}
\label{fig:Risk_clv_SE_tilde}
\end{figure}





\subsubsection{Bayesian Estimation}

\paragraph{Optimal Estimate: the Posterior Mean}

PGR: plots?

To find the optimal estimator, the squared-error loss is substituted into \eqref{eq:f_opt_xD}. Again, the function over $h$ is positive-definite; as such, the minimizing decision $h$ is the sole stationary point. Setting the first derivative of the function to zero, the optimal estimate is the expected value of $\yrm$ given the training data and the observed value $\xrm$, such that
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_SE}
f^*(\xrm;\Drm) & = & \argmin_{h \in \Rbb} \Erm_{\yrm | \xrm,\Drm} \left[ (h-\yrm)^2 \right] \\
& = & \mu_{\yrm | \xrm,\Drm} = \Erm_{\uptheta | \xrm,\Drm} \left[ \mu_{\yrm | \xrm,\uptheta} \right] \nonumber \;.
\end{IEEEeqnarray}

An interesting form for the optimal estimator is $f^*(\xrm;\Drm) = \Erm_{\uptheta | \xrm,\Drm} \big[ f_{\Theta}(\xrm;\uptheta) \big]$. Substituting the squared-error loss into the second line of \eqref{eq:f_opt_xD}, the optimal Bayes estimator is the condtional expected value of the clairvoyant estimate with respect to the model posterior distribution.







\paragraph{Minimum Risk: the Expected Posterior Variance}

The Bayes squared-error risk for a general learning function is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_SE}
\Rcal(f) & = & \Erm_{\uptheta} \Bigg[ \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm,\xrm | \uptheta} \Big[ \big( f(\xrm;\Drm)-\yrm \big)^2 \Big] \bigg] \Bigg] \\
& = & \Erm_{\xrm,\Drm} \bigg[ \Erm_{\yrm | \xrm,\Drm} \Big[ \big( f(\xrm;\Drm)-\yrm \big)^2 \Big] \bigg] \nonumber \\
& = & \Erm_{\uptheta}\big[\Rcal_{\Theta}^*(\uptheta)\big] + \Erm_{\xrm,\Drm,\uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \nonumber \\
& = & \Erm_{\xrm,\Drm} \left[ \Sigma_{\yrm | \xrm,\Drm} \right] + \Erm_{\xrm,\Drm} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\Drm} \big)^2 \Big] \nonumber \;.
\end{IEEEeqnarray}

Substituting the optimal estimator \eqref{eq:f_opt_SE} into Equation \eqref{eq:risk_SE}, the minimum Bayes risk is the expected conditional variance
\begin{IEEEeqnarray}{rCl} \label{eq:risk_min_SE}
\Rcal^* & = & \Erm_{\xrm,\Drm} \left[ \Sigma_{\yrm | \xrm,\Drm} \right] \\
& = & \Erm_{\xrm,\uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] + \Erm_{\xrm,\Drm} \left[ \Crm_{\uptheta | \xrm,\Drm} \left[ \mu_{\yrm | \xrm,\uptheta} \right] \right] \nonumber \\
& = & \Erm_{\uptheta}\big[\Rcal_{\Theta}^*(\uptheta)\big] + \Erm_{\xrm,\Drm} \Big[ \Crm_{\uptheta | \xrm,\Drm} \big[ f_{\Theta}(\xrm;\uptheta) \big] \Big] \nonumber \;.
\end{IEEEeqnarray}
The first term is the irreducible risk. The second term is the expected variance of the clairvoyant estimate $f_{\Theta}(\xrm;\uptheta) = \mu_{\yrm | \xrm,\uptheta}$ with respect to the model posterior PDF $\prm_{\uptheta | \xrm,\Drm}$


PGR: assess irreducible risk by performing theta agg conditioning???






\subsection{Classification: the 0-1 Loss}

In this section, the developed framework is applied to a common machine learning task: classification. In classification problems, the set $\Ycal$ is countable and typically finite. Furthermore, the hypothesis space is usually identical to the unobserved variable space; that is $\Hcal = \Ycal$. The 0-1 loss function is the most widely used for these problems; it is represented as
\begin{equation} \label{eq:loss_01}
\Lcal(h,y) = 1 - \delta[h,y] \;.
\end{equation}
Applying the 0-1 loss, the conditional risk \eqref{eq:risk_cond} for a general classifier is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_01}
\Rcal_{\Theta}(f ; \uptheta) & = & 1 - \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm,\xrm | \uptheta} \Big[ \delta\big[ f(\xrm;\Drm),\yrm \big] \Big] \bigg] \\
& = & 1 - \sum_{x \in \Xcal} \Erm_{\Drm | \uptheta} \Big[ \uptheta\big( f(x;\Drm),x \big) \Big] \nonumber \\
& = & 1 - \Erm_{\xrm | \uptheta} \bigg[ \Erm_{\Drm | \uptheta} \Big[ \Prm_{\yrm | \xrm,\uptheta}\big( f(\xrm;\Drm) | \xrm,\uptheta \big) \Big] \bigg] \nonumber \\
& = & 1 - \sum_{x \in \Xcal} \uptheta'(x) \Erm_{\Drm | \uptheta} \Big[ \tilde{\uptheta}\big( f(x;\Drm);x \big) \Big] \nonumber \;.
\end{IEEEeqnarray}



\subsubsection{Clairvoyant Hypothesis}

To find the clairvoyant classifier, the 0-1 loss is substituted into \eqref{eq:f_clv_x}; given an observation $x$, the optimum hypothesis is simply the value $y$ that maximizes the conditional model $\tilde{\uptheta}(x)$,
\begin{IEEEeqnarray}{rCl} \label{eq:f_clv_01}
f_{\Theta}(\xrm;\uptheta) & = & \argmin_{h \in \Ycal} \Erm_{\yrm | \xrm,\uptheta} \left[ 1 - \delta[h,y] \right] \\
& = & \argmax_{h \in \Ycal} \Prm_{\yrm | \xrm,\uptheta}(h | \xrm,\uptheta) \nonumber \\
& = & \argmax_{y \in \Ycal} \tilde{\uptheta}(y;\xrm) \nonumber \\
& = & \argmax_{y \in \Ycal} \uptheta(y,\xrm) \nonumber \;.
\end{IEEEeqnarray}
Substituting the 0-1 loss and clairvoyant hypothesis into \eqref{eq:risk_clv}, the resulting clairvoyant risk is
\begin{IEEEeqnarray}{rCl}
\Rcal_{\Theta}^*(\uptheta) & = & 1 - \Erm_{\xrm | \uptheta} \Big[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\uptheta}(y | \xrm,\uptheta) \Big] \\
& = & 1 - \sum_{x \in \Xcal} \uptheta'(x) \max_{y \in \Ycal} \tilde{\uptheta}(y;x) \nonumber \\
& = & 1 - \sum_{x \in \Xcal} \max_{y \in \Ycal} \uptheta(y,x) \nonumber \;.
\end{IEEEeqnarray}

Figure \ref{fig:Risk_clv_01_tilde} displays the clairvoyant risk for predictive models $\tilde{\theta}(x)$ independent of $x$. Intuitively, the models that are more concentrated lead to lower probability of error.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_clv_01_tilde.pdf}
\caption{Clairvoyant Risk for the 0--1 Loss Function, constant $\tilde{\theta}(x)$}
\label{fig:Risk_clv_01_tilde}
\end{figure}



\subsubsection{Bayesian Classification}

\paragraph{Optimal Hypothesis: Conditional Maximum \emph{a posteriori}}

PGR: decision region figures??

To determine the optimal learning function, the 0-1 loss from Equation \eqref{eq:loss_01} is substituted into Equation \eqref{eq:f_opt_xD} to find
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_01}
f^*(\xrm;\Drm) & = & \argmin_{h \in \Ycal} \Erm_{\yrm | \xrm,\Drm}\big[ 1 - \delta[h,\yrm] \big] \\
& = & \argmax_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \nonumber \;.
\end{IEEEeqnarray}
The optimal classifier chooses the value $y \in \Ycal$ that maximizes the conditional PMF for the observed values of $\xrm$ and $\Drm$.



\paragraph{Minimum Risk: Probability of Error}

Using the 0-1 loss, the Bayes probability of error \eqref{eq:risk_min} is
\begin{IEEEeqnarray}{rCl} \label{eq:risk_01}
\Rcal(f) & = & 1 - \Erm_{\xrm,\Drm} \left[ \Prm_{\yrm | \xrm,\Drm}\big( f(\xrm;\Drm) | \xrm,\Drm \big) \right] \;.
\end{IEEEeqnarray}

Substituting the optimal learner \eqref{eq:f_opt_01} into the general risk \eqref{eq:risk_01}, the minimum probability of error is 
\begin{IEEEeqnarray}{rCl} \label{eq:risk_min_01}
\Rcal^* & = & 1 - \Erm_{\xrm,\Drm} \left[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \right] \;.
\end{IEEEeqnarray}


















\newpage

\chapter{Finite Dirichlet Model}

This chapter demonstrates the optimal decision functions when the sets $\Ycal$ and $\Xcal$ have a finite number of elements and the model $\uptheta$ is characterized by a Dirichlet distribution.


\section{Probability Distributions}

To determine the optimal decision function, the joint PMF $\Prm_{\yrm,\xrm,\Drm}$ is required. Having already defined the distribution conditioned on the model $\uptheta$, all that remains is to select a PDF $\prm_{\uptheta}$ reflecting the user's prior knowledge. In this section, the Dirichlet distribution is used. The Dirichlet distribution possesses the desirable property of being the conjugate prior for the multinomial conditional distribution characterizing the data; as such, it will provide analytic forms for the model posterior distribution and lead to closed form expressions for the data conditional distribution used to design the decision function.

Other distributions of interest will be provided, such as the training data PMF $\Prm_{\Drm}$ and the conditional distribution $\Prm_{\yrm | \xrm,\Drm}$ used to form a decision given specific observations.



\subsection{Model PDF, $\prm_{\uptheta}$} \label{sec:P_theta}

The Dirichlet PDF for the model random process $\uptheta \in \Theta$ is \cite{bishop}
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta}(\theta) & = & \beta(\alpha)^{-1} \prod_{y \in \Ycal} \prod_{x \in \Xcal} \theta(y,x)^{\alpha(y,x) - 1} \nonumber \\
& = & \Dir\big( \theta ; \alpha \big) \;,
\end{IEEEeqnarray}
where the user-selected PDF parameters $\alpha : \Ycal \times \Xcal \mapsto \Rbb^+$ are introduced and $\beta$ is the generalized beta function.

The parameter $\alpha$ controls around which models $\uptheta$ the PDF concentrates and how strongly. For convenience, introduce the concentration parameter $\alpha_0 \equiv \sum_{y \in \Ycal} \sum_{x \in \Xcal} \alpha(y,x)$. 

The first and second joint moments of the model are 
\begin{equation}
\mu_{\uptheta} = \frac{\alpha}{\alpha_0}
\end{equation}
and
\begin{IEEEeqnarray}{rCl}
\Erm_{\uptheta}\big[ \uptheta(y,x) \uptheta(y',x') \big] & = & \frac{\alpha(y,x) \alpha(y',x') + \alpha(y,x) \delta[y,y'] \delta[x,x']}{\alpha_0 (\alpha_0+1)} \;.
\end{IEEEeqnarray}
Observe that $\Prm_{\yrm,\xrm}  = \mu_{\uptheta} = \alpha / \alpha_0$. The covariance is
\begin{IEEEeqnarray}{rCl}
\Sigma_{\uptheta}(y,x,y',x') & = & \Erm_{\uptheta}\Big[ \big(\uptheta(y,x)-\mu_{\uptheta}(y,x)\big) \big(\uptheta(y',x')-\mu_{\uptheta}(y',x')\big) \Big] \\
& = & \frac{\mu_{\uptheta}(y,x) \delta[y,y'] \delta[x,x'] - \mu_{\uptheta}(y,x) \mu_{\uptheta}(y',x')}{\alpha_0+1} \nonumber \;.
\end{IEEEeqnarray}
Also, for $\alpha(y,x) > 1$, the maximizing value of the distribution is
\begin{equation}
\theta_\mathrm{max} = \argmax_{\theta \in \Theta} \prm_{\uptheta}(\theta) = \frac{\alpha - 1}{\alpha_0 - |\Ycal||\Xcal|} \;.
\end{equation}
This can be easily shown by maximizing the logarithm of the distribution using the method of Lagrange multipliers, as demonstrated in \ref{app:MAP_theta}.

Of specific interest is how $\prm_{\uptheta}$ changes as the concentration parameter approaches its limiting values. For $\alpha_0 \to \infty$, the PDF concentrates at its mean, resulting in
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta}(\theta) & \to & \delta\left( \theta - \frac{\alpha}{\alpha_0} \right) \;.
\end{IEEEeqnarray}
Conversely, for $\alpha_0 \to 0$, the PDF tends toward
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta}(\theta) & \to & \sum_{y \in \Ycal} \sum_{x \in \Xcal} \frac{\alpha(y,x)}{\alpha_0} \delta\big( \theta - \delta[\cdot,y] \delta[\cdot,x] \big) \;,
\end{IEEEeqnarray}
which distributes its weight among the $|\Ycal| |\Xcal|$ models with an $\ell_0$ norm satisfying $\| \theta \|_0 = 1$. Note that the Dirac delta for these formulas is defined on the set $\Theta$, such that $\int_{\Theta} \delta(\theta) \mathrm{d}\theta = 1$.

PGR: formal proof for limiting PDFs??? stirling/gautschi?

These trends are demonstrated with Figure \ref{fig:P_theta}. The cardinalities $|\Ycal| = 3$ and $|\Xcal| = 1$ are chosen to enable visualization, despite the implication that $\xrm$ is deterministic; these cardinalities will be used for many subsequent figures as well. Note that for $\alpha_0=2.99$, $\alpha < 1$ and the PDF values at the boundaries of the domain tend to infinity; this is not captured by the plot color scale.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_theta.pdf}
\caption{Model prior PDF for different concentrations $\alpha_0$}
\label{fig:P_theta}
\end{figure}





\paragraph{Uniform Prior}

When the parameterizing function is $\alpha(y,x) = 1$, the distribution becomes a uniform PDF and is represented as
\begin{equation}
\prm_{\uptheta} = \big( |\Ycal||\Xcal|-1 \big)! \;.
\end{equation}
Note that the concentration parameter is $\alpha_0 = |\Ycal||\Xcal|$ and $\Prm_{\yrm,\xrm} = \big( |\Ycal||\Xcal| \big)^{-1}$ is also uniform. Figure \ref{fig:P_theta_uniform} shows the uniform distribution amplitude for $|\Ycal| = 3$ and $|\Xcal| = 1$.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_theta_uniform.pdf}
\caption{Uniform model prior PDF, $|\Ycal| = 3$, $|\Xcal| = 1$}
\label{fig:P_theta_uniform}
\end{figure}



\subsubsection{Marginal and Conditional Distributions}

PGR: move/add Dir figs here?

The marginal distribution $\uptheta'$ and the conditional distribution $\tilde{\uptheta}$ will also be of interest. By the aggregation property \cite{ferguson}, $\uptheta'$ is a Dirichlet random process parameterized by $\alpha' : \Xcal \mapsto \Rbb^+$, where $\alpha' \equiv \sum_{y \in \Ycal} \alpha(y,\cdot)$. Note that $\Prm_{\xrm} = \mu_{\uptheta'} = \alpha' / \alpha_0$.

PGR: introduce tilde alpha to match tilde theta?

Also of interest is the distribution of the predictive model $\tilde{\uptheta}$ conditioned on the marginal $\uptheta'$. As demonstrated in Appendix \ref{app:Dir_agg}, these random processes are jointly distributed as
\begin{IEEEeqnarray}{rCl}
\prm_{\tilde{\uptheta} | \uptheta'}\Big( \tilde{\theta} | \theta' \Big) & = & \prod_{x \in \Xcal} \Bigg[ \beta\big( \alpha(\cdot,x) \big)^{-1} \prod_{y \in \Ycal} \tilde{\theta}(y;x)^{\alpha(y,x)-1} \Bigg] \\
& = & \prod_{x \in \Xcal} \Dir\Big( \tilde{\theta}(x) ; \alpha(\cdot,x) \Big) \nonumber \;,
\end{IEEEeqnarray}
a product of Dirichlet distributions defined on $\tilde{\theta} \in \Pcal(\Ycal)^{\Xcal}$. As shown, the processes $\tilde{\uptheta}(x)$ are Dirichlet with parameterizing functions $\alpha(\cdot,x)$, independent of one another, and independent of the marginal distribution $\uptheta'$. Observe that the values $\alpha'(x)$ represent the concentration parameters for the individual Dirichlet processes; also, note that $\Prm_{\yrm | \xrm} = \mu_{\tilde{\uptheta}(\xrm)} = \alpha(\cdot,\xrm) / \alpha'(\xrm)$. 


PGR: use conditional independence property to simplify throughout?!? In loss app sections?!?!

PGR: implications of independence for posterior learning?


\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_theta_tilde.pdf}
\caption{Model prior PDF for different concentrations $\alpha'(x)$}
%\label{fig:P_theta}
\end{figure}






\subsection{Training Set PMF, $\Prm_{\Drm}$}

PGR: EVIDENCE TERMINOLOGY??

Next, the conditional distribution $\Prm_{\Drm | \uptheta}$ will be used to determine the marginal PMF, $\Prm_{\Drm}$ and properties will be discussed.


As the conditional distribution $\Prm_{\Drm | \uptheta}$ is of exponential form, it can be readily shown that the marginal distribution of the training data is \cite{minka-DirMult}
\begin{IEEEeqnarray}{rCl}
\Prm_{\Drm}(D) & = & \Erm_{\uptheta} \left[ \prod_{n=1}^N \Prm_{\Drm_n | \uptheta}\big( D_n | \uptheta \big) \right] \\
& = & \Erm_{\uptheta} \left[ \prod_{y \in \Ycal} \prod_{x \in \Xcal} \uptheta(y,x)^{\bar{N}(y,x;D)} \right] \nonumber \\
& = & \beta(\alpha)^{-1} \beta \left(  \alpha + \bar{N}(D) \right) \nonumber \;.
\end{IEEEeqnarray}
Note that values of the PMF $\Prm_{\Drm}$ are equivalent to joint moments of the model $\uptheta$. 

It is informative to consider the limiting forms of this distribution for the extreme values of the model concentration parameter $\alpha_0$. As $\alpha_0 \to \infty$, the model concetrates at its mean and the training data $\Drm$ distribution is
\begin{IEEEeqnarray}{rCl}
\Prm_{\Drm}(D) & \to & \Erm_{\uptheta}\left[ \prod_{n=1}^N \uptheta(Y_n,X_n) \right] \\
& = & \prod_{n=1}^N \frac{\alpha\big( Y_n,X_n \big)}{\alpha_0} \nonumber \;.
\end{IEEEeqnarray}
Conversely, as $\alpha_0 \to 0$, the distribution becomes
\begin{IEEEeqnarray}{rCl}
\Prm_{\Drm}(D) & \to & \sum_{y \in \Ycal} \sum_{x \in \Xcal} \frac{\alpha(y,x)}{\alpha_0} \prod_{n=1}^N \delta\big[ D_n,(y,x) \big] 
\end{IEEEeqnarray}
and the training data are identical.


Next, the distribution of the sufficient statistic $\nbarrm$ will be represented. As a Dirichlet distribution characterizes the parameters of the multinomial distribution $\Prm_{\Drm | \uptheta}$, the marginal PMF of $\nbarrm$ is a Dirichlet-Multinomial distribution \cite{johnson} parameterized by $\alpha$,
\begin{IEEEeqnarray}{rCl}
\Prm_{\nbarrm}(\bar{n}) & = & \Mcal(\bar{n}) \beta(\alpha)^{-1} \beta(\alpha + \bar{n}) \\
& = & \DM\big( \bar{n}; N,\alpha \big) \nonumber \;.
\end{IEEEeqnarray}

The first and second joint moments of $\bar{\nrm}$ are
\begin{IEEEeqnarray}{rCl}
\mu_{\bar{\nrm}} & = & N \frac{\alpha}{\alpha_0} = N \mu_\uptheta 
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{L}
\Erm_{\nbarrm}\big[ \bar{\nrm}(y,x) \bar{\nrm}(y',x') \big] \\
\qquad = \frac{N}{\alpha_0 (\alpha_0+1)} \Big( (\alpha_0 + N)\alpha(y,x) \delta[y,y'] \delta[x,x'] + (N-1) \alpha(y,x) \alpha(y',x') \Big) \nonumber \\
\qquad = \frac{N}{\alpha_0+1} \Big( (\alpha_0+N) \mu_\uptheta(y,x) \delta[y,y'] \delta[x,x'] + \alpha_0(N-1) \mu_\uptheta(y,x) \mu_\uptheta(y',x') \Big) \nonumber \;.
\end{IEEEeqnarray}
The covariance function is
\begin{IEEEeqnarray}{rCl}
\Sigma_{\nbarrm}(y,x,y',x') & = & \frac{N (\alpha_0+N)}{\alpha_0+1} \big( \mu_\uptheta(y,x) \delta[y,y'] \delta[x,x'] - \mu_\uptheta(y,x) \mu_\uptheta(y',x') \big) \\
& = & N (\alpha_0+N) \Sigma_{\uptheta}(y,x,y',x') \nonumber \;.
\end{IEEEeqnarray}


Again, the data PMF's for minimal and maximal concentration $\alpha_0$ are relevant. For $\alpha_0 \to \infty$, the model PDF $\prm_{\uptheta}$ concentrates at its mean and thus $\bar{\nrm}$ is characterized by a multinomial distribution,
\begin{IEEEeqnarray}{rCl}
\Prm_{\nbarrm}(\bar{n}) & \to & \Mcal(\bar{n}) \prod_{y \in \Ycal} \prod_{x \in \Xcal} \left(\frac{\alpha(y,x)}{\alpha_0}\right)^{\bar{n}(y,x)}
\end{IEEEeqnarray}
Conversely, for $\alpha_0 \to 0$, the PMF tends toward
\begin{IEEEeqnarray}{rCl} \label{eq:P_n_lim_zero}
\Prm_{\nbarrm}(\bar{n}) & \to & \sum_{y \in \Ycal} \sum_{x \in \Xcal} \frac{\alpha(y,x)}{\alpha_0} \delta\big[ \bar{n} , N \delta[\cdot,y] \delta[\cdot,x] \big] \;.
\end{IEEEeqnarray}

PGR: formal proofs for limiting PMFs? stirling/gautschi?

Figure \ref{fig:P_nbar_a0} displays the distribution of $\nbarrm$ for $N=10$ and different model concentrations $\alpha_0$. Observe that for large $\alpha_0$, the distribution approaches a multinomial distribution $\nbarrm \sim \Multi(N,\alpha/\alpha_0)$. Figure \ref{fig:P_nbar_N} shows how a specific model prior influences the data PMF differently for different $N$. Observe that as the number of training samples increases, the PMF $\Prm_{\nbarrm}$ tends toward $\Prm_{\nbarrm}(\bar{n}) \approx N^{1-|\Ycal||\Xcal|}\prm_{\uptheta}(\bar{n}/N)$; this can be proven using Gautschi's inequality \cite{wendel}.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_nbar_a0.pdf}
\caption{$\Prm(\nbarrm)$ for different prior concentrations $\alpha_0$}
\label{fig:P_nbar_a0}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_nbar_N.pdf}
\caption{$\Prm(\nbarrm)$ for different training set sizes $N$}
\label{fig:P_nbar_N}
\end{figure}




\paragraph{Uniform Prior}

For the uniform distribution, $\alpha(y,x) = 1$,
\begin{IEEEeqnarray}{rCl} \label{P_D_io}
\Prm_{\Drm}(D) & = & \Mcal\big( \{N,|\Ycal||\Xcal|-1\} \big)^{-1} \Mcal\big( \bar{N}(D) \big)^{-1}
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{rCl} \label{P_D_io}
\Prm_{\nbarrm} & = & |\bar{\Ncal}|^{-1} = \Mcal\big( \{N,|\Ycal||\Xcal|-1\} \big)^{-1} \;.
\end{IEEEeqnarray}
The distribution of $\nbarrm$ is uniform over the set $\bar{\Ncal}$. The PMF for $\Drm$ depends on the training data only through the multinomial coefficient; consequently, more ``concentrated''  training sets are more probable.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_nbar_uni.pdf}
\caption{$\Prm(\nbarrm)$ for uniform prior}
\label{fig:P_nbar_uni}
\end{figure}




\subsubsection{Marginal and Conditional Distributions}

It is also useful to express the marginal and conditional distributions for the training data given the Dirichlet prior. As $\Prm_{\Xrm | \uptheta}$ is of exponential form with respect to the marginal model $\uptheta'$, the marginal distribution of $\Xrm$ can be expressed as 
\begin{IEEEeqnarray}{rCl}
\Prm_{\Xrm}(X) & = & \Erm_{\uptheta'} \big[ \Prm_{\Xrm | \uptheta'} \big](X) \\
& = & \Erm_{\uptheta} \left[ \prod_{n=1}^N \Prm_{\Xrm_n | \uptheta}\big( X_n | \uptheta \big) \right] \nonumber \\
& = & \Erm_{\uptheta'} \left[ \prod_{x \in \Xcal} \uptheta'(x)^{N'(x;X)} \right] \nonumber \\
& = & \beta(\alpha')^{-1} \beta\big( \alpha' + N'(X) \big) \nonumber \;.
\end{IEEEeqnarray}
As the model marginal $\uptheta'$ and conditional $\tilde{\uptheta}$ are independent, the distribution $\Prm_{\Yrm | \Xrm}$ can be represented as
\begin{IEEEeqnarray}{rCl}
\Prm_{\Yrm | \Xrm}(Y | X) & = & \Erm_{\tilde{\uptheta}}\big[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}} \big](Y | X) \\
& = & \prod_{x \in \Xcal} \Erm_{\tilde{\uptheta}(x)}\left[ \prod_{y \in \Ycal} \tilde{\uptheta}(y;x)^{\bar{N}(y,x;Y,X)} \right] \nonumber \\
& = & \prod_{x \in \Xcal} \beta\big(\alpha(\cdot,x)\big)^{-1} \beta\big( \alpha(\cdot,x) + \bar{N}(\cdot,x;Y,X) \big) \nonumber \;.
\end{IEEEeqnarray}

The corresponding distributions for the sufficient statistics will be expressed as well. Recall that $\nrm' | \uptheta \sim \Multi(N,\uptheta')$; by the aggregation property of Dirichlet-Multinomial functions \cite{johnson}, the random process is distributed as $\nrm' \sim \DM(N,\alpha')$.

Also of interest is the distribution of $\nbarrm$ conditioned on its aggregation $\nrm'$. Using the Dirichlet-Multinomial properties presented in Appendix \ref{app:DM_agg}, it can be shown that
\begin{IEEEeqnarray}{rCl}
\Prm_{\bar{\nrm} | \nrm'}(\bar{n} | n') & = & \prod_{x \in \Xcal} \left[ \Mcal\big( \bar{n}(\cdot,x) \big) \beta\big( \alpha(\cdot,x) \big)^{-1} \beta\big( \alpha(\cdot,x) + \bar{n}(\cdot,x) \big) \right] \\
& = & \prod_{x \in \Xcal} \DM\Big( \bar{n}(\cdot,x) ; n'(x) , \alpha(\cdot,x) \Big) \nonumber
\end{IEEEeqnarray}
over the domain $\left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal \times \Xcal} : \sum_{y \in \Ycal} \bar{n}(y,\cdot) = n' \right\}$. Observe that conditioning on the aggregation renders the function segments $\nbarrm(\cdot,x)$ independent of one another and that they are also Dirichlet-Multinomial, such that $\nbarrm(\cdot,x) | \nrm'(x) \sim \DM\big( \nrm'(x),\alpha(\cdot,x) \big)$.












\subsection{Predictive PMF, $\Prm_{\yrm | \xrm,\Drm}$}

As shown in Equation \eqref{eq:f_opt_xD}, the decision selected by the optimally designed function depends on $\Prm_{\yrm | \xrm,\Drm}$, the distribution of the unobserved $\yrm$ conditioned on all observable random elements. This PMF will be expressed next.

First observe that since $\Prm_{\Drm | \uptheta}$ is of exponential form, the Dirichlet prior $\prm_{\uptheta}$ is its conjugate prior \cite{theodoridis-ML}; thus, the model posterior PDF given the training data is
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta | \Drm}(\theta | D) & = & \beta \left( \alpha + \bar{N}(D) \right)^{-1} \prod_{y \in \Ycal} \prod_{x \in \Xcal} 
\theta(y,x)^{\alpha(y,x) + \bar{N}(y,x;D) - 1} \;, 
\end{IEEEeqnarray}
a Dirichlet distribution with parameter function $\alpha + \bar{N}(D)$.

This posterior distribution is of specific interest in the machine learning literature. While Bayesian techniques are used here, often point estimates of the model $\uptheta$ are formed; perhaps the most common approach is to form the Maximum a posteriori estimate,
\begin{IEEEeqnarray}{rCl}
\theta_\mathrm{MAP}(D) & = & \argmax_{\theta \in \Theta} \Prm_{\uptheta | \Drm}(\theta | D) = \frac{\bar{N}(D) + \alpha - 1}{N + \alpha_0 - |\Ycal||\Xcal|} \;.
\end{IEEEeqnarray}
This maximizing value is only valid when $\bar{N}(D) >1$. For the uniform model prior, the maximizing value of the posterior is the empirical PMF $\bar{N}(D) / N$.

PGR: MAP discussion out of place??

Also, the concentration parameter increases proportionately with increasing volumes of training data; consequently, as $N \to \infty$, the posterior converges to $\prm_{\uptheta | \Drm} \to \delta\big( \cdot - \bar{N}(\Drm) / N \big)$. Thus, as more data is collected, the model can be more positively identified and used to formulate minimum risk decisions. Conversely, as $\alpha_0 \to \infty$, the prior model certainty is stronger and the posterior tends toward $\prm_{\uptheta | \Drm} \to \delta( \cdot - \alpha / \alpha_0)$, independent of the training data.

Figure \ref{fig:P_theta_D} shows the influence of the training data on the model distribution; after conditioning on the training data (via $\nbarrm$), the PDF concentration shifts away from the models favored by the prior knowledge and towards other models that better account for the observations.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_theta_post.pdf}
\caption{Model $\uptheta$ PDF, prior and posterior}
\label{fig:P_theta_D}
\end{figure}


The joint PMF of $\yrm$ and $\xrm$ conditioned on the training data is expressed as \cite{murphy}
\begin{IEEEeqnarray}{rCl}
\Prm_{\yrm,\xrm | \Drm} & = & \mu_{\uptheta | \Drm} = \frac{\alpha + \bar{N}(\Drm)}{\alpha_0 + N} \\
& = & \left(\frac{\alpha_0}{\alpha_0 + N}\right) \frac{\alpha}{\alpha_0} + \left(\frac{N}{\alpha_0 + N}\right) \frac{\bar{N}(\Drm)}{N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0 + N}\right) \Prm_{\yrm,\xrm} + \left(\frac{N}{\alpha_0 + N}\right) \frac{\bar{N}(\Drm)}{N} \nonumber \;.
\end{IEEEeqnarray}
This is a mixture distribution of the prior expectation $\mu_{\uptheta} = \alpha/\alpha_0$ and the empirical distribution $\bar{N}(\Drm)/N$. The more subjective the model prior (i.e. larger $\alpha_0$), the more the prior mean is favored; the more data, the more the empirical PMF is favored. The marginal distribution for $\xrm$ given $\Drm$ is
\begin{IEEEeqnarray}{rCl}
\Prm_{\xrm | \Drm} & = & \frac{\alpha' + N'(\Drm)}{\alpha_0 + N} \\
& = & \left(\frac{\alpha_0}{\alpha_0 + N}\right) \frac{\alpha'}{\alpha_0} + \left(\frac{N}{\alpha_0 + N}\right) \frac{N'(\Drm)}{N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0 + N}\right) \Prm_{\xrm} + \left(\frac{N}{\alpha_0 + N}\right) \frac{N'(\Drm)}{N} \nonumber \;.
\end{IEEEeqnarray}

Finally, the predictive distribution of interest is generated via Bayes rule as
\begin{IEEEeqnarray}{rCl} \label{eq:P_y_xD_dir}
\Prm_{\yrm | \xrm,\Drm} & = & \frac{\alpha(\cdot,\xrm) + \bar{N}(\cdot,\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \\
& = & \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \frac{\alpha(\cdot,\xrm)}{\alpha'(\xrm)} + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \frac{\bar{N}(\cdot,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \\
& = & \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \Prm_{\yrm | \xrm} + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \frac{\bar{N}(\cdot,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \;.
\end{IEEEeqnarray}
The last representation views the distribution as a convex combination of two conditional distributions. The first distribution $\Prm_{\yrm | \xrm} = \alpha(\cdot,\xrm) / \alpha'(\xrm)$ is independent of the training data and based on the prior knowledge implied via the model PDF parameter; the second distribution is the conditional empirical PMF and depends on $\Drm$, not on $\alpha$.

The weighting factors $\alpha'(x)$ and $N'(\xrm;\Drm)$ are the concentration of the conditional prior $\tilde{\uptheta}(x)$ and the number of training samples satisfying $X_n = \xrm$. As $N'(\xrm;\Drm) / \alpha'(x) \to 0$, the PMF tends toward the conditional distribution $\Prm_{\yrm | \xrm}$, which only depends on the model parameter $\alpha$. As $N'(\xrm;\Drm) / \alpha'(x) \to \infty$, $\Prm_{\yrm | \xrm,\Drm}$ tends towards the empirical conditional distribution. 




\paragraph{Uniform Prior}

For the uniform model prior PDF, the conditional distribution is
\begin{IEEEeqnarray}{rCl} \label{eq:P_y_xD_dir_uni}
\Prm_{\yrm | \xrm,\Drm} & = & \frac{\bar{N}(\cdot,\xrm;\Drm)+1}{N'(\xrm;\Drm) + |\Ycal|} \\
& = & \left( \frac{|\Ycal|}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{1}{|\Ycal|} + \nonumber \\
&& \quad \left( \frac{N'(\xrm;\Drm)}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{\bar{N}(\cdot,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \;.
\end{IEEEeqnarray}
Now the prior PMF contribution $\Prm_{\yrm | \xrm}$ is a uniform distribution over the $|\Ycal|$ possible outputs. The weighting factors are controlled by $\alpha'(\xrm) = |\Ycal|$; the more possible outcomes $|\Ycal|$ there are for a given training set size, the more the conditional distribution tends toward the uniform PMF implied by the model prior.



\subsubsection{Representation using the complete model posterior}

PGR: reference posterior equations!

PGR: DIR FIGS? for PDF asymptotics?


The Bayesian distributions $\Prm_{\xrm | \Drm}$ and $\Prm_{\yrm | \xrm,\Drm}$ can also be found from the posterior distributions $\prm_{\uptheta' | \Drm}$ and $\prm_{\tilde{\uptheta} | \xrm,\Drm}$, respectively. As the Dirichlet assumption renders $\uptheta'$ and $\tilde{\uptheta}$ independent, it can be shown that $\Prm_{\Yrm | \Xrm} = \Erm_{\tilde{\uptheta}}\big[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}} \big]$ and thus that $\uptheta'$ is conditionally independent of $\Yrm$ given $\Xrm$. Furthermore, the Dirichlet distribution $\prm_{\uptheta'}$ is the conjugate prior for $\Prm_{\Xrm | \uptheta'}$. As a result, $\uptheta' | \Drm \sim \Dir(\alpha' + N'(\Xrm))$ and
\begin{IEEEeqnarray}{rCl}
\Prm_{\xrm | \Drm}(D) & = & \mu_{\uptheta' | \Drm}(D) = \mu_{\uptheta' | \Xrm}(X) \\
& = & \frac{\alpha' + N'(X)}{\alpha_0 + N} \nonumber \;.
\end{IEEEeqnarray}
Similarly, the distribution can be expressed in terms of the empirical PMF sufficient statistic as
\begin{IEEEeqnarray}{rCl}
\Prm_{\xrm | \nbarrm}(\bar{n}) & = & \mu_{\uptheta' | \nbarrm}(\bar{n}) = \mu_{\uptheta' | \nrm'}\left( \sum_y \bar{n}(y,\cdot) \right) \\
& = & \frac{\alpha' + \sum_y \bar{n}(y,\cdot)}{\alpha_0 + N} \nonumber \;,
\end{IEEEeqnarray}
where the dependency on $\nbarrm$ is expressed only through the marginal random process $\nrm'$.

The posterior $\prm_{\tilde{\uptheta} | \xrm,\Drm}$ can be simplified by noting that the independence of $\uptheta'$ and $\tilde{\uptheta}$ implies $\Prm_{\Yrm | \Xrm,\xrm} = \Erm_{\tilde{\uptheta}}\big[ \Prm_{\Yrm | \Xrm,\tilde{\uptheta}} \big] = \Prm_{\Yrm | \Xrm}$. Consequently, $\tilde{\uptheta}$ is conditionally independent of $\xrm$ given $\Drm$. Thus, as $\prm_{\tilde{\uptheta}}$ is a conjugate prior for $\Prm_{\Yrm | \Xrm,\tilde{\uptheta}}$ the posterior distribution is
\begin{IEEEeqnarray}{rCl}
\prm_{\tilde{\uptheta} | \Drm,\xrm}\big( \tilde{\theta} | D,x \big) & = & \prm_{\tilde{\uptheta} | \Drm}\big( \tilde{\theta} | D \big) = \prod_{x' \in \Xcal} \prm_{\tilde{\uptheta}(x') | \Drm}\big(\tilde{\theta}(x') | D \big) \\
& = & \prod_{x' \in \Xcal} \Dir\big( \tilde{\theta}(x') ; \alpha(\cdot,x') + \bar{N}(\cdot,x';D) \big) \nonumber
\end{IEEEeqnarray}
and the distinct model conditional PMF's are independent from one another. A similar treatment demonstrates that
\begin{IEEEeqnarray}{rCl}
\prm_{\tilde{\uptheta} | \nbarrm,\xrm}\big( \tilde{\theta} | \bar{n},x \big) & = & \prm_{\tilde{\uptheta} | \nbarrm}\big( \tilde{\theta} | \bar{n} \big) = \prod_{x' \in \Xcal} \prm_{\tilde{\uptheta}(x') | \nbarrm(\cdot,x')}\big(\tilde{\theta}(x') | \bar{n}(\cdot,x') \big) \\
& = & \prod_{x' \in \Xcal} \Dir\big( \tilde{\theta}(x') ; \alpha(\cdot,x') + \bar{n}(\cdot,x') \big) \nonumber \;.
\end{IEEEeqnarray}
Observe that when the conditioning is performed using the sufficient statistic, the independent conditional models $\tilde{\uptheta}(x)$ are only dependent on their corresponding subset of the  empirical PMF, $\nbarrm(\cdot,x)$.

The Bayes predictive PMF can thus be expressed as
\begin{IEEEeqnarray}{rCl}
\Prm_{\yrm | \xrm,\Drm}(x,D) & = & \mu_{\tilde{\uptheta}(x) | \xrm,\Drm}(x,D) = \mu_{\tilde{\uptheta}(x) | \Drm}(D) \\
& = & \frac{\alpha(\cdot,x) + \bar{N}(\cdot,x;D)}{\alpha'(x) + N'(x;D)} \nonumber
\end{IEEEeqnarray}
or, via the sufficient statistic,
\begin{IEEEeqnarray}{rCl}
\Prm_{\yrm | \xrm,\nbarrm}(x,\bar{n}) & = & \mu_{\tilde{\uptheta}(x) | \xrm,\nbarrm}(x,\bar{n}) = \mu_{\tilde{\uptheta}(x) | \nbarrm(\cdot,x)}\big(\bar{n}(\cdot,x)\big) \\
& = & \frac{\alpha(\cdot,x) + \bar{n}(\cdot,x)}{\alpha'(x) + \sum_y \bar{n}(y,x)} \nonumber \;.
\end{IEEEeqnarray}
A consequence of the Dirichlet prior is that the predictive PMF for a given value of $\xrm$ only depends on the corresponding training data $\nbarrm(\cdot,\xrm)$, such that $\Prm_{\yrm | \xrm,\nbarrm}(x,\bar{n}) = \Prm_{\yrm | \xrm,\nbarrm(\cdot,\xrm)}\big( x,\bar{n}(\cdot,x) \big)$. This is intuitive considering the independence of the conditional models $\tilde{\theta}(x)$ from one another.



\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_theta_post_tilde.pdf}
\caption{Model PDF, prior and posterior}
%\label{fig:P_theta_D}
\end{figure}





\section{Model Estimation Perspective} \label{sec:predictive_est}

PGR: generalize, move before Dirichlet?

PGR: use poster table??


It is informative to treat the distribution $\Prm_{\yrm | \xrm,\nbarrm}$ as an estimate of the unknown conditional PMF $\Prm_{\yrm | \xrm,\uptheta} \equiv \tilde{\uptheta}(\xrm)$ and investigate the effects of subjective prior knowledge. For a given $\xrm$ and corresponding number of training samples $\nrm'(\xrm)$, the expected value of the estimate condtioned on the true model $\uptheta$ is
\begin{IEEEeqnarray}{L}
\Erm_{\nbarrm | \nrm',\uptheta}\big[ \Prm_{\yrm | \xrm,\nbarrm} \big] 
= \Erm_{\nbarrm(\cdot,\xrm) | \nrm'(\xrm),\tilde{\uptheta}(\xrm)}\big[ \Prm_{\yrm | \xrm,\nbarrm(\cdot,\xrm)} \big] \\ 
\quad = \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)}\right) \frac{\alpha(\cdot,\xrm)}{\alpha'(\xrm)} + \left(\frac{\nrm'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)}\right) \tilde{\uptheta}(\xrm) \nonumber \;,
\end{IEEEeqnarray}
where the properties of a multinomial distribution conditioned on its aggregation have been used. The result is a convex combination of the conditional data-independent distribution $\alpha(\cdot,x) / \alpha'(x)$ and the true conditional distribution $\tilde{\uptheta}(\xrm)$. The convex coefficients are dependent on the ``marginal'' values $\alpha'$ and $\nrm'$; note that as the number of matching training samples $\nrm'(x)$ increases relative to $\alpha'(x)$, the estimate tends towards the true conditional PMF. 

PGR: suppress delta dependency on nbar and theta?

To aid characterization of the estimator, define the random process $\Delta(\xrm,\nbarrm,\uptheta) \equiv \Prm_{\yrm | \xrm,\nbarrm} - \Prm_{\yrm | \xrm,\uptheta} \in \Rbb^{\Ycal}$. For a given $\xrm$ and corresponding number of training samples $\nrm'(\xrm)$, the bias of the conditional PMF estimate is
\begin{IEEEeqnarray}{rCl} \label{eq:predictive_bias}
\mathrm{Bias}(\xrm,\nrm',\uptheta) & = & \Erm_{\nbarrm | \nrm',\uptheta}\big[ \Delta(\xrm,\nbarrm,\uptheta) \big] \\
& = & \frac{\alpha'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)} \left( \frac{\alpha(\cdot,\xrm)}{\alpha'(\xrm)} - \tilde{\uptheta}(\xrm) \right) \nonumber 
\end{IEEEeqnarray}
and its covariance function is 
\begin{IEEEeqnarray}{L} \label{eq:predictive_cov}
\mathrm{Cov}(y,y';\xrm,\nrm',\uptheta) = \Crm_{\nbarrm | \nrm',\uptheta} \big[\Prm_{\yrm | \xrm,\nbarrm}(\cdot | \xrm,\nbarrm) \big](y,y') \\
\quad = \frac{\Sigma_{\nbarrm(\cdot,\xrm) | \nrm'(\xrm),\tilde{\uptheta}(\xrm)}(y,y')}{\big( \alpha'(\xrm) + \nrm'(\xrm) \big)^2} \nonumber \\
\quad = \frac{\nrm'(\xrm)}{\big( \alpha'(\xrm) + \nrm'(\xrm) \big)^2} \left( \tilde{\uptheta}(y;\xrm) \delta[y,y'] - \tilde{\uptheta}(y;\xrm) \tilde{\uptheta}(y';\xrm) \right) \nonumber \;,
\end{IEEEeqnarray}
where the properties of multinomial random processes have been used. Note that the bias is proportionate to the difference between the true conditional model and the data-independent estimate. The scaling factor tends from one to zero as $\nrm'(x)/\alpha'(x)$ tends from zero to infinity; as such, more subjective priors (large $\alpha'(x)$) will lead to PMF estimates that are prone to bias. Conversely, the variance of the PMF estimate tends to zero as $\alpha'(x) \to \infty$. 


Combining the estimator bias and variance, the conditional second moments of $\Delta(\xrm,\nbarrm,\uptheta)$ are
\begin{IEEEeqnarray}{rCl} \label{eq:predictive_del_sq}
\mathcal{E}(y,y' ; \xrm,\nrm',\uptheta) & = & \Erm_{\nbarrm | \nrm',\uptheta} \Big[ \Delta(y;\xrm,\nbarrm,\uptheta) \Delta(y';\xrm,\nbarrm,\uptheta) \Big] \\
& = & \mathrm{Bias}(y;\xrm,\nrm',\uptheta) \mathrm{Bias}(y';\xrm,\nrm',\uptheta) + \mathrm{Cov}(y,y';\xrm,\nrm',\uptheta) \nonumber \;.
\end{IEEEeqnarray}
As $\nrm'(x) \to \infty$, this function tends to zero and thus the underlying model $\tilde{\uptheta}(x)$ is determined precisely. A more practical case is estimation with a finite volume of training data. Specification of the Dirichlet model prior can be interpreted as providing a distribution estimate $\alpha(\cdot,x)/\alpha'(x)$ and a confidence level $\alpha'(x)$. Higher confidence reduces error due to the variance of the estimator, but increases the error due to bias between the true model and its estimate; low confidence renders the estimate unbiased, but maximizes the estimator variance. 



Also of interest, the conditional expectation of $\mathcal{E}(\cdot,\cdot;\xrm,\nrm',\uptheta)$ is
\begin{IEEEeqnarray}{L} \label{eq:predictive_E_del_sq}
\Erm_{\xrm,\nrm' | \uptheta}\Big[ \mathcal{E}(y,y' ; \xrm,\nrm',\uptheta) \Big] \\
\quad = \Erm_{\xrm | \uptheta}\left[ \Erm_{\nrm'(\xrm) | \uptheta}\left[ \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)}\right)^2 \right] \left( \frac{\alpha(y,\xrm)}{\alpha'(\xrm)} - \tilde{\uptheta}(y;\xrm) \right) \left( \frac{\alpha(y',\xrm)}{\alpha'(\xrm)} - \tilde{\uptheta}(y';\xrm) \right) \right] \nonumber \\
\qquad + \Erm_{\xrm | \uptheta}\left[ \Erm_{\nrm'(\xrm) | \uptheta}\left[ \frac{\nrm'(\xrm)}{\big( \alpha'(\xrm) + \nrm'(\xrm) \big)^2} \right] \left( \tilde{\uptheta}(y;\xrm) \delta[y,y'] - \tilde{\uptheta}(y;\xrm) \tilde{\uptheta}(y';\xrm) \right) \right] \nonumber \;.
\end{IEEEeqnarray}



To exemplify how the model estimate $\Prm_{\yrm | \xrm,\nbarrm}$ approximates $\Prm_{\yrm | \xrm,\uptheta}$, consider a scenario with $|\Ycal| = 10$. The data-independent PMF $\alpha(\cdot,x) / \alpha'(x)$ and true model $\tilde{\uptheta}(x)$ are shown in Figure \ref{fig:P_yx_error_N_0} - note the significant mismatch. 

PGR: ABOVE - STATE XCAL cardinality = 1

Figures \ref{fig:P_yx_error_a0_0_1} and \ref{fig:P_yx_error_a0_10} show how the bias and variance of the estimate change for different values of $\nrm'(x)$ and $\alpha'(x)$. The plot markers represent the conditional mean of the estimator, $\Erm_{\nbarrm | \nrm',\uptheta}\big[ \Prm_{\yrm | \xrm,\nbarrm}(y | \xrm,\nbarrm) \big]$; the upper and lower error bars represent the square-root of the expected squared deviation above and below the conditional mean, respectively. Each individual plot heading provides the error $\sqrt{\sum_{y \in \Ycal} \mathcal{E}(y,y ; \xrm,\nrm',\uptheta)}$ to assess the quality of the PMF estimate. 

Observe that for $\nrm'(x) = 1$, the high variance of the $\alpha'(x) = 0.1$ estimate (favoring the empirical PMF) renders it worse than the $\alpha_0 = 10$ estimate; in fact, the variance is so high that the error exceeds that of the data-independent estimate $\alpha(\cdot,x) / \alpha'(x)$ (Figure \ref{fig:P_yx_error_N_0}). Conversely, for $\nrm'(x) = 10$, the confidence of the $\alpha'(x) = 10$ estimate leads to high bias and the $\alpha'(x) = 0.1$ estimate is superior. For $\nrm'(x) = 100$, both the $\alpha'(x) = 0.1$ and $\alpha'(x) = 10$ estimates begin converging to the true distribution - this is guaranteed due to the full support of the Dirichlet prior.

PGR: full support discussion?


\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_yx_error_N_0.pdf}
\caption{Model $\uptheta$ estimate, no training data}
\label{fig:P_yx_error_N_0}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_yx_error_a0_0_1.pdf}
\caption{Model $\uptheta$ estimates, $\alpha_0 = 0.1$}
\label{fig:P_yx_error_a0_0_1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{P_yx_error_a0_10.pdf}
\caption{Model $\uptheta$ estimates, $\alpha_0 = 10$}
\label{fig:P_yx_error_a0_10}
\end{figure}























\newpage



\section{Applications to Common Loss Functions}

PGR: REMOVE GENERAL, RELOCATED MATERIAL!

PGR: equations, plots for specific theta results? subjective/objective tradeoff??? alpha/theta mismatch results???

In this section, the Dirichlet prior is applied to the regression and classification applications. Optimal learners $f^*$ are found,  the corresponding minimum Bayes risk $\Rcal^*$ is assessed, and the conditional risk $\Rcal_{\Theta}(f^*;\theta)$ is analyzed.

PGR: add formula for f(D)??? EMPIRICAL RISK DISCUSS, REGULARIZING weight

It is informative to substitute the Bayes predictive distribution using the Dirichlet prior \eqref{eq:P_y_xD_dir} into Equation \eqref{eq:f_opt_xD}, expressing the decision for a given input $\xrm$ and training set $\Drm$ as
\begin{IEEEeqnarray}{L} \label{eq:E_y|xD L}
f^*(\xrm;\Drm) = \argmin_{h \in \Hcal} \Erm_{\yrm | \xrm,\Drm} \big[ \Lcal(h,\yrm) \big] \\
= \argmin_{h \in \Hcal} \frac{\sum_{y \in \Ycal} \alpha(y,\xrm) \Lcal(h,y) + \sum_{y \in \Ycal} \bar{N}(y,\xrm;\Drm) \Lcal(h,y)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \nonumber \\
= \argmin_{h \in \Hcal} \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \sum_{y \in \Ycal} \frac{\alpha(y,\xrm)}{\alpha'(\xrm)} \Lcal(h,y) + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \sum_{y \in \Ycal} \frac{\bar{N}(y,\xrm;\Drm)}{N'(\xrm;\Drm)} \Lcal(h,y) \nonumber \\
= \argmin_{h \in \Hcal} \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \Erm_{\yrm | \xrm}\big[ \Lcal(h,\yrm) \big] + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \frac{\sum_{n=1}^N \delta\big[ \xrm,\Xrm_n \big] \Lcal\big( h,\Yrm_n \big)}{\sum_{n=1}^N \delta\big[ \xrm,\Xrm_n \big]} \nonumber \;.
\end{IEEEeqnarray}

The metric to be minimized can be represented as a convex combination of two expected losses. The first expected loss is evaluated with respect to the conditional distribution $\Prm_{\yrm | \xrm} = \alpha(\cdot,\xrm) / \alpha'(\xrm)$, which reflects the prior knowledge imparted by the model parameter $\alpha$. The second term is the conditional empirical risk, or the average loss among samples $\Yrm_n$ whose corresponding values $\Xrm_n$ match the observed value $\xrm$. The convex weights are inherited from the conditional distribution $\Prm_{\yrm | \xrm,\Drm}$; thus, for a given observation $\xrm$, the model prior parameter $\alpha'(\xrm)$ and the number of matching training samples $N'(\xrm;\Drm)$ dictate which of the two expectations are emphasized.




\subsection{Regression: the Squared-Error Loss}

PGR: Use finite hypothesis space instead, wait for continuous DP???

PGR: add Dir conditional risk and analysis!!!

The elements of the finite cardinality set $\Ycal$ are real numbers, such that $\Ycal \subset \Rbb$. Again, $\Hcal = \Rbb \supset \Ycal$.






\subsubsection{Optimal Estimate: the Posterior Mean}

PGR: plots?

Substituting in the Bayes predictive distribution for a Dirichlet prior \eqref{eq:P_y_xD_dir} into \eqref{eq:f_opt_SE}, the optimal Bayesian estimate is
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_SE_dir}
f^*(\xrm;\Drm) & = & \mu_{\yrm | \xrm,\Drm} \\
& = & \left( \frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \sum_{y \in \Ycal} y \frac{\alpha(y,\xrm)}{\alpha'(\xrm)} + \left( \frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \sum_{y \in \Ycal} y \frac{\bar{N}(y,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \\
& = & \left( \frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \mu_{\yrm | \xrm} + \left( \frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \frac{\sum_{n=1}^N \delta\big[ \xrm,\Xrm_n \big] \Yrm_n}{N'(\xrm;\Drm)} \nonumber \;.
\end{IEEEeqnarray}

The optimal estimate is interpreted as a convex combination of two separate estimates - the expected value of $\yrm$ conditioned on the observed $\xrm$ and the mean of the training values $\Yrm_n$ which have a value $\Xrm_n$ matching the observed value $\xrm$. The weighting factors are the same as those of $\Prm_{\yrm | \xrm,\Drm}$; thus, stronger prior information (larger $\alpha'(\xrm)$) provides more weight to the estimate $\mu_{\yrm|\xrm}$ and more voluminous training data puts emphasis on the empirical conditional mean.





\paragraph{Uniform Prior}

The optimal estimator for a uniform prior is
\begin{IEEEeqnarray}{rCl}
f^*(\xrm;\Drm) & = & \left( \frac{|\Ycal|}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y + \left( \frac{N'(\xrm;\Drm)}{N'(\xrm;\Drm) + |\Ycal|} \right) \sum_{y \in \Ycal} y \frac{\bar{N}(y,\xrm;\Drm)}{N'(\xrm;\Drm)} \\
& = & \left( \frac{|\Ycal|}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y + \left( \frac{N'(\xrm;\Drm)}{N'(\xrm;\Drm) + |\Ycal|} \right) \frac{\sum_{n=1}^N \delta\big[ \xrm,\Xrm_n \big] \Yrm_n}{N'(\xrm;\Drm)} \nonumber \;.
\end{IEEEeqnarray}
Now, the model prior contribution to the weighting factors depends on the cardinality $|\Ycal|$ and the prior expectation is simply the average of the elements of $\Ycal$.




\subsubsection{Minimum Risk: the Expected Posterior Variance}

PGR: determine irreducible risk separately, before??


The minimum Bayes squared-error is $\Rcal^* = \Erm_{\xrm,\Drm} \left[ \Sigma_{\yrm | \xrm,\Drm} \right]$. Using the sufficient statistic $\nbarrm \equiv \bar{N}(\Drm)$, the minimum risk can also be represented as $\Erm_{\xrm,\nbarrm} \left[ \Sigma_{\yrm | \xrm,\nbarrm} \right]$; as such, the expectations are performed over $\nbarrm$. Decompose the conditional variance as
\begin{IEEEeqnarray}{rCl}
\Sigma_{\yrm | \xrm,\nbarrm} & = & \Erm_{\yrm | \xrm,\nbarrm}[\yrm^2] - \mu_{\yrm | \xrm,\nbarrm}^2 
\end{IEEEeqnarray}
and assess the expected values of these terms separately using distributions derived from the Dirichlet prior. The first term is simply
\begin{IEEEeqnarray}{rCl}
\Erm_{\xrm,\nbarrm} \left[ \Erm_{\yrm | \xrm,\nbarrm}[\yrm^2] \right] & = & \Erm_{\yrm}[\yrm^2] = \sum_{y \in \Ycal} y^2 \left( \sum_{x \in \Xcal} \frac{\alpha(y,x)}{\alpha_0} \right) \nonumber \\
& = & \Erm_{\xrm} \big[ \Erm_{\yrm | \xrm} [ \yrm^2 ] \big] = \sum_{x \in \Xcal} \frac{\alpha'(x)}{\alpha_0} \sum_{y \in \Ycal} y^2 \frac{\alpha(y,x)}{\alpha'(x)} \nonumber \;,
\end{IEEEeqnarray}
where the different functions of $\alpha$ are represented by the PMF's of $\yrm$ and $\xrm$. Next, find, 
\begin{IEEEeqnarray}{rCl}
\Erm_{\xrm,\nbarrm} \Big[ \mu_{\yrm | \xrm,\nbarrm}^2 \Big] & = & \Erm_{\xrm} \left[ \Erm_{\nbarrm | \xrm} \left[ \frac{\big( \alpha'(\xrm) \mu_{\yrm|\xrm} + \sum_{y \in \Ycal} y \bar{\nrm}(y,\xrm) \big)^2}{\alpha'(\xrm) \big(\alpha'(\xrm) + \nrm'(\xrm) \big)^2} \right] \right] \\
& = & \Erm_{\xrm} \left[ \Erm_{\nbarrm} \left[ \frac{\alpha_0 \big( \alpha'(\xrm) \mu_{\yrm|\xrm} + \sum_{y \in \Ycal} y \bar{\nrm}(y,\xrm) \big)^2}{\alpha'(\xrm) \big(\alpha'(\xrm) + \nrm'(\xrm) \big) (\alpha_0+N)} \right] \right] \nonumber \\
& = & \Erm_{\xrm} \left[ \Erm_{\nrm'} \left[ \frac{\alpha_0 \Erm_{\nbarrm | \nrm'} \left[ \big( \alpha'(\xrm) \mu_{\yrm|\xrm} + \sum_{y \in \Ycal} y \bar{\nrm}(y,\xrm) \big)^2 \right]}{\alpha'(\xrm) \big(\alpha'(\xrm) + \nrm'(\xrm) \big) (\alpha_0+N)} \right] \right] \nonumber \\
& = & \ldots \nonumber \\
& = & \Erm_{\xrm} \left[ \frac{\alpha_0 \Erm_{\nrm'} \Big[ \nrm'(\xrm) \Erm_{\yrm|\xrm}[\yrm^2] + \big( \alpha'(\xrm) + \nrm'(\xrm) + 1 \big) \alpha'(\xrm) \mu_{\yrm|\xrm}^2 \Big]}{\alpha'(\xrm) \big(\alpha'(\xrm) + 1 \big) (\alpha_0+N)} \right] \nonumber \\
& = & \Erm_{\xrm} \left[ \frac{N \Erm_{\yrm|\xrm}[\yrm^2] + \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \alpha_0 \big) \mu_{\yrm|\xrm}^2 }{\big( \alpha'(\xrm)+1 \big) (\alpha_0+N)} \right] \nonumber \;.
\end{IEEEeqnarray}

PGR: provide additional steps?

%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\nbarrm} \Big[ \mu_{\yrm | \xrm,\nbarrm}^2 \Big] =
%\sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm} \Big[ \Erm_{\nbarrm | \xrm} \big[ \Prm_{\yrm | \xrm,\nbarrm}(y | \xrm,\nbarrm) \Prm_{\yrm | \xrm,\nbarrm}(y' | \xrm,\nbarrm) \big] \Big] \\
%= \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm} \left[ \Erm_{\nbarrm} \left[ \frac{\alpha_0 \big( \alpha(y,\xrm)+\bar{\nrm}(y,\xrm) \big) \big(\alpha(y',\xrm) + \bar{\nrm}(y',\xrm) \big)}{\alpha'(\xrm) \big(\alpha'(\xrm) + \nrm'(\xrm) \big) (\alpha_0+N)} \right] \right] \nonumber \\
%= \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm} \left[ \Erm_{\nrm'} \left[ \frac{\alpha_0 \Erm_{\nbarrm | \nrm'} \left[ \big( \alpha(y,\xrm)+\bar{\nrm}(y,\xrm) \big) \big(\alpha(y',\xrm)+\bar{\nrm}(y',\xrm) \big) \right]}{\alpha'(\xrm) \big(\alpha'(\xrm) + \nrm'(\xrm) \big) (\alpha_0+N)} \right] \right] \nonumber \\
%= \ldots \nonumber \\
%= \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm} \left[ \frac{\alpha_0 \Erm_{\nrm'} \Big[ \nrm'(\xrm) \alpha(y,\xrm) \delta[y,y'] + \big( \alpha'(\xrm) + \nrm'(\xrm) + 1 \big) \alpha(y,\xrm) \alpha(y',\xrm) \Big]}{\alpha'(\xrm)^2 \big(\alpha'(\xrm) + 1 \big) (\alpha_0+N)} \right] \nonumber \\
%= \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm} \left[ \frac{ N \alpha'(\xrm) \alpha(y,\xrm) \delta[y,y'] + \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \alpha_0 \big) \alpha(y,\xrm) \alpha(y',\xrm)}{\alpha'(\xrm)^2 \big(\alpha'(\xrm) + 1 \big) (\alpha_0+N)} \right] \nonumber \\
%= \Erm_{\xrm} \left[ \frac{N \Erm_{\yrm|\xrm}[\yrm^2] + \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \alpha_0 \big) \mu_{\yrm|\xrm}^2 }{\big( \alpha'(\xrm)+1 \big) (\alpha_0+N)} \right] \nonumber \;.
%\end{IEEEeqnarray}
%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\nbarrm} \Big[ \mu_{\yrm | \xrm,\nbarrm}^2 \Big] =
%\sum_{\bar{n} \in \bar{\Ncal}} \sum_{x \in \Xcal} \Prm_{\xrm,\nbarrm}(x,\bar{n}) \left( \sum_{y \in \Ycal} y \Prm_{\yrm | \xrm,\nbarrm}(y | x,\bar{n}) \right)^2 \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\nbarrm} \left[ \frac{\big( \alpha(y,x)+\bar{\nrm}(y,x) \big) \big(\alpha(y',x)+\bar{\nrm}(y',x) \big)}{(\alpha_0+N) \big(\alpha'(x) + \nrm'(x) \big)} \right] \nonumber \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\nrm'} \left[ \frac{\Erm_{\nbarrm | \nrm'} \left[ \big( \alpha(y,x)+\bar{\nrm}(y,x) \big) \big(\alpha(y',x)+\bar{\nrm}(y',x) \big) \right]}{(\alpha_0+N) \big(\alpha'(x) + \nrm'(x) \big)} \right] \nonumber \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \frac{\Erm_{\nrm'} \Big[ \nrm'(x) \alpha'(x) \alpha(y,x) \delta[y,y'] + \alpha'(x) \big( \alpha'(x) + \nrm'(x) + 1 \big) \alpha(y,x) \alpha(y',x) \Big]}{(\alpha_0+N) \big(\alpha'(x) + 1 \big) \alpha'(x)^2} \nonumber \\
%= \sum_{x \in \Xcal} \frac{\Erm_{\nrm'} \Big[ \nrm'(x) \Erm_{\yrm|\xrm}[\yrm^2](x) + \alpha'(x) \big( \alpha'(x) + \nrm'(x) + 1 \big) \mu_{\yrm | \xrm}^2(x) \Big]}{(\alpha_0+N) \big(\alpha'(x) + 1 \big)} \nonumber \\
%= \sum_{x \in \Xcal} \frac{N \alpha'(x) \Erm_{\yrm|\xrm}[\yrm^2](x) + \alpha'(x) \big( \alpha_0 \alpha'(x) + N \alpha'(x) + 1 \big) \mu_{\yrm | \xrm}^2(x) }{\alpha_0 (\alpha_0+N) \big(\alpha'(x) + 1 \big)} \nonumber \\
%= \Erm_{\xrm} \left[ \frac{N \Erm_{\yrm|\xrm}[\yrm^2] + \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \alpha_0 \big) \mu_{\yrm|\xrm}^2 }{(\alpha_0+N) \big( \alpha'(\xrm)+1 \big)} \right] \nonumber \;.
%\end{IEEEeqnarray}
%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\nbarrm} \Big[ \mu_{\yrm | \xrm,\nbarrm}^2 \Big] =
%\sum_{\bar{n} \in \bar{\Ncal}} \sum_{x \in \Xcal} \Prm_{\xrm,\nbarrm}(x,\bar{n}) \left( \sum_{y \in \Ycal} y \Prm_{\yrm | \xrm,\nbarrm}(y | x,\bar{n}) \right)^2 \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\nbarrm} \left[ \frac{\big( \alpha(y,x)+\bar{\nrm}(y,x) \big) \big(\alpha(y',x)+\bar{\nrm}(y',x) \big)}{(\alpha_0+N) \big(\alpha'(x) + \nrm'(x) \big)} \right] \nonumber \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\nrm'} \left[ \frac{\Erm_{\nbarrm | \nrm'} \left[ \big( \alpha(y,x)+\bar{\nrm}(y,x) \big) \big(\alpha(y',x)+\bar{\nrm}(y',x) \big) \right]}{(\alpha_0+N) \big(\alpha'(x) + \nrm'(x) \big)} \right] \nonumber \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \frac{\Erm_{\nrm'} \Big[ \nrm'(x) \frac{\alpha(y,x)}{\alpha'(x)} \delta[y,y'] + \alpha'(x) \big( \alpha'(x) + \nrm'(x) + 1 \big) \frac{\alpha(y,x)}{\alpha'(x)} \frac{\alpha(y',x)}{\alpha'(x)} \Big]}{(\alpha_0+N) \big(\alpha'(x) + 1 \big)} \nonumber \\
%= \sum_{x \in \Xcal} \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \left(\frac{\alpha'(x)}{\alpha_0}\right)  \frac{N \frac{\alpha(y,x)}{\alpha'(x)} \delta[y,y'] + \big( N \alpha'(x) + \alpha_0 \alpha'(x) + \alpha_0 \big) \frac{\alpha(y,x)}{\alpha'(x)} \frac{\alpha(y',x)}{\alpha'(x)}}{(\alpha_0+N) \big( \alpha'(x)+1 \big)} \nonumber \\
%= \sum_{x \in \Xcal} \left(\frac{\alpha'(x)}{\alpha_0}\right)  \frac{N \left( \sum_{y \in \Ycal} \frac{\alpha(y,x)}{\alpha'(x)} y^2 \right) + \big( N \alpha'(x) + \alpha_0 \alpha'(x) + \alpha_0 \big) \left( \sum_{y \in \Ycal} \frac{\alpha(y,x)}{\alpha'(x)} y \right)^2 }{(\alpha_0+N) \big( \alpha'(x)+1 \big)} \nonumber \;.
%\end{IEEEeqnarray}
The above formulation exploits the statistical characterization of the aggregation, $\nrm' \sim \DM(N,\alpha')$; also used is the property that the Dirichlet-Multinomial random process $\nbarrm$ conditioned on its aggregation $\nrm'$ yields independent  conditional DM functions $\bar{\nrm}(\cdot,x) | \nrm'(x) \sim \DM\big( \nrm'(x),\alpha(\cdot,x) \big)$.

PGR: move to appendix???

Finally, combine the two formulas to represent the mininum Bayes risk,
\begin{IEEEeqnarray}{L}
\Rcal^* = \Erm_{\xrm,\nbarrm} \left[ \Erm_{\yrm | \xrm,\nbarrm}[\yrm^2] - \mu_{\yrm | \xrm,\nbarrm}^2 \right] \\
= \Erm_{\xrm} \left[ \frac{\alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \alpha_0}{\big( \alpha'(\xrm)+1 \big) (\alpha_0+N)} \Sigma_{\yrm | \xrm} \right] \nonumber \\
= \Erm_{\xrm} \left[ \frac{\Prm_{\xrm}(\xrm) + (\alpha_0+N)^{-1}}{\Prm_{\xrm}(\xrm) + \alpha_0^{-1}} \Sigma_{\yrm | \xrm} \right] \nonumber \;.
\end{IEEEeqnarray}
The minimum risk is the expected value of the scaled conditional variance with respect to $\Prm_{\yrm | \xrm} = \alpha(\cdot,\xrm)/\alpha'(\xrm)$. The expectation is taken with respect to the prior marginal distribution $\Prm_{\xrm} = \alpha'/\alpha_0$. 

The scaling factor for each term $\Sigma_{\yrm | \xrm}$ depends on the marginal $\Prm_{\xrm}$, as well as on the prior concentration $\alpha_0$ and the number of training samples $N$. Observe that with no training data ($N = 0$), the scaling factor becomes unity and the risk is $\Rcal^* = \Erm_{\xrm} \left[ \Sigma_{\yrm | \xrm} \right]$. Conversely, as $N \to \infty$, the Bayes risk is $\Rcal^* \to \Erm_{\xrm} \left[ \frac{\Prm_{\xrm}(\xrm)}{\Prm_{\xrm}(\xrm) + \alpha_0^{-1}} \Sigma_{\yrm | \xrm} \right]$; note that this is equivalent to the irreducible risk $\Erm_{\uptheta}\big[\Rcal_{\Theta}^*(\uptheta)\big] = \Erm_{\xrm,\uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right]$. Also, as the model concentration parameter $\alpha_0 \to 0$, the risk tends to zero (for $N > 0$); as $\alpha_0 \to \infty$, the risk tends toward $\Erm_{\xrm} \left[ \Sigma_{\yrm | \xrm} \right]$.

PGR: first/second derivatives of alpha0??

To illustrate these trends, explicitly define the sets $\Ycal = \{ i/M_{\yrm} : i = 0,\ldots,M_{\yrm}-1 \}$ and $\Xcal = \{ i/M_{\xrm} : i = 0,\ldots,M_{\xrm}-1 \}$. Assume that the conditional variance $\Sigma_{\yrm | \xrm}$ is independent of $\xrm$; in this case, the squared-error becomes the conditional variance scaled by a factor dependent on the marginal distribution $\Prm_{\xrm}$, such that $\Rcal^* = \Sigma_{\yrm | \xrm} \Erm_{\xrm} \left[ \frac{\Prm_{\xrm}(\xrm) + (\alpha_0+N)^{-1}}{\Prm_{\xrm}(\xrm) + \alpha_0^{-1}} \right]$.  Figures \ref{fig:Risk_SE_Dir_IO_N_leg_a0} and \ref{fig:Risk_SE_Dir_IO_a0_leg_N} display how the risk changes with $N$ and $\alpha_0$ when $\Prm_{\yrm | \xrm}$ and $\Prm_{\xrm}$ are fixed.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_N_leg_a0.pdf}
\caption{Minimum SE Risk for different training set sizes $N$}
\label{fig:Risk_SE_Dir_IO_N_leg_a0}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_a0_leg_N.pdf}
\caption{Minimum SE Risk for different prior concentrations $\alpha_0$}
\label{fig:Risk_SE_Dir_IO_a0_leg_N}
\end{figure}

It may not seem intutitve for the risk to decrease when $\alpha_0$ is smaller -- the variance of the model $\uptheta$ increases and the prior knowledge is less definitive. This is a result of the Dirichlet PDF weight shifting towards the $|\Ycal||\Xcal|$ models which have $\ell_0$ norms satisfying $\| \theta \|_0 = 1$. Although these PMF's are maximally separated (and uncorrelated), they all have zero variance. The optimal learner \eqref{eq:f_opt_SE_dir} will simply use the empirical distribution supplied via the training data - this allows exact identification of $\uptheta$ with a single training pair.

It is also informative to visualize how the minimum squared-error changes for fixed volume of training data $N$ and a fixed prior concentration $\alpha_0$. First, consider how the risk changes with the conditional PMF $\Prm_{\yrm | \xrm}$. Figure \ref{fig:Risk_SE_Dir_IO_Pyx} demonstrates how the squared-error tends towards zero for PMFs that have $\ell_0$-norm equal to one.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_Pyx.pdf}
\caption{Minimum SE Risk for different prior means $\Prm_{\yrm | \xrm}$}
\label{fig:Risk_SE_Dir_IO_Pyx}
\end{figure}

Next, consider the effect of the marginal distribution $\Prm_{\xrm}$. Figure \ref{fig:Risk_SE_Dir_IO_Px_N_10_a0_1} demonstrates how the risk changes with this marginal PMF. Observe that the risk is maximal at the distributions satisfying $\| \Prm_{\xrm} \|_0 = 1$; the scaling factor for the conditional variance $\Sigma_{\yrm | \xrm}$ becomes $\frac{1 + (\alpha_0+N)^{-1}}{1 + \alpha_0^{-1}}$. Conversely, for $\Prm_{\xrm} = 1/|\Xcal|$ the scaling factor becomes $\frac{|\Xcal|^{-1} + (\alpha_0+N)^{-1}}{|\Xcal|^{-1} + \alpha_0^{-1}}$ and the risk is minimal. Figures \ref{fig:Risk_SE_Dir_IO_N_leg_Px} and \ref{fig:Risk_SE_Dir_IO_a0_leg_Px} show how different marginals $\Prm_{\xrm}$ affect the risk as a function of $N$ and $\alpha_0$, respectively.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_Px_N_10_a0_1.pdf}
\caption{Minimum SE Risk for different prior means $\Prm_{\xrm}$}
\label{fig:Risk_SE_Dir_IO_Px_N_10_a0_1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_N_leg_Px.pdf}
\caption{Minimum SE Risk for different training set volumes $N$}
\label{fig:Risk_SE_Dir_IO_N_leg_Px}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_Dir_IO_a0_leg_Px.pdf}
\caption{Minimum SE Risk for different prior concentrations $\alpha_0$}
\label{fig:Risk_SE_Dir_IO_a0_leg_Px}
\end{figure}




\paragraph{Uniform Prior}

For the uniform model prior, the risk reduces to
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & \frac{|\Ycal| \big( N/|\Xcal| + |\Ycal| + 1 \big)}{\big( |\Ycal| + 1 \big) \big( N/|\Xcal| + |\Ycal| \big)} \left[ \left( \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y^2 \right) - \left( \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y \right)^2 \right] \\
& = & \frac{1 + \big( N/|\Xcal| + |\Ycal| \big)^{-1}}{1 + |\Ycal|^{-1}} \left[ \left( \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y^2 \right) - \left( \frac{1}{|\Ycal|} \sum_{y \in \Ycal} y \right)^2 \right] \nonumber \;.
\end{IEEEeqnarray}
Since all possible values of $\xrm$ are equally probable and the conditional probability $\Prm_{\yrm | \xrm}$ is uniform and independent of $\xrm$, the risk simply becomes the variance of the set $\Ycal$ scaled by a factor dependent on $|\Ycal|$ and on $N/|\Xcal|$. Without training data ($N=0$), the scaling is unity; as $N/|\Xcal| \to \infty$, the scaling factor is $\big( 1 + |\Ycal|^{-1} \big)^{-1}$.

To visualize the performance, use the explicit sets $\Ycal$ and $\Xcal$ defined earlier. The conditional variance becomes
\begin{equation}
\Sigma_{\yrm | \xrm} = \frac{|\Ycal|^2 - 1}{12 |\Ycal|^2} = \frac{1 - |\Ycal|^{-2}}{12} 
\end{equation}
and the minimum risk is expressed as
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & \frac{\big(1 - |\Ycal|^{-1}\big) \Big(1 + \big(N/|\Xcal| + |\Ycal|\big)^{-1} \Big)}{12} \\
& = & \left(\frac{|\Ycal|}{N/|\Xcal| + |\Ycal|}\right) \frac{1 - |\Ycal|^{-2}}{12} + \left(\frac{N/|\Xcal|}{N/|\Xcal| + |\Ycal|}\right) \frac{1 - |\Ycal|^{-1}}{12} \nonumber \;.
\end{IEEEeqnarray}

Interestingly, the minimum squared-error for the uniform prior can be represented as a convex combination of two separate risk values with weighting factors dependent on $|\Ycal|$ and $N/|\Xcal|$. Thus for a uniform prior, the risk depends on the number of elements in $\Ycal$ and the number of training samples ``per element of $\Xcal$''. Note the relationship of these weighting factors to those of the conditional PMF $\Prm_{\yrm | \xrm,\Drm}$, which depend on $\alpha'(\xrm)$ and on $N'(\xrm;\Drm)$. For the uniform prior, $\alpha'(\xrm) = |\Ycal|$ and $\Erm_{\Drm}\big[ N'(\Drm) \big] = N/|\Xcal|$.

The first risk is the conditional variance $\Sigma_{\yrm|\xrm}$ - this is intuitively satisfying as the corresponding weight becomes unity when $N=0$. The second risk is the squared-error with infinite training data. Note that the reduction of the risk between these two extreme cases is modest, and that the attenuating factor increases towards unity for applications with more possible outcomes. Figure \ref{fig:Risk_SE_uniform_N_lim} illustrates the difference between these cases.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_SE_uniform_N_lim.pdf}
\caption{Minimum SE Risk, Uniform Prior, zero and infinite training data}
\label{fig:Risk_SE_uniform_N_lim}
\end{figure}


PGR: additional figures for uniform case?

%Figure \ref{fig:Risk_SE_IO_N} displays how the risk increases with $M_x$; Figure \ref{fig:Risk_SE_IO_N-Mx} makes the dependency on $N/M_x$ explicit.
%
%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_SE_IO_N.pdf}
%\caption{Optimal SE Risk for different $|\Xcal|$, Uniform Prior}
%\label{fig:Risk_SE_IO_N}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_SE_IO_N-Mx.pdf}
%\caption{Optimal SE Risk vs $N/|\Xcal|$, Uniform Prior}
%\label{fig:Risk_SE_IO_N-Mx}
%\end{figure}




\subsubsection{Conditional Squared-Error for a Dirichlet-based Estimator}

Having derived the optimal estimator based on a Dirichlet model prior, it is informative to consider the conditional risk $\Rcal_{\Theta}(f^* ; \uptheta)$ and analyze how different prior parametrizations $\alpha$ influence the squared-error for different models $\theta$. Starting from the conditional squared-error risk \eqref{eq:risk_cond_SE} and substituting the Bayesian estimator \eqref{eq:f_opt_SE}, the formula simplifies to
\begin{IEEEeqnarray}{rCl} \label{eq:risk_cond_SE_dir}
\Rcal_{\Theta}(f^* ; \uptheta) & = & \Rcal_{\Theta}^*(\uptheta) + \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f^*(\xrm;\Drm) - f_{\Theta}(\xrm;\uptheta) \big)^2 \Big] \\
& = & \Erm_{\xrm | \uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] + \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( \mu_{\yrm | \xrm,\Drm} - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \nonumber \;.
\end{IEEEeqnarray}
Defining the excess conditional risk $\Rcal_{\Theta, \mathrm{ex}}(f ; \uptheta) \equiv \Rcal_{\Theta}(f ; \uptheta) - \Rcal_{\Theta}^*(\uptheta)$, the second term above is $\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) = \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( \mu_{\yrm | \xrm,\Drm} - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big]$, the average squared bias between the Bayesian predictive mean and the true predictive mean.

PGR: general above? move before dir? simplify form in terms of Ptheta?

PGR: define excess clairvoyant/bayesian risk up front before dir?

Evaluation of the excess risk for an estimator based on the Dirichlet prior will be performed using the sufficient statistic $\nbarrm$ in place of the training set $\Drm$. Using the random process $\Delta(\xrm,\nbarrm,\uptheta) \equiv \Prm_{\yrm | \xrm,\nbarrm} - \Prm_{\yrm | \xrm,\uptheta} \in \Rbb^{\Ycal}$ introduced in \ref{sec:predictive_est}, the term is expressed as
\begin{IEEEeqnarray}{L} \label{eq:risk_cond_SE_dir_ex}
\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) = \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( \mu_{\yrm | \xrm,\Drm} - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \\
\quad = \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm,\nbarrm | \uptheta} \Big[ \Delta(y;\xrm,\nbarrm,\uptheta) \Delta(y';\xrm,\nbarrm,\uptheta) \Big] \nonumber \\
\quad = \sum_{y \in \Ycal} y \sum_{y' \in \Ycal} y' \Erm_{\xrm,\nrm' | \uptheta}\Big[ \mathcal{E}(y,y' ; \xrm,\nrm',\uptheta) \Big] \nonumber \\
\quad = \Erm_{\xrm | \uptheta}\left[ \Sigma_{\yrm | \xrm,\uptheta} \Erm_{\nrm'(\xrm) | \uptheta'(\xrm)}\left[ \frac{\nrm'(\xrm)}{\big( \alpha'(\xrm) + \nrm'(\xrm) \big)^2} \right] \right] \nonumber \\
\qquad \qquad + \Erm_{\xrm | \uptheta}\left[ \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \Erm_{\nrm'(\xrm) | \uptheta'(\xrm)}\left[ \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)}\right)^2 \right] \right] \nonumber \;,
%\quad = \Erm_{\xrm | \uptheta}\left[ \Sigma_{\yrm | \xrm,\uptheta} \Erm_{\nrm'(\xrm) | \uptheta}\left[ \frac{\alpha_0^{-2} \nrm'(\xrm)}{\big( \Prm_{\xrm}(\xrm) + \alpha_0^{-1} \nrm'(\xrm) \big)^2} \right] \right] \nonumber \\
%\qquad \qquad + \Erm_{\xrm | \uptheta}\left[ \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \Erm_{\nrm'(\xrm) | \uptheta}\left[ \left(\frac{\Prm_{\xrm}(\xrm)}{\Prm_{\xrm}(\xrm) + \alpha_0^{-1} \nrm'(\xrm)}\right)^2 \right] \right] \nonumber \;,
\end{IEEEeqnarray}
where the function $\mathcal{E}$ is defined in \eqref{eq:predictive_del_sq}. 

The excess conditional risk can thus be represented as the conditional expectation (with respect to $\Prm_{\xrm | \uptheta}$) of a sum of two random functions of $\xrm$. The first function measures the additional variance beyond that of the clairvoyant estimator (i.e. the clairvoyant squared-error); like the clairvoyant risk, it depends on $\Sigma_{\yrm | \xrm,\uptheta}$, the conditional variance of the clairvoyant estimate for a given observation of $\xrm$. The second function is dependent on the squared bias between the clairvoyant estimate $\mu_{\yrm | \xrm,\uptheta}$ and the data-independent estimate $\mu_{\yrm | \xrm}$. This term alone is influenced by the data-independent Bayes predictive distribution $\Prm_{\yrm | \xrm} = \alpha(\cdot,\xrm) / \alpha'(\xrm)$.

The two second-order (in terms of $y$) terms are scaled by factors dependent on the prior concentrations $\alpha'(x)$ and on $\uptheta'(x)$ and $N$ via conditional expectations with respect to $\nrm'(x)$. Note that by the aggregation property of multinomial distributions, the random variable $\nrm'(x) | \uptheta'(x) \sim \Bi \big(N,\uptheta'(x)\big)$. Closed-forms have not been found for the function expectations of binomial random variables above.

PGR: binomial citations?

It is informative to consider the trends in the conditional squared-error risk \eqref{eq:risk_cond_SE_dir_ex} for different volumes of training data $N$ and for different selections of $\alpha$.


First consider how the excess risk changes with the training volume $N$. For $N=0$, it is evident that the excess risk is $\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) \to \Erm_{\xrm | \uptheta}\left[ \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \right]$,  the expected squared bias between the clairvoyant and data-independent estimators. As $N$ tends to infinity, the binomial distribution controlling the scaling factors  concentrates at $\nrm'(x) \approx N \theta'(x)$; as such, the two expectations of interest tend to zero and thus $\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) \to 0$. This desirable property of the estimator is a consequence of the full support of the Dirichlet prior, ensuring that the model posterior concentrates at the empirical PMF.

Another interesting point regarding the depedency of the excess conditional risk on $N$ is that, depending on the learner parameterization, there may be a local maximum. Consider the trivial case of $|\Xcal| = 1$ - treating $N$ as a real number, there would be a maximum at 
\begin{equation}
N \equiv \alpha'(x) \left( 1 - 2 \alpha'(x) \frac{\left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2}{\Sigma_{\yrm | \xrm,\uptheta}} \right) \;.
\end{equation}
Note that as the squared-difference between the prior mean and true mean increases, the maximizing value decreases (even below zero). Thus, the worse the prior estimate, the more likely the excess squared-error will decrease monotonically with $N$. Conversely, if the prior estimate is accurate, a local maximum may occur and additional training data may (temporarily) compromize the estimator performance. Also consider the effect of prior concentration; subjective priors with sufficiently high $\alpha'(x)$ will not have the local maxima.

The excess risk at this potentially non-integral value would be 
\begin{equation}
\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) \to \Erm_{\xrm | \uptheta}\left[ \frac{\frac{1}{\alpha'(\xrm)} - \frac{\left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2}{\Sigma_{\yrm | \xrm,\uptheta}}}{4\left( 1 - \frac{\left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2}{\Sigma_{\yrm | \xrm,\uptheta}} \right)} \Sigma_{\yrm | \xrm,\uptheta} \right] \;.
\end{equation}

PGR: better form above???

Figures \ref{fig:Risk_cond_SE_Dir_N_leg_a0_unbiased} and \ref{fig:Risk_cond_SE_Dir_N_leg_a0_biased} exemplify the excess conditional squared-error as a function of $N$ for estimators based on Dirichlet priors of varying concentration $\alpha'(x)$. The former shows local maxima for an unbiased estimator; note that higher concentration results in superior performance. The latter uses biased estimators and as such, learners based on low concentration achieve lower risk.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_SE_Dir_N_leg_a0_unbiased.pdf}
\caption{Conditional SE Risk versus $N$, unbiased Dirichlet estimators of varying concentration}
\label{fig:Risk_cond_SE_Dir_N_leg_a0_unbiased}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_SE_Dir_N_leg_a0_biased.pdf}
\caption{Conditional SE Risk versus $N$, biased Dirichlet estimators of varying concentration}
\label{fig:Risk_cond_SE_Dir_N_leg_a0_biased}
\end{figure}







Next consider the effects of the Dirichlet prior parameters. The analysis will interpret the Dirichlet parameters as the conditional prior distributions $\alpha(\cdot,x)/\alpha'(x)$ and their concentrations $\alpha'(x)$. 

First consider the conditional prior PMF's $\alpha(\cdot,x) / \alpha'(x)$; as shown, they manifest themselves in the risk through the squared estimator bias. It is clear that regardless of how the values $\alpha'(x)$ are chosen, the best selections for these conditional priors must have first moments matching those of the corresponding clarivoyant predictive distrbutions $\Prm_{\yrm | \xrm,\uptheta}$ for each $x \in \Xcal$. Such estimators are unbiased; as a result, the excess conditional risk is thus equivalent to the first term in \eqref{eq:risk_cond_SE_dir_ex}, measuring additional variance due to model uncertainty.



The other user-selected Dirichlet parameters $\alpha'(x)$ are the concentration parameters for the corresponding conditional distributions; they control important bias/variance trade-offs via the two scaling factors in \eqref{eq:risk_cond_SE_dir_ex}. First, consider the asymptotic trends.

Consider how the excess risk tends as the priors become maximally concentrated. As the parameters $\alpha'(x) \to \infty$, the excess risk tends to $\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) \to \Erm_{\xrm | \uptheta}\left[ \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \right]$, the expected conditional squared-error between the means of the Bayesian predicitve PMF and the clairvoyant predictive PMF. This is intuitive given that the estimator tends toward a data-independent solution; analagous to the discussion in Section \ref{sec:predictive_est}, the estimator may be biased, but will have no variance due to the training data statistics.

Conversely, if concentrations $\alpha'(x) \to 0$ are chosen, the Bayesian estimate tends to the empirical mean, independent of $\alpha(\cdot,\xrm) / \alpha'(\xrm)$, and the excess risk tends to
\begin{IEEEeqnarray}{rCl}
\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) & \to & \Erm_{\xrm | \uptheta}\left[ \Sigma_{\yrm | \xrm,\uptheta} \sum_{n=1}^N \binom{N}{n} \uptheta'(\xrm)^n \big( 1 - \uptheta'(\xrm) \big)^{N-n} \frac{1}{n} \right] \nonumber \\
&& \qquad + \Erm_{\xrm | \uptheta}\left[ \big( 1 - \uptheta'(\xrm) \big)^N \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \right] \nonumber \;.
\end{IEEEeqnarray}
Note that the first term's scaling factor is proportionate to the first inverse moment of a positive binomial random variable \cite{stephan}. The second term's scaling factor tends to $\Prm_{\nrm'(\xrm) | \uptheta'(\xrm)}\big( 0 | \theta'(x) \big)$, the probability that no training samples are observed matching the value $\xrm$. As $N$ increases, this term tends to zero, the risk due to the prior estimate bias decreases, and the excess risk becomes a function of $\uptheta$ only.



Of further interest are the values $\alpha'(x)$ that minimize the excesss squared-error for a given prior conditional distribution $\alpha(\cdot,x) / \alpha'(x)$. With the asymptotic values of the excess risk known, all that remains is to determine any local minima. Since the excess risk is a sum of $|\Xcal|$ terms of identical form, each dependent on their own concentration $\alpha'(x)$, only one component needs to be minimized. 

PGR: add the derivative details below???

Calculating the first derivative with respect to $\alpha'(x)$, it can be shown that for $N > 0$ and $\theta'(x) > 0$, only one stationary point exists, at 
\begin{equation} \label{eq:alpha_x_min_Rex}
\alpha'(x) \equiv \frac{\Sigma_{\yrm | \xrm,\uptheta}}{\left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2} \;.
\end{equation}
Calculation of the second derivative confirms that this value is a local minimum. Furthermore, the excess risk evaluated at these values is 
\begin{equation}
\Rcal_{\Theta, \mathrm{ex}}(f^* ; \uptheta) = \Erm_{\xrm | \uptheta}\left[ \Erm_{\nrm'(\xrm) | \uptheta'(\xrm)}\left[ \frac{1}{\nrm'(\xrm)\Sigma_{\yrm | \xrm,\uptheta}^{-1} + \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^{-2}} \right] \right] \;,
\end{equation}
which can be easily shown to be less than both the asymptotic values for $\alpha'(x) \to 0$ and $\alpha'(x) \to \infty$. Thus the concentation values \eqref{eq:alpha_x_min_Rex} provide the minimum excess risk for the given prior conditional distributions.

Note that the minimizing concentration values $\alpha'(x)$ are inversely proportional to the squared-bias of the prior conditional mean. This is sensible; the better the match between the true and prior predictive distributions, the more confidence should be expressed. Also, low concentrations are preferable when the model has low conditional variance; these models can be quickly identified with learners prioritizing the emprical PMF estimate over the prior estimate. Additionally, note that these values $\alpha'(x)$ do not depend on the training volume $N$.



%\Erm_{\xrm | \uptheta}\left[ \Sigma_{\yrm | \xrm,\uptheta} \Erm_{\nrm'(\xrm) | \uptheta'(\xrm)}\left[ \frac{\nrm'(\xrm)}{\big( \alpha'(\xrm) + \nrm'(\xrm) \big)^2} \right] \right] \nonumber \\
%\qquad \qquad + \Erm_{\xrm | \uptheta}\left[ \left( \mu_{\yrm | \xrm} - \mu_{\yrm | \xrm,\uptheta} \right)^2 \Erm_{\nrm'(\xrm) | \uptheta'(\xrm)}\left[ \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + \nrm'(\xrm)}\right)^2 \right] \right] \nonumber


Figures \ref{fig:Risk_cond_SE_Dir_a0_leg_N_unbiased} and \ref{fig:Risk_cond_SE_Dir_a0_leg_N_biased} show how the excess conditional squared-error trends as a function of the Dirichlet learner concentration. Note that the latter is based on a biased prior estimate and thus the optimal Dirichlet concentration value is lower.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_SE_Dir_a0_leg_N_unbiased.pdf}
\caption{Conditional SE Risk versus $\alpha'(x)$, unbiased Dirichlet estimator using varying training set volumes}
\label{fig:Risk_cond_SE_Dir_a0_leg_N_unbiased}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_SE_Dir_a0_leg_N_biased.pdf}
\caption{Conditional SE Risk versus $\alpha'(x)$, biased Dirichlet estimator using varying training set volumes}
\label{fig:Risk_cond_SE_Dir_a0_leg_N_biased}
\end{figure}

PGR: plot captions, alpha zero or x???









\newpage

PGR: newpage






\subsection{Classification: the 0-1 Loss}

This section derives 0-1 loss classifiers based on the Dirichlet prior distribution and assesses their performance.

\subsubsection{Optimal Hypothesis: Conditional Maximum \emph{a posteriori}}

PGR: decision region figures??

PGR: weighted conditional majority decision

To determine the optimal learning function, the 0-1 loss from Equation \eqref{eq:loss_01} is substituted into Equation \eqref{eq:E_y|xD L} and Equation \eqref{eq:f_opt_xD} to find
\begin{IEEEeqnarray}{rCl} \label{eq:f_opt_01_dir}
f^*(x;D) & = & \argmax_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | x,D) \\
& = & \argmax_{y \in \Ycal} \frac{\alpha(y,x) + \bar{N}(y,x;D)}{\alpha'(x) + N'(x;D)} \nonumber \\
& = & \argmax_{y \in \Ycal} \left( \alpha(y,x) + \bar{N}(y,x;D) \right) \nonumber \;.
\end{IEEEeqnarray}
Using the Dirichlet prior, different classes are ``scored'' by counting the number of training samples with a value of $\Xrm_n$ matching that of $\xrm$ and combining with the prior parameters $\alpha(\cdot,\xrm)$.  



\paragraph{Uniform Prior}

When the uniform prior is used, the Bayes classifier simplifies to 
\begin{IEEEeqnarray}{rCl}
f^*(x;D) & = & \argmax_{y \in \Ycal} \bar{N}(y,x;D) \;,
\end{IEEEeqnarray}
a conditional majority decision which chooses the class from $\Ycal$ most often represented among training set samples $\Drm$ with a matching input value $\xrm$. This is intuitive, as the model PDF parameter $\alpha$ imparts no confidence as to which classes may be most likely.







\subsubsection{Minimum Risk: Probability of Error}

PGR: GENERATE NON-SIM FIGS!!!!!!!

PGR: DIR SIM FIGS COMMENTED!!!

PGR: no closed-forms found???


Evaluating the minimum risk \eqref{eq:risk_min_01} using the distributions derived from the Dirichlet prior, the Bayes minimum probability of error is 
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & 1 - \Erm_{\xrm,\Drm} \left[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \right] \\
& = & 1 - \Erm_{\xrm,\nbarrm} \left[ \frac{ \max_{y \in \Ycal} \big( \alpha(y,x) + \bar{\nrm}(y,x) \big)}{\alpha'(x) + \nrm'(x)} \right] \nonumber \\
& = & 1 - \sum_{x \in \Xcal} \frac{\Erm_{\nbarrm} \Big[ \max_{y \in \Ycal} \big( \alpha(y,x) + \bar{\nrm}(y,x) \big) \Big]}{\alpha_0 + N} \nonumber \;.
\end{IEEEeqnarray}
Figures \ref{fig:Risk_01_Dir_N_leg_a0} and \ref{fig:Risk_01_Dir_a0_leg_N} plot the minimum Bayes probability of error against training data volume $N$ and prior concentration $\alpha_0$, respectively. Note that for $N = 0$, the Bayes risk is $\Rcal^* = 1 - \sum_{x \in \Xcal} \frac{\max_{y \in \Ycal} \alpha(y,x)}{\alpha_0}$. Additionally, consider the risk for maximal/minimal values of the Dirichlet concentration. For $\alpha_0 \to 0$ (and $N > 1$), the risk is $\Rcal^* = 0$; conversely, for $\alpha_0 \to \infty$, the risk tends to $\Rcal^* \to 1 - \sum_{x \in \Xcal} \frac{\max_{y \in \Ycal} \alpha(y,x)}{\alpha_0}$. These trends can be visualized in Figures \ref{fig:Risk_01_Dir_Pyx__a0_high} and \ref{fig:Risk_01_Dir_Pyx__a0_low}.

PGR: risk for $N \to \infty$?

PGR: missing info for Dir gen graphics? fixed y given x conditional alpha???

PGR: comment on simulation!



\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_Dir_N_leg_a0.pdf}
\caption{Minimum 0-1 Risk for different training data volumes $N$}
\label{fig:Risk_01_Dir_N_leg_a0}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_Dir_a0_leg_N.pdf}
\caption{Minimum 0-1 Risk for different prior concentrations $\alpha_0$}
\label{fig:Risk_01_Dir_a0_leg_N}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_Dir_Pyx__a0_high.pdf}
\caption{Minimum 0-1 Risk for different prior means $\Prm_{\yrm | \xrm}$}
\label{fig:Risk_01_Dir_Pyx__a0_high}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_Dir_Pyx__a0_low.pdf}
\caption{Minimum 0-1 Risk for different prior means $\Prm_{\yrm | \xrm}$}
\label{fig:Risk_01_Dir_Pyx__a0_low}
\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_01_Dir_muTheta_N_1000_a0_3.pdf}
%\caption{Minimum 0-1 Risk vs $\mu_{\uptheta}$ (sim)}
%\label{fig:Risk_01_Dir_muTheta_N_1000_a0_3}
%\end{figure}
%
%
%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_01_Dir_IO_N_leg_Px.pdf}
%\caption{Minimum 0-1 Risk vs $N$ (sim)}
%\label{fig:Risk_01_Dir_IO_N_leg_Px}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_01_Dir_IO_a0_leg_Px.pdf}
%\caption{Minimum 0-1 Risk vs $\alpha_0$ (sim)}
%\label{fig:Risk_01_Dir_IO_a0_leg_Px}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Risk_01_Dir_IO_Px_N_10_a0_1.pdf}
%\caption{Minimum 0-1 Risk vs $\Prm(x)$ (sim)}
%\label{fig:Risk_01_Dir_IO_Px_N_10_a0_1}
%\end{figure}








\paragraph{Uniform Prior}

PGR: COMPUTATIONAL COMPLEXITY savings for risk formula?

PGR: Can uniform minimal risk be approximated as a function of My and Mx/N, as is for SE loss???

PGR: use Mcal not binom!

PGR: add nmax CDF fig!


Using the uniform prior, the minimum Bayes 0-1 risk is 
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & 1 - \Erm_{\xrm,\Drm} \left[ \max_{y \in \Ycal} \Prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \right] \\
& = & 1 - \sum_{x \in \Xcal} \frac{\Erm_{\nbarrm} \big[ \max_{y \in \Ycal} \bar{\nrm}(y,x) \big] + 1}{|\Ycal||\Xcal| + N} \nonumber \\
& = & 1 - \frac{1 + |\Xcal|^{-1} \sum_{x \in \Xcal} \Erm_{\nbarrm} \big[ \max_{y \in \Ycal} \bar{\nrm}(y,x) \big]}{|\Ycal| + N/|\Xcal|} \nonumber \;.
\end{IEEEeqnarray}
The expectation operates on the maximum value from a subset of a uniform Dirichlet-Multinomial random process. Via the Dirichlet-Multinomial aggregation property \cite{johnson}, a consequence of the the uniform PMF $\Prm_{\nbarrm}$ is that the individual segments $\nbarrm(\cdot,x)$ are identically distributed; thus, the expectation will be same for every value $x$.

To evaluate this expectation, new random variables $\nbarrm_{\max}(x) \equiv \max_{y \in \Ycal} \nbarrm(y,x)$ are introduced and characterized by their identical PMF. To this end, the probability of the event $\Prm\big( \nbarrm_{\max}(x) \geq n \big) = \Prm\big( \cup_{y \in \Ycal} \{ \nbarrm(y,x) \geq n \} \big)$ will be determined. As the distribution of $\nbarrm$ is uniform, the event probability is proportionate to the cardinality of the set $\cup_{y \in \Ycal} \{ \bar{n}: \bar{n}(y,x) \geq n \}$. Using the inclusion-exclusion principle \cite{brualdi}, the cardinality is represented as
\begin{IEEEeqnarray}{L}
\big| \cup_{y \in \Ycal} \{ \bar{n} : \bar{n}(y,x) \geq n \} \big| \\
\quad = \begin{cases} \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} & \mathrm{if} \ n < 0, \\ \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \binom{N-mn+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1} H\Big( \big\lfloor\frac{N}{m}\big\rfloor - n \Big) & \mathrm{if} \ 0 \leq n \leq N, \\ 0 & \mathrm{if} \ n > N, \end{cases} \nonumber
\end{IEEEeqnarray}
where $H: \Zbb \mapsto \{0,1\}$ is the discrete Heaviside step function. For $n < 0$, the cardinality is equivalent to $|\bar{\Ncal}|$. 

For $0 \leq n < N$, the cardinality is an alternating binomial summation where the $m^\mathrm{th}$ term accounts for the different intersections of $m$ of the $|\Ycal|$ individual sets $\{ \bar{n} : \bar{n}(y,x) \geq n \}$. Observe that the cardinality of the intersections is only dependent on the number of contributing sets $m$ and not on which sets intersect. Furthermore, note the dependency of the intersection cardinalities on the argument $n$. The step function contributes such that if $n > \big\lfloor\frac{N}{m}\big\rfloor$, only up to $m-1$ individual sets will intersect. The binomial coefficient $\Mcal\big( \{N-mn,|\Ycal||\Xcal|-1\} \big)$ provides the intersection cardinality for a given $m$; note the similarity to the cardinality $|\bar{\Ncal}|$ - the only difference is the number of points characterizing the $|\Ycal||\Xcal|-1$ dimensional region.

The probability of interest can thus be expressed as
\begin{IEEEeqnarray}{L}
\Prm\big( \nbarrm_{\max}(x) \geq n \big) = \binom{N+|\Ycal||\Xcal|-1}{|\Ycal||\Xcal|-1}^{-1} \big| \cup_{y \in \Ycal} \{ \bar{n} : \bar{n}(y,x) \geq n \} \big| \\
\quad = \begin{cases} 1 & \mathrm{if} \ n < 0, \\ \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big) H\Big( \big\lfloor\frac{N}{m}\big\rfloor - n \Big) & \mathrm{if} \ 0 \leq n \leq N, \\ 0 & \mathrm{if} \ n > N. \end{cases} \nonumber
\end{IEEEeqnarray}


PGR: use Mcal op?

PGR: Heaviside reference?

As the PMF of $\nbarrm_{\max}(x)$ has support on $n \in [0,\ldots,N]$, the expectation over $\nbarrm$ is evaluated as
\begin{IEEEeqnarray}{rCl}
\Erm_{\nbarrm}\big[ \nbarrm_{\max}(x) \big] & = & \sum_{n=0}^N n \Big( \Prm\big( \nbarrm_{\max}(x) \geq n \big) - \Prm\big( \nbarrm_{\max}(x) \geq n+1 \big) \Big) \\
& = & -1 + \sum_{n=0}^N \Prm\big( \nbarrm_{\max}(x) \geq n \big) \nonumber \\
& = & -1 + \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big) \nonumber 
\end{IEEEeqnarray}
and the minimum 0-1 risk is
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & 1 - \frac{\sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big)}{|\Ycal| + N/|\Xcal|} \; .
\end{IEEEeqnarray}




It is informative to express the risk for minimal and maximal volumes of training data. Using the binomial summation identity 
\begin{equation}
\sum_{m=0}^M \binom{M}{m} (-1)^m g(m) = 0 \; ,
\end{equation}
where $g$ is a polynomial function of degree less than $M$ \cite{graham}, it can be shown that for $N = 0$, the minimum risk is $\Rcal^*  = 1 - |\Ycal|^{-1}$. This is sensible, as the classes are equiprobable with $\Prm_{\yrm} = |\Ycal|^{-1}$.

PGR: use ruiz citation for identity?

PGR: find irreducible risk explicitly from theta?

To find the risk for $N \to \infty$, note that
\begin{IEEEeqnarray}{L}
\lim_{N \to \infty} \big( |\Ycal| + N/|\Xcal| \big)^{-1} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \prod_{l=1}^{|\Ycal||\Xcal|-1} \Big( 1-\frac{mn}{N+l} \Big) \\
\qquad = \lim_{N/m \to \infty} \frac{|\Xcal|}{m} \sum_{n=0}^{\big\lfloor\frac{N}{m}\big\rfloor} \left( 1 - \frac{mn}{N} \right)^{|\Ycal||\Xcal|-1} \frac{m}{N} \nonumber \\
\qquad = \frac{|\Xcal|}{m} \int_0^1 (1-t)^{|\Ycal||\Xcal|-1} \mathrm{d} t \nonumber \\
\qquad = \frac{1}{m|\Ycal|} \nonumber \;.
\end{IEEEeqnarray}
The irreducible 0-1 risk for the uniform prior tends toward
\begin{IEEEeqnarray}{rCl}
\Rcal^* & \to & 1 - |\Ycal|^{-1} \sum_{m=1}^{|\Ycal|} \binom{|\Ycal|}{m} (-1)^{m-1} m^{-1} \\
& = & 1 - |\Ycal|^{-1} \sum_{m=1}^{|\Ycal|} m^{-1} \nonumber \;,
\end{IEEEeqnarray}
providing a lower bound for the achievable 0-1 Bayes risk. The above formulation has made use of the alternating summation identity from \cite{roman} to display the risk with a form including the $|\Ycal|^\mathrm{th}$ harmonic number $H_{|\Ycal|} \equiv \sum_{m=1}^{|\Ycal|} m^{-1}$. Observe that the irreducible risk does not depend on the cardinality $|\Xcal|$.

PGR: harmonic reference?


Figure \ref{fig:Risk_01_uni_N_leg_My} demonstrates how the minimum 0-1 risk decreases with training volume $N$; observe that the risk is more severe for sequences corresponding to higher $|\Ycal|$. It is sensible that the probability of error should increase when more classes have to be considered. Figure \ref{fig:Risk_01_uni_N_leg_Mx} illustrates the minimum risk with multiple sequences for different cardinalities $|\Xcal|$. Note that risk increases with $|\Xcal|$. Considering $\Erm_{\Drm}\big[N'(\Drm)\big] = \mu_{\nrm'} = N/|\Xcal|$, this should be intuitive - each conditional empirical distribution $\bar{N}(\cdot,x;D) / N'(x;D)$ is forced to approximate $\tilde{\uptheta}(x)$ with less data.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_uni_N_leg_My.pdf}
\caption{Minimum 0-1 Risk vs training set volume $N$}
\label{fig:Risk_01_uni_N_leg_My}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_uni_N_leg_Mx.pdf}
\caption{Minimum 0-1 Risk vs training set volume $N$}
\label{fig:Risk_01_uni_N_leg_Mx}
\end{figure}

Further insight into how $|\Xcal|$ affects the risk can be acquired by plotting the risk as a function of $N/|\Xcal|$. In Figure \ref{fig:Risk_01_uni_N-Mx}, it is shown that the minimal risk can be approximated by a function dependent only on $N/|\Xcal|$; of the series plotted, only the series for $|\Xcal| = 1$ shows notable non-negligible from the others.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_uni_N-Mx.pdf}
\caption{Minimum 0-1 Risk vs $N/|\Xcal|$}
\label{fig:Risk_01_uni_N-Mx}
\end{figure}


It is also informative to graph the $N=0$ and $N \to \infty$ minimum risk as a function of $|\Ycal|$; both formulas are independent of $|\Xcal|$. Figure \ref{fig:Risk_01_uni_N_bounds} displays these bounds; note the margin in the probability of error between the optimal $N=0$ and $N \to \infty$ classifiers. For $|\Ycal| = 2$ binary classification, both sequences are at their minimum and infinite training data provides a reduction in expected probability of error from 0.5 to 0.25. As $|\Ycal|$ increases, the classification risk for both the $N=0$ and $N \to \infty$ cases tend to unity and the error reduction for $N \to \infty$ decreases. 





\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_01_uni_N_bounds.pdf}
\caption{Minimum 0-1 Risk vs $|\Ycal|$}
\label{fig:Risk_01_uni_N_bounds}
\end{figure}




\newpage
PGR: newpage

\subsubsection{Conditional Probability of Error for a Dirichlet-based Classifier}

PGR: INCOMPLETE

PGR: comment on alpha0 versus alphax simplification


Substituting the optimal Dirichlet-based classifier into the formula for the conditional probability of error \ref{eq:risk_cond_01}, the risk is
\begin{IEEEeqnarray}{rCl}
\Rcal_{\Theta}(f ; \uptheta) & = & 1 - \sum_{x \in \Xcal} \uptheta'(x) \Erm_{\nbarrm | \uptheta} \bigg[ \tilde{\uptheta}\Big( \argmax_{y \in \Ycal} \big( \nbarrm(y,x) + \alpha(y,x) \big) ;x \Big) \bigg] \;.
\end{IEEEeqnarray}
Figures \ref{fig:Risk_cond_01_Dir_N_leg_a0__subj_good} and \ref{fig:Risk_cond_01_Dir_N_leg_a0__subj_bad} show how the conditional risk trends for classifiers based on well-matched and poorly-matched informative Dirichlet priors, respectively. Note that the well-matched prior does better with higher prior concentrations $\alpha_0$; this is reflective of the fact that the maximizing arguments $y \in \Ycal$ of both the true model $\tilde{\theta}(x)$ and the prior mean $\alpha(\cdot,x) / \alpha'(x)$ are the same.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_01_Dir_N_leg_a0__subj_good.pdf}
\caption{Excess conditional probability of error, well-matched informative Dirichlet-based classifier}
\label{fig:Risk_cond_01_Dir_N_leg_a0__subj_good}
\end{figure}
%
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_01_Dir_N_leg_a0__subj_bad.pdf}
\caption{Excess conditional probability of error, poorly-matched informative Dirichlet-based classifier}
\label{fig:Risk_cond_01_Dir_N_leg_a0__subj_bad}
\end{figure}

Also, it is important to consider how a given classifier performs for varying models $\theta$. Figures \ref{fig:Risk_cond_ex_01_Dir_theta__uni} and \ref{fig:Risk_cond_ex_01_Dir_theta__subj} demonstrate the excess conditional probability of error achieved by the conditional majority decision (based on a non-informative Dirichlet prior) and by a classifier derived from an informative Dirichlet prior, respectively. Note that while the former has fewer models for which the error is critically high, the latter has more models for which the clairvoyant risk $\Rcal_{\Theta}^*(\theta)$ is achieved. This a fundamental trade-off between Bayesian learners based on non-informative versus informative priors.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_ex_01_Dir_theta__uni.pdf}
\caption{Excess conditional probability of error, conditional majority decision}
\label{fig:Risk_cond_ex_01_Dir_theta__uni}
\end{figure}
%
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{Risk_cond_ex_01_Dir_theta__subj.pdf}
\caption{Excess conditional probability of error, informative Dirichlet-based classifier}
\label{fig:Risk_cond_ex_01_Dir_theta__subj}
\end{figure}





























\newpage

\chapter{Extention to Infinite-Dimensional Spaces - Countably Infinite}

PGR: MERGE with finite chapter???

\section{Intro}

This chapter extends previous results for applications where the space $\Ycal$ is countably infinite, that is $|\Ycal| = \aleph_0$. Specifically, the model prior distribution will be characterized by a discrete-domain Dirichlet process.




\section{Basic Model}


\subsection{Probability Distributions}

PGR: ???


\subsubsection{Model PDF, $\prm(\uptheta)$}

PGR: Valid model representation? Marginals instead?


\begin{IEEEeqnarray}{rCl}
\prm(\uptheta) & = & \beta(\alpha)^{-1} \prod_{y \in \Ycal} \uptheta(y)^{\alpha(y) - 1} \;,
\end{IEEEeqnarray}

\begin{equation}
\beta(\alpha) = \frac{\prod_{y \in \Ycal} \Gamma\big( \alpha(y) \big)}{\Gamma \left( \sum_{y \in \Ycal} \alpha(y) \right)} \;.
\end{equation}

The first and second joint moments of the model are 
\begin{equation}
\mu_{\uptheta}(y) = \Erm_{\uptheta}\big[ \uptheta(y) \big] = \frac{\alpha(y)}{\alpha_0}
\end{equation}
and
\begin{IEEEeqnarray}{rCl}
\Erm_{\uptheta}\big[ \uptheta(y) \uptheta(y') \big] & = & \frac{\alpha(y) \alpha(y') + \alpha(y) \delta[y,y']}{\alpha_0 (\alpha_0+1)} \;.
\end{IEEEeqnarray}




\subsubsection{Training Data PMF, $\Prm(\Drm)$}

\begin{equation}
\Prm(\Drm | \uptheta) = \prod_{y \in \Ycal} \uptheta(y)^{\bar{N}(y;\Drm)} \;.
\end{equation}

\begin{equation}
\Prm(\nbarrm | \uptheta) = \Mcal(\nbarrm) \prod_{y \in \Ycal} \uptheta(y)^{\bar{\nrm}(y)} \;,
\end{equation}

Thus,
\begin{IEEEeqnarray}{rCl}
\Prm(\nbarrm) & = & \Mcal(\nbarrm) \beta(\alpha)^{-1} \beta(\alpha + \nbarrm) \;.
\end{IEEEeqnarray}

The first and second joint moments of $\nbarrm$ are
\begin{equation}
\Erm_{\bar{\nrm}}\big[ \bar{\nrm}(y) \big] = N \frac{\alpha(y)}{\alpha_0}
\end{equation}
and
\begin{equation}
\Erm_{\bar{\nrm}}\big[ \bar{\nrm}(y) \bar{\nrm}(y') \big] 
= \frac{N}{\alpha_0 (\alpha_0+1)} \big( (\alpha_0 + N)\alpha(y) \delta[y,y'] + (N-1) \alpha(y) \alpha(y') \big) \;.
\end{equation}

Also,
\begin{equation}
\Prm(\Drm) = \beta(\alpha)^{-1} \beta \big( \alpha + \bar{N}(\Drm) \big) \;.
\end{equation}






\subsubsection{Output conditional PMF, $\Prm(\yrm | \Drm)$}

\begin{IEEEeqnarray}{rCL}
\prm(\uptheta | \Drm) & = & \frac{\Prm(\Drm | \uptheta) \prm(\uptheta)}{\Prm(\Drm)} \\
& = & \beta \left( \alpha + \bar{N}(\Drm) \right)^{-1} \prod_{y \in \Ycal} \uptheta(y)^{\alpha(y) + \bar{N}(y;\Drm) - 1} \nonumber 
\end{IEEEeqnarray}

\begin{IEEEeqnarray}{rCL}
\prm(\theta | \nbarrm) = \beta \left( \alpha + \nbarrm \right)^{-1} 
\prod_{y \in \Ycal} \uptheta(y)^{\alpha(y) + \bar{\nrm}(y) - 1} \;,
\end{IEEEeqnarray}

The PMF of interest is
\begin{IEEEeqnarray}{rCl}
\Prm(\yrm | \Drm) & = & \Erm_{\uptheta | \Drm}\big[ \theta(\yrm) \big] \\
& = & \frac{\alpha(\yrm) + \bar{N}(\yrm;\Drm)}{\alpha_0 + N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0+N}\right) \frac{\alpha(\yrm)}{\alpha_0} + \left(\frac{N}{\alpha_0+N}\right) \frac{\bar{N}(\yrm;\Drm)}{N} \nonumber
\end{IEEEeqnarray}

\begin{IEEEeqnarray}{rCl}
\Prm(\yrm | \nbarrm) & = & \Erm_{\theta | \nbarrm} \big[ \theta(\yrm) | \nbarrm \big] \\
& = & \frac{\alpha(\yrm) + \bar{\nrm}(\yrm)}{\alpha_0 + N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0+N}\right) \frac{\alpha(\yrm)}{\alpha_0} + \left(\frac{N}{\alpha_0+N}\right) \frac{\bar{\nrm}(\yrm)}{N} \nonumber
\end{IEEEeqnarray}



\section{Application to Common Loss Functions}

PGR: Definitely regression. Classification sensible for countably infinite?

PGR: Results identical to finite Dirichlet

\begin{IEEEeqnarray}{L}
\Erm_{\yrm | \Drm} \big[ \Lcal(h,\yrm) \big] = \sum_{y \in \Ycal} \Lcal(h,y) \Prm_{\yrm | \Drm}(y | \Drm) \\
= \frac{\sum_{y \in \Ycal} \alpha(y) \Lcal(h,y) + \sum_{y \in \Ycal} \bar{N}(y;\Drm) \Lcal(h,y)}{\alpha_0+N} \nonumber \\
= \frac{\sum_{y \in \Ycal} \alpha(y) \Lcal(h,y) + \sum_{n=1}^N \Lcal\big( h,\Drm_n \big)}{\alpha_0+N} \nonumber \\
= \left( \frac{\alpha_0}{\alpha_0+N} \right) \sum_{y \in \Ycal} \Lcal(h,y) \frac{\alpha(y)}{\alpha_0} +  \left( \frac{N}{\alpha_0+N} \right) N^{-1} \sum_{n=1}^N \Lcal\big( h,\Drm_n \big) \nonumber \;.
\end{IEEEeqnarray}


\section{General Model}

Extension to output and input spaces $\Ycal$ and $\Xcal$ can have an infinite number of elements. 


\section{Applications: General Model}

PGR: Definitely regression. Classification sensible for countably infinite?

PGR: Results identical to finite Dirichlet















\chapter{Extention to Infinite-Dimensional Spaces - Uncountably Infinite}

PGR: SPECIFY EUCLIDEAN/HILBERT??

PGR: account for impulsive alpha?


\section{Intro}

This chapter extends further to the case where $\Ycal$ is a Euclidean space and the model $\uptheta$ is a continuous-domain Dirichlet process. 


\section{Basic Model}



\subsection{Probability Distributions}

PGR: ???


\subsubsection{Model $\uptheta$ Characterization}

The model is now a continuous-domain Dirichlet process $\uptheta \sim \DP(\alpha)$. The concentration parameter is $\alpha_0 \equiv \int_{y \in \Ycal} \alpha(y) \mathrm{d} y$. By definition, for any partition of the set $\Ycal$, $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$, we can generate a Dirichlet random process $\upphi(z) \equiv \int_{\Scal(z)} \uptheta(y) \mathrm{d} y$ with parameterizing function $\lambda(z) \equiv \int_{\Scal(z)} \alpha(y) \mathrm{d} y$. This is commonly referred to as the aggregation property. The PDF for the aggregation is thus
\begin{IEEEeqnarray}{rCl}
\Prm_{\upphi}(\phi) & = & \beta(\lambda)^{-1} \prod_{z \in \Zcal} \phi(z)^{\lambda(z) - 1} \;.
\end{IEEEeqnarray}

As detailed in Appendix \ref{app:E_DP}, the first and second moments of the Dirichlet process $\uptheta$ are
\begin{equation}
\mu_{\uptheta} = \frac{\alpha}{\alpha_0}
\end{equation}
and
\begin{IEEEeqnarray}{rCl}
\Erm_{\uptheta}\big[ \uptheta(y) \uptheta(y') \big] & = & \frac{\alpha(y) \alpha(y') + \alpha(y) \delta(y-y')}{\alpha_0 (\alpha_0+1)} \;.
\end{IEEEeqnarray}
Again, $\prm_{\yrm} = \prm_{\Drm_n} = \mu_{\uptheta}$ for all $n$.






\subsubsection{Output conditional PDF, $\prm_{\yrm | \Drm}$}

In Appendix \ref{app:DP_post}, it was shown that if the model $\uptheta \sim \DP(\alpha)$ is a Dirichlet process, then the model conditioned on the training data $\Drm$ is also a Dirichlet process with parameterizing function $\alpha + \bar{N}(\Drm)$, where $\bar{N}(y;\Drm) = \sum_{n=1}^N \delta\left( y - \Drm_n \right)$ generalizes for a continuous domain. Thus, the conditional PDF of interest can be formulated as
\begin{IEEEeqnarray}{rCl}
\prm_{\yrm | \Drm} & = & \mu_{\uptheta | \Drm} \\
& = & \frac{\alpha + \bar{N}(\Drm)}{\alpha_0 + N} = \frac{\alpha + \sum_{n=1}^N \delta\big( \cdot - \Drm_n \big)}{\alpha_0 + N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0+N}\right) \frac{\alpha}{\alpha_0} + \left(\frac{N}{\alpha_0+N}\right) \frac{\sum_{n=1}^N \delta\big( \cdot - \Drm_n \big)}{N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0+N}\right) \prm_{\yrm} + \left(\frac{N}{\alpha_0+N}\right) \frac{\sum_{n=1}^N \delta\big( \cdot - \Drm_n \big)}{N} \nonumber \;.
\end{IEEEeqnarray}
With the generalization to a Euclidean space $\Ycal$, the training data dependent component of the PDF is formulated with Dirac delta functions. 



\subsubsection{Training Data PDF, $\prm(\Drm)$}

To represent the training data distribution, note that the Dirichlet process conditional model also provides
\begin{IEEEeqnarray}{rCl}
\prm_{\Drm_{n+1} | \Drm_n,\ldots,\Drm_1} & = & \frac{\alpha + \sum_{i=1}^n \delta\big( \cdot - \Drm_i \big)}{\alpha_0 + N} \;
\end{IEEEeqnarray}
and thus the complete PDF is
\begin{IEEEeqnarray}{rCl}
\prm_{\Drm}(D) & = & \prm_{\Drm_1}\big( D_1 \big) \prod_{n=2}^N \prm_{\Drm_n | \Drm_{n-1},\ldots,\Drm_1}\big( \Drm_n | \Drm_{n-1},\ldots,\Drm_1 \big) \\
& = & \frac{\alpha\big( D_1 \big)}{\alpha_0} \prod_{n=2}^N \frac{\alpha\big( D_n \big) + \sum_{i=1}^{n-1} \delta\big( D_n - D_i \big)}{\alpha_0+n-1} \nonumber \;.
\end{IEEEeqnarray}

Additionally, note that since $\prm_{\Drm_n | \uptheta} = \uptheta$ is independent of sample index $n$, the PDF does not vary when the input arguments are permuted. Furthermore, all marginal distributions of $\Drm$ will have the same form, regardless of which training samples $\Drm_n$ are used.

Using these properties, the first and second joint moments of $\Drm$ are found to be
\begin{IEEEeqnarray}{rCl}
\mu_{\Drm_n} & = & \int_{\Ycal} y \prm_{\Drm_n}(y) \mathrm{d} y = \int_{\Ycal} y \Erm_{\uptheta}\big[ \Prm_{\Drm_n | \uptheta}(y) \big] \mathrm{d} y \\
& = & \int_{\Ycal} y \mu_{\uptheta}(y) \mathrm{d}y \nonumber \\
& = & \int_{\Ycal} y \frac{\alpha(y)}{\alpha_0} \mathrm{d} y \equiv \mu_{\yrm} \nonumber
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{rCl}
\Erm_{\Drm}\big[ \Drm_n^2 \big] & = & \int_{\Ycal} y^2 \prm_{\Drm_n}(y) \mathrm{d} y = \int_{\Ycal} y^2 \Erm_{\uptheta}\big[ \Prm_{\Drm_n | \uptheta}(y) \big] \mathrm{d}y \\
& = & \int_{\Ycal} y^2 \mu_{\uptheta}(y) \mathrm{d}y \nonumber \\
& = & \int_{\Ycal} y^2 \frac{\alpha(y)}{\alpha_0} \mathrm{d}y = \Erm_{\yrm}[\yrm^2] \nonumber \;,
\end{IEEEeqnarray}

\begin{IEEEeqnarray}{rCl}
\Erm_{\Drm}\big[ \Drm_n\Drm_{n'} \big] & = & \int_{\Ycal} \int_{\Ycal} y y' \prm_{\Drm_n,\Drm_{n'}}(y,y') \mathrm{d} y \mathrm{d}y' \\
& = & \int_{\Ycal} \int_{\Ycal} y y' \Erm_{\uptheta} \big[ \prm_{\Drm_n | \uptheta}(y) \prm_{\Drm_{n'} | \theta}(y') \big] \mathrm{d}y \mathrm{d}y' \nonumber \\
& = & \int_{\Ycal} \int_{\Ycal} y y' \Erm_{\uptheta}\big[ \uptheta(y) \uptheta(y') \big] \mathrm{d}y \mathrm{d}y' \nonumber \\
& = & \int_{\Ycal} \int_{\Ycal} y y' \frac{\alpha(y) \alpha(y') + \alpha(y) \delta(y-y')}{\alpha_0 (\alpha_0+1)} \mathrm{d}y \mathrm{d}y' \nonumber \\
& = & \frac{\alpha_0 \mu_{\yrm}^2 + \Erm_{\yrm}[\yrm^2]}{\alpha_0 + 1} \nonumber \;.
\end{IEEEeqnarray}

Combining,
\begin{equation}
\Erm_{\Drm}\big[ \Drm_n\Drm_{n'} \big] = \Erm_{\yrm}[\yrm^2] - \big(1 - \delta[n,n']\big) \frac{\alpha_0}{\alpha_0+1} \Sigma_{\yrm} \;.
\end{equation}

PGR: move proofs to appendix???


PGR PGR: Dirichlet-Multinomial Process perspective???

Define the Dirichlet-Multinomial process $\bar{\nrm} \equiv \bar{N}(\Drm)$. The mean and correlation functions below are found in Appendix \ref{app:DMP}. The mean function is
\begin{IEEEeqnarray}{rCl}
\mu_{\nbarrm} & = & N \frac{\alpha}{\alpha_0} 
\end{IEEEeqnarray}
and the correlation function is
\begin{IEEEeqnarray}{rCl}
\Erm_{\nbarrm}\big[ \bar{\nrm}(y) \bar{\nrm}(y') \big] & = & \frac{N}{\alpha_0 (\alpha_0+1)} \big[ (N-1)\alpha(y) \alpha(y') + (\alpha_0+N) \alpha(y) \delta(y-y') \big] \;.
\end{IEEEeqnarray}




\section{Application to Common Loss Functions}

PGR: Discuss continuous notation

\begin{IEEEeqnarray}{L}
\Erm_{\yrm | \Drm} \big[ \Lcal(h,\yrm) \big] = \int_{\Ycal} \Lcal(h,y) \prm_{\yrm | \Drm}(y | \Drm) \mathrm{d}y \\
= \frac{\int_{\Ycal} \alpha(y) \Lcal(h,y) \mathrm{d} y + \int_{\Ycal} \sum_{n=1}^N \delta\big( y-\Drm_n \big) \Lcal(h,y) \mathrm{d} y}{\alpha_0+N} \nonumber \\
= \frac{\int_{\Ycal} \alpha(y) \Lcal(h,y) \mathrm{d} y + \sum_{n=1}^N \Lcal\big( h,\Drm_n \big)}{\alpha_0+N} \nonumber \\
= \left( \frac{\alpha_0}{\alpha_0+N} \right) \int_{\Ycal} \frac{\alpha(y)}{\alpha_0} \Lcal(h,y) \mathrm{d}y +  \left( \frac{N}{\alpha_0+N} \right) N^{-1} \sum_{n=1}^N \Lcal\big( h,\Drm_n \big) \nonumber \\
= \left( \frac{\alpha_0}{\alpha_0+N} \right) \Erm_{\yrm}\big[ \Lcal(h,\yrm) \big] +  \left( \frac{N}{\alpha_0+N} \right) N^{-1} \sum_{n=1}^N \Lcal\big( h,\Drm_n \big) \nonumber 
\end{IEEEeqnarray}


\subsection{Regression: the Squared-Error Loss}

\begin{equation}
\Lcal(h,y) = (h-y)^2 \;.
\end{equation}

Now we choose for the regression function to map to $\Hcal = \Ycal = \Rbb$.

\begin{IEEEeqnarray}{rCl}
\Rcal(f) & = & \Erm_{\uptheta} \Bigg[ \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm | \uptheta} \Big[ \big( f(\Drm)-\yrm \big)^2 \Big] \bigg] \Bigg] \\
& = & \Erm_{\uptheta} \Big[ \Erm_{\yrm | \uptheta} \big[ (\yrm - \mu_{\yrm | \uptheta})^2 \big] \Big] + \Erm_{\uptheta} \bigg[ \Erm_{\Drm | \uptheta} \Big[ \big( f(\Drm) - \mu_{\yrm | \uptheta} \big)^2 \Big] \bigg] \nonumber \\
& = & \Erm_{\uptheta} \left[ \Sigma_{\yrm | \uptheta} \right] + \Erm_{\uptheta} \bigg[ \Erm_{\Drm | \uptheta} \Big[ \big( f(\Drm) - \mu_{\yrm | \uptheta} \big)^2 \Big] \bigg] \nonumber
\end{IEEEeqnarray}


\subsubsection{Optimal Learner}

The optimal function is again the expected value of the output conditional PMF,
\begin{IEEEeqnarray}{rCl}
f^*(\Drm) & = & \argmin_{h \in \Rbb} \Erm_{\yrm | \Drm} \big[ (h-\yrm)^2 \big]  \\
& = & \mu_{\yrm | \Drm} = \Erm_{\uptheta | \Drm} [ \mu_{\yrm | \uptheta} ] \nonumber \\
& = & \left( \frac{\alpha_0}{\alpha_0+N} \right) \int_{\Ycal} \frac{\alpha(y)}{\alpha_0} y \mathrm{d}y +  \left( \frac{N}{\alpha_0+N} \right) \frac{1}{N} \sum_{n=1}^N \Drm_n \nonumber \\
& = & \left( \frac{\alpha_0}{\alpha_0+N} \right) \mu_{\yrm} +  \left( \frac{N}{\alpha_0+N} \right) \frac{1}{N} \sum_{n=1}^N \Drm_n \nonumber \\
& = & \left( \frac{\alpha_0}{\alpha_0+N} \right) \mu_{\yrm} +  \left( \frac{N}{\alpha_0+N} \right) \int_{\Ycal} y \frac{\bar{N}(y;\Drm)}{N} \mathrm{d}y \nonumber \;.
\end{IEEEeqnarray}




\subsubsection{Minimum Risk}

\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & \Erm_{\Drm} \left[ \Sigma_{\yrm | \Drm} \right] \\
& = & \Erm_{\uptheta} \left[ \Sigma_{\yrm | \uptheta} \right] + \Erm_{\Drm} \bigg[ \Erm_{\uptheta | \Drm} \Big[ \big( \mu_{\yrm | \uptheta} - \Erm_{\uptheta | \Drm}[\mu_{\yrm | \uptheta}] \big)^2 \Big] \bigg] \nonumber \\
& = & \Erm_{\uptheta} \left[ \Sigma_{\yrm | \uptheta} \right] + \Erm_{\Drm} \big[ \Crm_{\uptheta | \Drm} [ \mu_{\yrm | \uptheta} ] \big] \nonumber \\
& = & \Erm_{\nbarrm} \left[ \Sigma_{\yrm | \nbarrm} \right] \nonumber \;.
\end{IEEEeqnarray}


The conditional variance is expanded as
\begin{IEEEeqnarray}{rCl}
\Sigma_{\yrm | \Drm} & = & \Erm_{\yrm | \Drm}[\yrm^2] - \mu_{\yrm | \Drm}^2 
\end{IEEEeqnarray}
and the expectations of the two terms are evaluated separately.


\begin{IEEEeqnarray}{rCl}
\Erm_{\Drm}\big[\Erm_{\yrm | \Drm}[\yrm^2]\big] & = & \Erm_{\yrm}[\yrm^2]
\end{IEEEeqnarray}


\begin{IEEEeqnarray}{L}
\Erm_{\Drm}\big[ \mu_{\yrm | \Drm}^2 \big] \\
\quad = \frac{\alpha_0^2 \mu_{\yrm}^2 + 2\alpha_0 \mu_{\yrm} \sum_{n=1}^N \mu_{\Drm_n} + \sum_{n=1}^N \sum_{n'=1}^N \Erm_{\Drm}\big[ \Drm_n\Drm_{n'} \big]}{(\alpha_0+N)^2} \nonumber \\
\quad = \frac{\alpha_0^2 \mu_{\yrm}^2 + 2\alpha_0 N \mu_{\yrm}^2 + N^2\Erm_{\yrm}[\yrm^2] - N(N-1)\alpha_0(\alpha_0+1)^{-1} \Sigma_{\yrm}}{(\alpha_0+N)^2} \nonumber \\
\quad = \mu_{\yrm}^2 + \frac{N}{(\alpha_0+1) (\alpha_0+N)} \Sigma_{\yrm} \nonumber
\end{IEEEeqnarray}

PGR: DMP PERSPECTIVE???

\begin{IEEEeqnarray}{L}
\Erm_{\Drm}\big[ \mu_{\yrm | \Drm}^2 \big] = \Erm_{\bar{\nrm}}\big[ \mu_{\yrm | \bar{\nrm}}^2 \big] \\
\quad = \frac{\alpha_0^2 \mu_{\yrm}^2 + 2\alpha_0 \mu_{\yrm} \int_{\Ycal} y \mu_{\bar{\nrm}}(y) \big] \mathrm{d}y + \int_{\Ycal} \int_{\Ycal} y y'\Erm_{\nbarrm}\big[ \bar{\nrm}(y)\bar{\nrm}(y') \big] \mathrm{d}y \mathrm{d}y'}{(\alpha_0+N)^2} \nonumber \\
\quad = \frac{\alpha_0^2 \mu_{\yrm}^2 + 2\alpha_0 N \mu_{\yrm}^2 + N(N-1)\alpha_0(\alpha_0+1)^{-1} \mu_{\yrm}^2 + N(\alpha_0+N)(\alpha_0+1)^{-1}\Erm_{\yrm}[\yrm^2]}{(\alpha_0+N)^2} \nonumber \\
\quad = \frac{\alpha_0(\alpha_0+N+1)\mu_{\yrm}^2 + N\Erm_{\yrm}[\yrm^2]}{(\alpha_0+1)(\alpha_0+N)} \nonumber
\end{IEEEeqnarray}

PGR: nicer algebra with DMP!

PGR: DMP


The minimal risk is again

\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & \left( 1 - \frac{N}{(\alpha_0+1) (\alpha_0+N)} \right) \Sigma_{\yrm} \\
& = & \frac{\alpha_0 (\alpha_0+N+1)}{(\alpha_0+1)(\alpha_0+N)} \Sigma_{\yrm} \nonumber \\
& = & \frac{1+(\alpha_0+N)^{-1}}{1+\alpha_0^{-1}} \Sigma_{\yrm} \nonumber \;.
\end{IEEEeqnarray}

As for Dirichlet distributions for functions with countable domains, the minimal risk is dependent on the model only through the concentration parameter $\alpha_0$ and the variance of the expected distribution $\Prm_{\yrm} = \mu_{\uptheta}$.




\section{General Model}

PGR: change dirac deltas to kronecker in fractions, no divide by zero???

\subsection{Model Extension}

This section adds the input space $\Xcal$, now considered to be uncountably infinite; that is $|\Xcal| \geq \aleph_1$. The model distribution is a Dirichlet process over the space $\Ycal \times \Xcal$.


\subsection{General Probability Distributions}

PGR


\subsubsection{Model $\uptheta$ Characterization}

The model is characterized by a Dirichlet process $\uptheta \sim \DP(\alpha)$ with parameter function $\alpha : \Ycal \times \Xcal \mapsto \Rbb^+$. The concentration parameter generalizes to $\alpha_0 = \int_{\Ycal} \int_{\Xcal} \alpha(y,x) \mathrm{d} x \mathrm{d} y$. Using the aggregation property, any partition of the set $\Ycal \times \Xcal$, $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$ is a Dirichlet random process $\upphi(z) = \iint_{\Scal(z)} \uptheta(y,x) \mathrm{d} x \mathrm{d} y$ with parameterizing function $\lambda(z) = \iint_{\Scal(z)} \alpha(y,x) \mathrm{d} x \mathrm{d} y$. The PDF for the aggregation is
\begin{IEEEeqnarray}{rCl}
\prm_{\upphi}(\phi) & = & \beta(\lambda)^{-1} \prod_{z \in \Zcal} \phi(z)^{\lambda(z) - 1} \;.
\end{IEEEeqnarray}

The expected value of a Dirichlet process generalizes to $\DP(\alpha)$
\begin{equation}
\mu_{\uptheta} = \frac{\alpha}{\alpha_0}
\end{equation}
and the correlation function is
\begin{IEEEeqnarray}{rCl}
\Erm_{\uptheta}\big[ \uptheta(y,x) \uptheta(y',x') \big] & = & \frac{\alpha(y,x) \alpha(y',x') + \alpha(y,x) \delta(y-y')\delta(x-x')}{\alpha_0 (\alpha_0+1)} \;.
\end{IEEEeqnarray}






\subsubsection{Output conditional PDF, $\prm_{\yrm | \xrm,\Drm}$}

The properties of the Dirichlet distribution proven in Appendix \ref{app:DP_post} generalize, such that the model conditioned on the training data $\Drm$ is Dirichlet with parameterizing function $\alpha + \bar{N}(\Drm)$, where $\bar{N}(y,x;\Drm) = \sum_{n=1}^N \delta\left( y - \Yrm_n \right)\left( x - \Xrm_n \right)$. Recall that $\Drm_n = \big( \Yrm_n,\Xrm_n \big)$ with $Y \in \Ycal^N$ and $X \in \Xcal^N$.

The PDF $\prm_{\yrm,\xrm | \Drm}$ is thus
\begin{IEEEeqnarray}{rCl}
\prm_{\yrm,\xrm | \Drm} & = & \mu_{\uptheta | \Drm} \\
& = & \frac{\alpha + \bar{N}(\Drm)}{\alpha_0 + N} \nonumber \\
& = & \frac{\alpha + \sum_{n=1}^N \delta\big( \cdot - \Yrm_n \big)\delta\big( \cdot - \Xrm_n \big)}{\alpha_0 + N} \nonumber \\
& = & \left(\frac{\alpha_0}{\alpha_0+N}\right) \frac{\alpha}{\alpha_0} + \left(\frac{N}{\alpha_0+N}\right) \frac{\sum_{n=1}^N \delta\big( \cdot - \Yrm_n \big) \delta\big( \cdot - \Xrm_n \big)}{N} \nonumber
\end{IEEEeqnarray}
and the conditional PDF of interest is
\begin{IEEEeqnarray}{rCl}
\prm_{\yrm | \xrm,\Drm} & = & \frac{\alpha(\cdot,\xrm) + \bar{N}(\cdot,\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \\
& = & \frac{\alpha(\cdot,\xrm) + \sum_{n=1}^N \delta\big( \cdot - \Yrm_n \big) \delta\big( \xrm - \Xrm_n \big)}{\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big)} \nonumber \\
& = & \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm)+N'(\xrm;\Drm)}\right) \frac{\alpha(\cdot,\xrm)}{\alpha'(\xrm)} + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm)+N'(\xrm;\Drm)}\right) \frac{\bar{N}(\cdot,\xrm;\Drm)}{N'(\xrm;\Drm)} \nonumber \;,
\end{IEEEeqnarray}
where $\alpha'(x) = \int_{\Ycal} \alpha(y,x) \mathrm{d} y$ and $N'(x;\Drm) = \sum_{n=1}^N \delta\big( x - \Xrm_n \big)$.

The conditional distribution when $\Xcal$ is a Euclidean space has notable differences from its form for a countable set $\Xcal$. Specifically, as $N'(x;\Drm) \in [0,\infty)$, and in fact will either be zero or tend towards infinity, the coefficients dictating the convex combination of distributions will be zero or one (assuming a non-impulsive model parameter $\alpha$). Thus, the distribution for a given observation $\xrm$ will be either strictly dependent on either the training data or the prior knowledge regarding $\uptheta$.



\subsubsection{Training Data PDF, $\prm_{\Drm}$}

By the Dirichlet process properties,
\begin{IEEEeqnarray}{L}
\prm_{\Drm_{n+1} | \Drm_n,\ldots,\Drm_1} = \\
\quad \frac{\alpha(\cdot,\cdot) + \sum_{n'=1}^n \delta\big( \cdot - \Yrm_i \big) \delta\big( \cdot - \Xrm_i \big)}{\alpha_0 + N} \nonumber
\end{IEEEeqnarray}
and thus
\begin{IEEEeqnarray}{rCl}
\prm_{\Drm}(D) & = & \prm\big (D_1 \big) \prod_{n=2}^N \prm\big( D_n | D_{n-1},\ldots,D_1 \big) \\
& = & \frac{\alpha\big( Y_1,X_1 \big)}{\alpha_0} \prod_{n=2}^N \frac{\alpha\big( Y_n,X_n \big) + \sum_{i=1}^{n-1} \delta\big( Y_n -  Y_i \big) \delta\big( X_n - X_i \big)}{\alpha_0+n-1} \nonumber
\end{IEEEeqnarray}

It is informative to find the PDF's for the training output values $\Yrm$ given the input values $\Xrm$, as well as the marginal PDF for the input values alone. Observe that the PDF for $\Xrm$ can be represented as
\begin{IEEEeqnarray}{rCl}
\prm_{\Xrm}(X) & = & \Erm_{\uptheta}\big[ \prm_{\Xrm | \uptheta}(X | \uptheta) \big] = \Erm_{\uptheta}\left[ \prod_{n=1}^N \uptheta'\big( X_n \big) \right] \\
& = & \frac{\alpha'\big( X_1 \big)}{\alpha_0} \prod_{n=2}^N \frac{\alpha'\big( X_n \big) + \sum_{i=1}^{n-1} \delta\big( X_n-X_i \big)}{\alpha_0+n-1} \nonumber
\end{IEEEeqnarray}
since, by the aggregation principle, $\uptheta' = \int_{\Ycal} \uptheta(y,\cdot) \mathrm{d} y$ is a Dirichlet process with parameter function $\alpha': \Xcal \mapsto \Rbb^+$.


%The PDF for $\Xrm$ is
%\begin{IEEEeqnarray}{rCl}
%\prm(X) & = & \int_{Y(1)} \mathrm{d}Y(1) \ldots \int_{Y(N)} \mathrm{d}Y(N) \frac{\alpha(Y(1),X(1))}{\alpha_0} \\
%&& \quad \prod_{n=2}^N \frac{\alpha(Y_n,X_n) + \sum_{i=1}^{n-1} \delta(Y_n-Y(i)) \delta(X_n-X(i))}{\alpha_0+n-1} \\
%& = & \int_{Y(1)} \mathrm{d}Y(1) \ldots \int_{Y(N-1)} \mathrm{d}Y(N-1) \frac{\alpha(Y(1),X(1))}{\alpha_0} \\
%&& \quad \prod_{n=2}^{N-1} \frac{\alpha(Y_n,X_n) + \sum_{i=1}^{n-1} \delta(Y_n-Y(i)) \delta(X_n-X(i))}{\alpha_0+n-1} \\
%&& \qquad \frac{\alpha'(X(N)) + \sum_{i=1}^{N-1} \delta(X(N)-X(i))}{\alpha_0+N-1} \\
%& = & \ldots \\
%& = & \frac{\alpha'(X(1))}{\alpha_0} \prod_{n=2}^N \frac{\alpha'(X_n) + \sum_{i=1}^{n-1} \delta(X_n-X(i))}{\alpha_0+n-1}
%\end{IEEEeqnarray}

Additionally, by the invariance principle, the PDF's for the first-degree marginals are
\begin{IEEEeqnarray}{rCl}
\prm_{\Xrm_n} & = & \frac{\alpha'}{\alpha_0} \;.
\end{IEEEeqnarray}
which equivalent to $\prm_{\xrm}$.

PGR: express conditional below using Dir aggregation conditional independence properties?

The conditional distribution of intererest is
\begin{IEEEeqnarray}{rCl}
\prm_{\Yrm | \Xrm}(Y | X) & = & \frac{\alpha\big( Y_1,X_1 \big)}{\alpha'\big( X_1 \big)} \prod_{n=2}^N \frac{\alpha\big( Y_n,X_n \big) + \sum_{i=1}^{n-1} \delta\big( Y_n-Y_i \big) \delta\big( X_n-X_i \big)}{\alpha'\big( X_n \big) + \sum_{i=1}^{n-1} \delta\big( X_n-X_i \big)} 
\end{IEEEeqnarray}

Marginalized conditional PDF's for the first and second samples are found. Observe that the marginal distribution for the first $N-1$ values of $\Yrm$ is
\begin{IEEEeqnarray}{L}
\prm_{\Yrm_1,\ldots,\Yrm_{N-1} | \Xrm}\big( Y_1,\ldots,Y_{N-1} | X \big) \\
= \int_{\Ycal} \frac{\alpha\big( Y_1,X_1 \big)}{\alpha'\big( X_1 \big)} \prod_{n=2}^N \frac{\alpha \big( Y_n,X_n \big) + \sum_{i=1}^{n-1} \delta\big( Y_n-Y_i \big) \delta\big( X_n-X_i \big)}{\alpha'\big( X_n \big) + \sum_{i=1}^{n-1} \delta\big( X_n-X_i \big)} \mathrm{d}Y_N \nonumber \\
= \frac{\alpha\big( Y_1,X_1 \big)}{\alpha'\big( X_1 \big)} \prod_{n=2}^{N-1} \frac{\alpha\big( Y_n,X_n \big) + \sum_{i=1}^{n-1} \delta\big( Y_n-Y_i \big) \delta\big( X_n-X_i \big)}{\alpha'\big( X_n \big) + \sum_{i=1}^{n-1} \delta\big( X_n-X_i \big)} \nonumber
\end{IEEEeqnarray}
which is independent of $\Xrm_N$. Repeated integrations and an application of the permutation invariance principle can show that when conditioned on $\Xrm$ any subset of training data values $\Yrm_1,\ldots,\Yrm_N$ will only be dependent on the corresponding values $\Xrm_n$. The first and second order conditional distributions are
\begin{IEEEeqnarray}{rCl}
\Prm_{\Yrm_n | \Xrm_n} (y | x) & = & \frac{\alpha(y,x)}{\alpha'(x)} = \Prm_{\yrm | \xrm} (y | x)
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{L}
\Prm_{\Yrm_n,\Yrm_{n'} | \Xrm_n,\Xrm_{n'}} (y,y' | x,x') \\
\quad = \frac{\alpha(y,x) \alpha(y',x') + \alpha(y,x) \delta(y-y') \delta(x-x')}{\alpha'(x) \alpha'(x') + \alpha'(x') \delta(x-x')} \nonumber
\end{IEEEeqnarray}
and the first and second order moments of interest are
\begin{equation}
\mu_{\Yrm_n | \Xrm} = \mu_{\yrm|\xrm}\big( \Xrm_n \big) \;,
\end{equation}
\begin{equation}
\Erm_{\Yrm_n | \Xrm}\big[ \Yrm_n^2 \big] = \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \big( \Xrm_n \big) \;,
\end{equation}
and 
\begin{IEEEeqnarray}{L}
\Erm_{\Yrm_n | \Xrm}\big[ \Yrm_n \Yrm_{n'} \big] \\
\quad = \frac{\alpha'\big( \Xrm_n \big) \mu_{\yrm|\xrm}\big( \Xrm_n ) \mu_{\yrm|\xrm}\big (\Xrm_{n'} \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \big( \Xrm_n \big) \delta\big( \Xrm_n - \Xrm_{n'} \big)}{\alpha'\big( \Xrm_n \big) + \delta\big( \Xrm_n - \Xrm_{n'} \big)} \nonumber \;.
\end{IEEEeqnarray}


PGR: formalize permutation invariance principle???

PGR: Add Y given X equations (with Betas) for discrete case in previous chapters?



PGR: Dirichlet-Multinomial Process perspective

We have the DMP $\bar{\nrm} \equiv \bar{N}(\Drm)$ with mean and correlation functions
\begin{IEEEeqnarray}{rCl}
\mu_{\bar{\nrm}} & = & N \frac{\alpha}{\alpha_0} 
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{L}
\Erm_{\nbarrm}\big[ \bar{\nrm}(y,x) \bar{\nrm}(y',x') \big] \\
\quad = \frac{N}{\alpha_0 (\alpha_0+1)} \big[ (N-1)\alpha(y,x) \alpha(y',x') + (\alpha_0+N) \alpha(y,x) \delta(y-y') \delta(x-x') \big] \nonumber \;.
\end{IEEEeqnarray}

Observe that by the aggregation principle, $\nrm' = \int_{\Ycal} \bar{\nrm}(y,\cdot) \mathrm{d}y \equiv \sum_{n=1}^N \delta\big( \cdot - \Xrm_n \big)$ is a DMP over the set $\Xcal$ with parametrizing function $\alpha' : \Xcal \mapsto \Rbb^+$.

Additionally, the 1-dimensional subsets conditioned on the marginalized DMP are characterized as

\begin{equation}
\frac{\bar{\nrm}(\cdot,x)}{\delta(0)} \Big| \nrm'(x) \sim \DMP\left( \frac{\nrm'(x)}{\delta(0)},\frac{\alpha(\cdot,x)}{\delta(0)} \right)
\end{equation}

PGR: add proof???








\section{Applications: General Model}

PGR: COPIED, incomplete

\begin{IEEEeqnarray}{L}
\Erm_{\yrm | \xrm,\Drm} \big[ \Lcal(h,\yrm) \big] = \int_{\Ycal} \Lcal(h,y) \prm_{\yrm | \xrm,\Drm}(y | \xrm,\Drm) \mathrm{d}y \\
= \frac{\int_{\Ycal} \alpha(y,\xrm) \Lcal(h,y) \mathrm{d}y + \int_{\Ycal} \bar{N}(y,\xrm;\Drm) \Lcal(h,y) \mathrm{d}y}{\alpha'(\xrm)+N'(\xrm;\Drm)} \nonumber \\
= \frac{\int_{\Ycal} \alpha(y,\xrm) \Lcal(h,y) \mathrm{d}y + \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \Lcal\big( h,\Drm_n \big)}{\alpha'(\xrm)+N'(\xrm;\Drm)} \nonumber \\
= \left(\frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \Erm_{\yrm | \xrm}\big[ \Lcal(h,\yrm) \big] + \left(\frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)}\right) \frac{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \Lcal\big( h,\Yrm_n \big)}{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big)} \nonumber \;.
\end{IEEEeqnarray}



\subsection{Regression: the Squared-Error Loss}

\begin{equation}
\Lcal(h,y) = (h-y)^2 \;.
\end{equation}

Now we choose for the regression function to map to $\Hcal = \Ycal = \Rbb$.

\begin{IEEEeqnarray}{rCl}
\Rcal(f) & = & \Erm_{\uptheta} \Bigg[ \Erm_{\Drm | \uptheta} \bigg[ \Erm_{\yrm,\xrm | \uptheta} \Big[ \big( f( \xrm;\Drm)-\yrm \big)^2 \Big] \bigg] \Bigg] \\
& = & \Erm_{\xrm,\uptheta} \Big[ \Erm_{\yrm | \xrm,\uptheta} \big[ (\yrm - \mu_{\yrm | \xrm,\uptheta})^2 \big] \Big] + \Erm_{\uptheta} \bigg[ \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \bigg] \nonumber \\
& = & \Erm_{\xrm,\uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] + \Erm_{\uptheta} \bigg[ \Erm_{\xrm,\Drm | \uptheta} \Big[ \big( f(\xrm;\Drm) - \mu_{\yrm | \xrm,\uptheta} \big)^2 \Big] \bigg] \nonumber
\end{IEEEeqnarray}



\subsubsection{Optimal Learner}

The optimal function is the expected value of the output conditional PDF,
\begin{IEEEeqnarray}{rCl}
f^*(\xrm;\Drm) & = & \mu_{\yrm | \xrm,\Drm}  = \Erm_{\uptheta | \xrm,\Drm} \left[ \mu_{\yrm | \xrm,\uptheta} \right] \\
& = & \frac{\int_{\Ycal} y \big( \alpha(y,\xrm) + \bar{N}(y,\xrm;\Drm) \big) \mathrm{d}y}{\alpha'(\xrm) + N'(\xrm;\Drm)} \nonumber \\
& = & \left( \frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \int_{\Ycal} y \frac{\alpha(y,\xrm)}{\alpha'(\xrm)} \mathrm{d}y \nonumber \\
&& \quad + \left( \frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \frac{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \Yrm_n}{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big)} \nonumber \\
& = & \left( \frac{\alpha'(\xrm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \mu_{\yrm | \xrm} \nonumber \\
&& \quad + \left( \frac{N'(\xrm;\Drm)}{\alpha'(\xrm) + N'(\xrm;\Drm)} \right) \frac{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \Yrm_n}{\sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big)} \nonumber \;.
\end{IEEEeqnarray}



\subsubsection{Minimum Risk}

Generalizing from the basic model discussion, we again have
\begin{IEEEeqnarray}{rCl}
\Rcal^* & = & \Erm_{\xrm,\Drm} \left[ \Sigma_{\yrm | \xrm,\Drm} \right]
= \Erm_{\xrm,\bar{\nrm}} \left[ \Sigma_{\yrm | \xrm,\bar{\nrm}} \right] \\
& = & \Erm_{\xrm,\uptheta} \left[ \Sigma_{\yrm | \xrm,\uptheta} \right] + \Erm_{\xrm,\Drm} \Big[ \Crm_{\uptheta | \xrm,\Drm} \big[ \mu_{\yrm | \xrm,\uptheta} \big] \Big] \nonumber \;,
\end{IEEEeqnarray}
where we choose to perform the expectation over $\bar{\nrm}$. 

The conditional varance is now
\begin{IEEEeqnarray}{rCl}
\Sigma_{\yrm | \xrm,\bar{\nrm}} & = & \Erm_{\yrm | \xrm,\bar{\nrm}}\big[ \yrm^2 \big]
- \mu_{\yrm | \xrm,\bar{\nrm}}^2 \;.
\end{IEEEeqnarray}
and the two terms are independently evaluated.


\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\Drm}\big[ \Erm_{\yrm | \xrm,\Drm}\big[ \yrm^2 \big] \big] = \Erm_{\xrm,\bar{\nrm}}\Big[ \Erm_{\yrm | \xrm,\bar{\nrm}}\big[ \yrm^2 \big] \Big] \\
\quad = \Erm_{\yrm}[\yrm^2] = \int_{\Ycal} y^2 \int_{\Xcal} \frac{\alpha(y,x)}{\alpha_0} \mathrm{d} x \mathrm{d}y \nonumber \\
\quad = \Erm_{\xrm}\big[ \Erm_{\yrm | \xrm}[\yrm^2] \big] = \int_{\Xcal} \frac{\alpha'(x)}{\alpha_0} \int_{\Ycal} y^2 \frac{\alpha(y,x)}{\alpha'(x)} \mathrm{d} y \mathrm{d}x \nonumber \;.
\end{IEEEeqnarray}



PGR: D PERSPECTIVE

\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\Drm} \left[ \mu_{\yrm | \xrm,\Drm}^2 \right] = \Erm_{\xrm} \Erm_{\Drm | \xrm} \left[ \mu_{\yrm | \xrm,\Drm}^2 \right] \\
\quad = \Erm_{\xrm} \left[ \Erm_{\Drm | \xrm} \left[ \left( \frac{\alpha'(\xrm) \mu_{\yrm | \xrm} + \sum_{n=1}^N \Yrm_n \delta\big( \xrm - \Xrm_n \big)}{\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big)} \right)^2 \right] \right] \nonumber \\
\quad = \Erm_{\xrm} \left[ \Erm_{\Drm} \left[ \frac{ \alpha_0 \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \sum_{n=1}^N \Yrm_n \delta\big( \xrm - \Xrm_n \big) \right)^2 }{\alpha'(\xrm) \left(\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big) \right) (\alpha_0 + N)} \right] \right] \nonumber \\
\quad = \Erm_{\xrm} \left[ \Erm_{\Xrm} \left[ \frac{\alpha_0 \Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \sum_{n=1}^N \Yrm_n \delta\big( \xrm - \Xrm_n \big) \right)^2 \right] }{\alpha'(\xrm) \left(\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big) \right) (\alpha_0+N)} \right] \right] \nonumber 
\end{IEEEeqnarray}

Evaluating the expectation over $\Yrm$ given $\Xrm$, we have
\begin{IEEEeqnarray}{L}
\Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \sum_{n=1}^N \Yrm_n \delta\big( \xrm - \Xrm_n \big) \right)^2 \right] \\ 
= \alpha'(\xrm)^2 \mu_{\yrm | \xrm}^2 + 2\alpha'(\xrm) \mu_{\yrm | \xrm} \sum_{n=1}^N \mu_{\yrm | \xrm}\big( \Xrm_n \big) \delta\big( \xrm - \Xrm_n \big) \nonumber \\
\quad + \sum_{n=1}^N \Erm_{\yrm|\xrm}\big[ \yrm^2 \big]\big( \Xrm_n \big) \delta\big( \xrm - \Xrm_n \big)^2 \nonumber \\
\quad + \sum_{n \neq n'} \frac{\alpha'\big( \Xrm_n \big) \mu_{\yrm | \xrm}\big(\Xrm_n \big) \alpha'\big( \Xrm_{n'} \big) \mu_{\yrm | \xrm}\big( \Xrm_{n'} \big) + \alpha'\big( \Xrm_n \big) \Erm_{\yrm|\xrm}\big[ \yrm^2 \big]\big( \Xrm_n \big) \delta\big( \Xrm_n-\Xrm_{n'} \big)}{\alpha'\big( \Xrm_n \big) \alpha'\big( \Xrm_{n'} \big) + \alpha'\big( \Xrm_n \big) \delta\big( \Xrm_n-\Xrm_{n'} \big)} \nonumber \\
\qquad \quad \delta\big( \xrm - \Xrm_n \big) \delta\big( \xrm - \Xrm_{n'} \big) \nonumber \\
= \ldots \nonumber \\
= \alpha'(\xrm)^2 \mu_{\yrm | \xrm}^2 + 2\alpha'(\xrm) \mu_{\yrm | \xrm}^2 \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big)^2 \nonumber \\
\quad + \frac{\alpha'(\xrm) \mu_{\yrm | \xrm}^2 + \Erm_{\yrm|\xrm}\big[ \yrm^2  \big] \delta(0)}{\alpha'(\xrm) + \delta(0)} \sum_{n \neq n'} \delta\big( \xrm - \Xrm_n \big) \delta\big( \xrm - \Xrm_{n'} \big) \nonumber \\
\ldots \nonumber \\
= \frac{\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big)}{\alpha'(\xrm) + \delta(0)} \nonumber \\
\quad \left( \Erm_{\yrm|\xrm}\big[ \yrm^2  \big] \delta(0) \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) + \alpha'(\xrm) \mu_{\yrm | \xrm}^2 \left( \alpha'(\xrm) + \delta(0) + \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \right) \right) \nonumber
\end{IEEEeqnarray}


PGR: Y given X PDFs, moments??? In PDF section, or in Appendix?





Plugging,
\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\Drm} \Big[ \mu_{\yrm | \xrm,\Drm}^2 \Big] \\
\quad = \Erm_{\xrm} \left[ \Erm_{\Xrm} \left[ \frac{\alpha_0 \Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \sum_{n=1}^N \Yrm_n \delta\big( \xrm - \Xrm_n \big) \right)^2 \right] }{\alpha'(\xrm) \left(\alpha'(\xrm) + \sum_{n=1}^N \delta\big( \xrm - \Xrm_n \big) \right) (\alpha_0+N)} \right] \right] \nonumber \\
\quad = \Erm_{\xrm} \left[ \frac{\alpha_0 \Erm_{\Xrm} \left[ \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) + \alpha'(\xrm) \mu_{\yrm | \xrm}^2 \left( \alpha'(\xrm) + \delta(0) + \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \right) \right] }{\alpha'(\xrm) \big( \alpha'(\xrm) + \delta(0) \big) (\alpha_0+N)} \right] \nonumber 
\end{IEEEeqnarray}

Evaluating the expectation over $\Xrm$,
\begin{IEEEeqnarray}{L}
\Erm_{\Xrm} \left[ \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) + \alpha'(\xrm) \mu_{\yrm | \xrm}^2 \left( \alpha'(\xrm) + \delta(0) + \sum_{n=1}^N \delta\big( \xrm-\Xrm_n \big) \right) \right] \nonumber \\
\quad = \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N \frac{\alpha'(\xrm)}{\alpha_0} + \alpha'(\xrm) \mu_{\yrm | \xrm}^2 \left( \alpha'(\xrm) + \delta(0) + N \frac{\alpha'(\xrm)}{\alpha_0} \right) \nonumber \\
\quad = \frac{\alpha'(\xrm)}{\alpha_0} \Big( \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N + \mu_{\yrm | \xrm}^2 \big( \alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm) \big) \Big) \nonumber
\end{IEEEeqnarray}

Plugging,
\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\Drm} \Big[ \mu_{\yrm | \xrm,\Drm}^2 \Big] \\
\quad = \Erm_{\xrm} \left[ \frac{\Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N + \mu_{\yrm | \xrm}^2 \big( \alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm) \big)}{ \big( \alpha'(\xrm) + \delta(0) \big) (\alpha_0+N)} \right] \nonumber
\end{IEEEeqnarray}

Combining with the second moment produces the risk,
\begin{IEEEeqnarray}{L}
\Rcal^* = \Erm_{\xrm,\Drm} \Big[ \Erm_{\yrm | \xrm,\Drm}\big[ \yrm^2 \big] - \mu_{\yrm | \xrm,\Drm}^2 \Big] \\
= \Erm_{\xrm} \left[ \frac{\alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm)}{(\alpha_0+N) \big( \alpha'(\xrm)+\delta(0) \big)} \Sigma_{\yrm | \xrm} \right] \nonumber \\
= \Erm_{\xrm} \left[ \frac{\Prm_{\xrm}(\xrm) + (\alpha_0+N)^{-1} \delta(0)}{\Prm_{\xrm}(\xrm) + \alpha_0^{-1} \delta(0)} \Sigma_{\yrm | \xrm} \right] \nonumber
\end{IEEEeqnarray}

%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\Drm} \left[ \mu_{\yrm | \xrm,\Drm}^2 \right] \\
%\quad = \int_{\Xcal} \int_{\Dcal} \prm_{\xrm,\Drm}(x,D) \left( \int_{\Ycal} y \prm_{\yrm | \xrm,\Drm}(y | x,D) \mathrm{d}y \right)^2 \mathrm{d}D \mathrm{d}x \nonumber \\
%\quad = \int_{\Xcal} \Erm_{\Drm} \left[ \int_{\Ycal} y \prm_{\yrm,\xrm | \Drm}(y,x | \Drm) \mathrm{d} y \int_{\Ycal} y' \prm_{\yrm |\xrm,\Drm}(y' | x,\Drm) \mathrm{d} y' \right] \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \Erm_{\Yrm,\Xrm} \left[ \frac{ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \sum_{n=1}^N \Yrm_n \delta\big( x - \Xrm_n \big) \right)^2 }{(\alpha_0+N) \left(\alpha'(x) + \sum_{n=1}^N \delta\big( x - \Xrm_n \big) \right)} \right] \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \Erm_{\Xrm} \left[ \frac{ \Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \sum_{n=1}^N \Yrm_n \delta\big( x - \Xrm_n \big) \right)^2 \right] }{(\alpha_0+N) \left(\alpha'(x) + \sum_{n=1}^N \delta\big( x - \Xrm_n \big) \right)} \right] \mathrm{d}x \nonumber 
%\end{IEEEeqnarray}
%
%Evaluating the expectation over $\Yrm$ given $\Xrm$, we have
%\begin{IEEEeqnarray}{L}
%\Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \sum_{n=1}^N \Yrm_n \delta\big( x - \Xrm_n \big) \right)^2 \right] \\ 
%= \alpha'(x)^2 \mu_{\yrm | \xrm}^2(x) + 2\alpha'(x) \mu_{\yrm | \xrm}(x) \sum_{n=1}^N \mu_{\yrm | \xrm}\big( \Xrm_n \big) \delta\big( x - \Xrm_n \big) \nonumber \\
%\quad + \sum_{n=1}^N \Erm_{\yrm|\xrm}\big[ \yrm^2 \big]\big( \Xrm_n \big) \delta\big( x - \Xrm_n \big)^2 \nonumber \\
%\quad + \sum_{n \neq n'} \frac{\alpha'\big( \Xrm_n \big) \mu_{\yrm | \xrm}\big(\Xrm_n \big) \alpha'\big( \Xrm_{n'} \big) \mu_{\yrm | \xrm}\big( \Xrm_{n'} \big) + \alpha'\big( \Xrm_n \big) \Erm_{\yrm|\xrm}\big[ \yrm^2 \big]\big( \Xrm_n \big) \delta\big( \Xrm_n-\Xrm_{n'} \big)}{\alpha'\big( \Xrm_n \big) \alpha'\big( \Xrm_{n'} \big) + \alpha'\big( \Xrm_n \big) \delta\big( \Xrm_n-\Xrm_{n'} \big)} \nonumber \\
%\qquad \delta\big( x - \Xrm_n \big) \delta\big( x - \Xrm_{n'} \big) \nonumber \\
%= \ldots \nonumber \\
%= \alpha'(x)^2 \mu_{\yrm | \xrm}^2(x) + 2\alpha'(x) \mu_{\yrm | \xrm}^2(x) \sum_{n=1}^N \delta\big( x - \Xrm_n \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \sum_{n=1}^N \delta\big( x - \Xrm_n \big)^2 \nonumber \\
%\quad + \frac{\alpha'(x) \mu_{\yrm | \xrm}^2(x) + \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \delta(0)}{\alpha'(x) + \delta(0)} \nonumber \\
%\qquad \sum_{n \neq n'} \delta\big( x - \Xrm_n \big) \delta\big( x - \Xrm_{n'} \big) \nonumber \\
%\ldots \nonumber \\
%= \frac{\alpha'(x) + \sum_{n=1}^N \delta\big( x-\Xrm_n \big)}{\alpha'(x) + \delta(0)} \nonumber \\
%\quad \left( \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \delta(0) \sum_{n=1}^N \delta\big( x-\Xrm_n \big) + \alpha'(x) \mu_{\yrm | \xrm}^2(x) \left( \alpha'(x) + \delta(0) + \sum_{n=1}^N \delta\big( x-\Xrm_n \big) \right) \right) \nonumber
%\end{IEEEeqnarray}
%
%
%PGR: Y given X PDFs, moments??? In PDF section, or in Appendix?
%
%
%
%
%
%Plugging,
%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\Drm} \Big[ \mu_{\yrm | \xrm,\Drm}^2 \Big] \\
%\quad = \int_{\Xcal} \Erm_{\Xrm} \left[ \frac{ \Erm_{\Yrm | \Xrm} \left[ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \sum_{n=1}^N \Yrm_n \delta\big( x - \Xrm_n \big) \right)^2 \right] }{(\alpha_0+N) \left(\alpha'(x) + \sum_{n=1}^N \delta\big( x - \Xrm_n \big) \right)} \right] \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \frac{ \Erm_{\Xrm} \left[ \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \delta(0) \sum_{n=1}^N \delta\big( x-\Xrm_n \big) + \alpha'(x) \mu_{\yrm | \xrm}^2(x) \left( \alpha'(x) + \delta(0) + \sum_{n=1}^N \delta\big( x-\Xrm_n \right) \big) \right] }{(\alpha_0+N) \big( \alpha'(x) + \delta(0) \big)} \mathrm{d}x \nonumber 
%\end{IEEEeqnarray}
%
%Evaluating the expectation over $\Xrm$,
%\begin{IEEEeqnarray}{L}
%\Erm_{\Xrm} \left[ \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \delta(0) \sum_{n=1}^N \delta\big( x-\Xrm_n \big) + \alpha'(x) \mu_{\yrm | \xrm}^2(x) \left( \alpha'(x) + \delta(0) + \sum_{n=1}^N \delta\big( x-\Xrm_n \big) \right) \right] \nonumber \\
%\quad = \Erm_{\yrm|\xrm}\big[ \yrm^2  \big](x) \delta(0) N \frac{\alpha'(x)}{\alpha_0} + \alpha'(x) \mu_{\yrm | \xrm}^2(x) \left( \alpha'(x) + \delta(0) + N \frac{\alpha'(x)}{\alpha_0} \right) \nonumber \\
%\quad = \frac{\alpha'(x)}{\alpha_0} \Big( \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \delta(0) N + \mu_{\yrm | \xrm}^2(x) \big( \alpha_0 \alpha'(x) + \alpha_0 \delta(0) + N \alpha'(x) \big) \Big) \nonumber
%\end{IEEEeqnarray}
%
%Plugging,
%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\Drm} \Big[ \mu_{\yrm | \xrm,\Drm}^2 \Big] \\
%\quad = \Erm_{\xrm} \frac{\Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N + \mu_{\yrm | \xrm}^2 \big( \alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm) \big)}{(\alpha_0+N) \big( \alpha'(\xrm) + \delta(0) \big)} \nonumber
%\end{IEEEeqnarray}
%
%Combining with the second moment produces the risk,
%\begin{IEEEeqnarray}{L}
%\Rcal^* = \Erm_{\xrm,\Drm} \left[ \Erm_{\yrm | \xrm,\Drm}\big[ \yrm^2 \big] - \mu_{\yrm | \xrm,\Drm}^2 \right] \\
%= \Erm_{\xrm} \left[ \frac{\alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm)}{(\alpha_0+N)(\alpha'(\xrm)+\delta(0))} \Sigma_{\yrm | \xrm} \right] \nonumber \\
%= \Erm_{\xrm} \left[ \frac{\Prm(\xrm) + (\alpha_0+N)^{-1} \delta(0)}{\Prm(\xrm) + \alpha_0^{-1} \delta(0)} \Sigma_{\yrm | \xrm} \right] \nonumber
%\end{IEEEeqnarray}

PGR: Discuss Dirac deltas!!!


PGR: DMP PERSPECTIVE???

To perform the expectation over the Dirichlet-Multinomial process $\bar{\nrm} \sim \DMP(\alpha)$, split the expectation into an expectation over the marginal DMP $\nrm' \sim \DMP(\alpha')$ and a conditional expectation over $\bar{\nrm}$ given $\nrm'$. The characterization of the conditional DMP is found in Appendix \ref{app:DM_agg}.

\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\bar{\nrm}} \Big[ \mu_{\yrm | \xrm,\bar{\nrm}}^2 \Big] \\
\quad = \Erm_{\xrm} \left[ \Erm_{\nbarrm | \xrm} \left[ \left( \frac{\alpha'(\xrm) \mu_{\yrm | \xrm} + \int_{\Ycal} y \bar{\nrm}(y,\xrm) \mathrm{d}y}{\alpha'(\xrm) + \nrm'(\xrm)} \right)^2 \right] \right] \nonumber \\
\quad = \Erm_{\xrm} \Erm_{\nrm'} \left[ \frac{\alpha_0 \Erm_{\bar{\nrm} | \nrm'} \left[ \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \int_{\Ycal} y \bar{\nrm}(y,\xrm) \mathrm{d}y \right)^2 \right] }{\alpha'(\xrm)\big(\alpha'(x) + \nrm'(x) \big) (\alpha_0+N)} \right] \nonumber
\end{IEEEeqnarray}


Evaluating the conditional expectation,
\begin{IEEEeqnarray}{L}
\Erm_{\bar{\nrm} | \nrm'} \left[ \left( \alpha'(\xrm) \mu_{\yrm | \xrm} + \int_{\Ycal} y \bar{\nrm}(y,\xrm) \mathrm{d}y \right)^2 \right] \\
\quad = \alpha'(\xrm)^2 \mu_{\yrm | \xrm}^2 + 2 \alpha'(\xrm) \mu_{\yrm | \xrm} \int_{\Ycal} y \delta(0) \frac{\nrm'(\xrm)}{\delta(0)} \frac{\delta(0)^{-1} \alpha(y,\xrm)}{\delta(0)^{-1} \alpha'(\xrm)} \mathrm{d}y \nonumber \\
\qquad + \delta(0)^2 \frac{\delta(0)^{-1} \nrm'(\xrm)}{\big( \delta(0)^{-1}\alpha'(\xrm) \big) \big( \delta(0)^{-1}\alpha'(\xrm) + 1 \big)} \int_{\Ycal} \int_{\Ycal} y y' \Bigg[ \left( \frac{\nrm'(\xrm)}{\delta(0)} - 1 \right) \frac{\alpha(y,\xrm)}{\delta(0)} \frac{\alpha(y',\xrm)}{\delta(0)} \nonumber \\
\qquad + \left( \frac{\alpha'(\xrm)}{\delta(0)} + \frac{\nrm'(\xrm)}{\delta(0)} \right) \frac{\alpha(y,\xrm)}{\delta(0)} \delta(y-y') \Bigg] \mathrm{d}y \mathrm{d}y' \nonumber \\
\quad = \alpha'(\xrm)^2 \mu_{\yrm | \xrm}^2 + 2 \alpha'(\xrm) \nrm'(\xrm) \mu_{\yrm | \xrm}^2 \nonumber \\
\qquad + \frac{\nrm'(\xrm)}{\alpha'(\xrm)\big( \alpha'(\xrm)+\delta(0) \big)} \Big[ \big( \nrm'(\xrm) - \delta(0) \big) \alpha'(\xrm)^2 \mu_{\yrm | \xrm}^2 + \delta(0) \big( \alpha'(\xrm)+\nrm'(\xrm) \big) \alpha'(\xrm) \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \Big] \nonumber \\
\quad = \frac{\alpha'(\xrm)+\nrm'(\xrm)}{\alpha'(\xrm)+\delta(0)} \Big[ \mu_{\yrm | \xrm}^2 \alpha'(\xrm) \big( \alpha'(\xrm) + \nrm'(\xrm) + \delta(0) \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) \nrm'(\xrm) \Big] \nonumber
\end{IEEEeqnarray}

Plugging,
\begin{IEEEeqnarray}{L}
\Erm_{\xrm,\bar{\nrm}} \Big[ \mu_{\yrm | \xrm,\bar{\nrm}}^2 \Big] \\
\quad = \Erm_{\xrm} \left[ \frac{\alpha_0 \Erm_{\nrm'} \Big[ \mu_{\yrm | \xrm}^2 \alpha'(\xrm) \big( \alpha'(\xrm) + \nrm'(\xrm) + \delta(0) \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) \nrm'(\xrm) \Big] }{\alpha'(\xrm) \big( \alpha'(\xrm)+\delta(0) \big) (\alpha_0+N)} \right] \nonumber \\ 
\quad = \Erm_{\xrm} \left[ \frac{\mu_{\yrm | \xrm}^2 \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \delta(0) \alpha_0 \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N}{(\alpha_0+N) \big( \alpha'(\xrm)+\delta(0) \big)} \right] \nonumber
\end{IEEEeqnarray}

Combining produces the risk,
\begin{IEEEeqnarray}{L}
\Rcal^* = \Erm_{\xrm} \left[ \frac{\alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm)}{(\alpha_0+N)\big( \alpha'(\xrm)+\delta(0) \big)} \Sigma_{\yrm | \xrm} \right] \\
= \Erm_{\xrm} \left[ \frac{\Prm(\xrm) + (\alpha_0+N)^{-1} \delta(0)}{\Prm(\xrm) + \alpha_0^{-1} \delta(0)} \Sigma_{\yrm | \xrm} \right] \nonumber
\end{IEEEeqnarray}


%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\bar{\nrm}} \Big[ \mu_{\yrm | \xrm,\bar{\nrm}}^2 \Big] \\
%\quad = \Erm_{\xrm,\bar{\nrm}} \Bigg[ \left( \int_{\Ycal} y \prm_{\yrm | \xrm,\bar{\nrm}}(y | \xrm,\bar{\nrm}) \mathrm{d}y \right)^2 \Bigg] \nonumber \\
%\quad = \int_{\Xcal} \Erm_{\bar{\nrm}} \left[ \int_{\Ycal} y \prm_{\yrm,\xrm | \bar{\nrm}}(y,x | \bar{\nrm}) \mathrm{d}y \int_{\Ycal} y' \prm_{\yrm | \xrm,\bar{\nrm}}(y' | x,\bar{\nrm}) \mathrm{d}y' \right] \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \Erm_{\nrm'} \left[ \frac{ \Erm_{\bar{\nrm} | \nrm'} \left[ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \int_{\Ycal} y \bar{\nrm}(y,x) \mathrm{d}y \right)^2 \right] }{(\alpha_0+N) \big(\alpha'(x) + \nrm'(x) \big)} \right] \mathrm{d}x \nonumber
%\end{IEEEeqnarray}
%
%
%Evaluating the conditional expectation,
%\begin{IEEEeqnarray}{L}
%\Erm_{\bar{\nrm} | \nrm'} \left[ \left( \alpha'(x) \mu_{\yrm | \xrm}(x) + \int_{\Ycal} y \bar{\nrm}(y,x) \mathrm{d}y \right)^2 \right] \\
%\quad = \alpha'(x)^2 \mu_{\yrm | \xrm}^2(x) + 2 \alpha'(x) \mu_{\yrm | \xrm}(x) \int_{\Ycal} y \delta(0) \frac{\nrm'(x)}{\delta(0)} \frac{\delta(0)^{-1} \alpha(y,x)}{\delta(0)^{-1} \alpha'(x)} \mathrm{d}y \nonumber \\
%\qquad + \delta(0)^2 \frac{\delta(0)^{-1} \nrm'(x)}{\big( \delta(0)^{-1}\alpha'(x) \big) \big( \delta(0)^{-1}\alpha'(x) + 1 \big)} \int_{\Ycal} \int_{\Ycal} y y' \Bigg[ \left( \frac{\nrm'(x)}{\delta(0)} - 1 \right) \frac{\alpha(y,x)}{\delta(0)} \frac{\alpha(y',x)}{\delta(0)} \nonumber \\
%\qquad + \left( \frac{\alpha'(x)}{\delta(0)} + \frac{\nrm'(x)}{\delta(0)} \right) \frac{\alpha(y,x)}{\delta(0)} \delta(y-y') \Bigg] \mathrm{d}y \mathrm{d}y' \nonumber \\
%\quad = \alpha'(x)^2 \mu_{\yrm | \xrm}^2(x) + 2 \alpha'(x) \nrm'(x) \mu_{\yrm | \xrm}^2(x) \nonumber \\
%\qquad + \frac{\nrm'(x)}{\alpha'(x)\big( \alpha'(x)+\delta(0) \big)} \Big[ \big( \nrm'(x) - \delta(0) \big) \alpha'(x)^2 \mu_{\yrm | \xrm}^2(x) + \delta(0) \big( \alpha'(x)+\nrm'(x) \big) \alpha'(x) \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \Big] \nonumber \\
%\quad = \frac{\alpha'(x)+\nrm'(x)}{\alpha'(x)+\delta(0)} \Big[ \mu_{\yrm | \xrm}^2(x) \alpha'(x) \big( \alpha'(x) + \nrm'(x) + \delta(0) \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \delta(0) \nrm'(x) \Big] \nonumber
%\end{IEEEeqnarray}
%
%Plugging,
%\begin{IEEEeqnarray}{L}
%\Erm_{\xrm,\bar{\nrm}} \Big[ \mu_{\yrm | \xrm,\bar{\nrm}}^2 \Big] \\
%\quad = \int_{\Xcal} \frac{ \Erm_{\nrm'} \Big[ \mu_{\yrm | \xrm}^2(x) \alpha'(x) \big( \alpha'(x) + \nrm'(x) + \delta(0) \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \delta(0) \nrm'(x) \Big] }{(\alpha_0+N) \big( \alpha'(x)+\delta(0) \big)} \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \frac{\mu_{\yrm | \xrm}^2(x) \alpha'(x) \big( \alpha'(x) + N \alpha_0^{-1} \alpha'(x) + \delta(0) \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \delta(0) N \alpha_0^{-1} \alpha'(x)}{(\alpha_0+N) \big( \alpha'(x)+\delta(0) \big)} \mathrm{d}x \nonumber \\ 
%\quad = \int_{\Xcal} \frac{\alpha'(x)}{\alpha_0} \frac{\mu_{\yrm | \xrm}^2(x) \big( \alpha_0 \alpha'(x) + N \alpha'(x) + \delta(0) \alpha_0 \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big](x) \delta(0) N}{(\alpha_0+N) \big( \alpha'(x)+\delta(0) \big)} \mathrm{d}x \nonumber \\
%\quad = \Erm_{\xrm} \left[ \frac{\mu_{\yrm | \xrm}^2 \big( \alpha_0 \alpha'(\xrm) + N \alpha'(\xrm) + \delta(0) \alpha_0 \big) + \Erm_{\yrm|\xrm}\big[ \yrm^2 \big] \delta(0) N}{(\alpha_0+N) \big( \alpha'(\xrm)+\delta(0) \big)} \right] \nonumber
%\end{IEEEeqnarray}
%
%Combining produces the risk,
%\begin{IEEEeqnarray}{L}
%\Rcal^* = \Erm_{\xrm} \left[ \frac{\alpha_0 \alpha'(\xrm) + \alpha_0 \delta(0) + N \alpha'(\xrm)}{(\alpha_0+N)\big( \alpha'(\xrm)+\delta(0) \big)} \Sigma_{\yrm | \xrm} \right] \\
%= \Erm_{\xrm} \left[ \frac{\Prm(\xrm) + (\alpha_0+N)^{-1} \delta(0)}{\Prm(\xrm) + \alpha_0^{-1} \delta(0)} \Sigma_{\yrm | \xrm} \right] \nonumber
%\end{IEEEeqnarray}

















\newpage

\appendix



\chapter{}

PGR: notation/formatting

PGR: remove beta set arguments by introducing alpha sub z?


\section{Dirichlet random process conditioned on its aggregation}
\label{app:Dir_agg}

This section details an important property of Dirichlet distributed random processes. The following development first considers Dirichlet random processes over a countable domain and then generalizes for continuous-domain Dirichlet processes. 

First, define the PDF of a Dirichlet aggregation \cite{ferguson}. Let the random process $\uptheta \in \Theta = \Pcal(\Ycal)$ be Dirichlet over the countable set $\Ycal$ with parameterizing function $\alpha \in {\Rbb^+}^{\Ycal}$. Define an arbitrary partition of $\Ycal$: $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$; the aggregation $\uptheta' \in \Pcal(\Zcal)$, $\uptheta'(z) \equiv \sum_{y \in \Scal(z)} \uptheta(y)$ is thus also Dirichlet and has a parameterizing function $\alpha' \in {\Rbb^+}^{\Zcal}$, $\alpha'(z) \equiv \sum_{y \in \Scal(z)} \alpha(y)$.

The PDF of the original random process $\uptheta$ conditioned on its aggregation $\uptheta'$ can be formulated as
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta | \uptheta'}(\theta | \theta') & = & \frac{\beta(\alpha') \prod_{y \in \Ycal} \theta(y)^{\alpha(y)-1}}{\beta(\alpha) \prod_{z \in \Zcal} \theta'(z)^{\alpha'(z)-1}} \\
& = & \prod_{z \in \Zcal} \Bigg[ \beta\Big( \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big)^{-1} \frac{\prod_{y \in \Scal(z)} \theta(y)^{\alpha(y)-1}}{\theta'(z)^{\alpha'(z)-1}} \Bigg] \nonumber \\ 
& = & \prod_{z \in \Zcal} \Bigg[ \frac{\theta'(z)^{1-|\Scal(z)|}}{\beta\Big( \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big)} \prod_{y \in \Scal(z)} \left(\frac{\theta(y)}{\theta'(z)}\right)^{\alpha(y)-1} \Bigg] \nonumber \;,
\end{IEEEeqnarray}
which is defined for $\left\{ \theta \in {\Rbb_{\geq 0}}^{\Ycal} : \sum_{y \in \Scal(z)} \theta(y) = \theta'(z), \quad \forall z \in \Zcal \right\}$.

Observe that the partitioned segements are conditionally independent; introduce subscript notation to refer to the function segment $\uptheta_z = \big\{ \uptheta(y): y \in \Scal(z) \big\}$. The PDF $\prm_{\uptheta | \uptheta'}$ can now be decomposed as $\prm_{\uptheta | \uptheta'}(\ldots,\theta_z,\ldots | \theta') = \prod_{z \in \Zcal} \prm_{\uptheta_z | \uptheta'(z)}\big( \theta_z | \theta'(z) \big)$

Next, normalize the segments of $\uptheta$ to form $\tilde{\uptheta} = \big( \ldots,\tilde{\uptheta}_z,\ldots \big)$, where $\tilde{\uptheta}_z \equiv \uptheta_z / \uptheta'(z)$, and formulate the conditional PDF
\begin{IEEEeqnarray}{rCl}
\prm_{\tilde{\uptheta} | \uptheta'}\left( \tilde{\theta} | \theta' \right) & = & \prod_{z \in \Zcal} \Bigg[ \frac{\prod_{y \in \Scal(z)} \tilde{\theta}_z(y)^{\alpha(y)-1}}{\beta\Big( \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big)} \Bigg] \\
& = & \prod_{z \in \Zcal} \Dir\Big( \tilde{\theta}_z ; \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big) \nonumber \;.
\end{IEEEeqnarray}
which is defined for $\tilde{\theta} \in \prod_{z \in \Zcal} \left\{ \tilde{\theta}_z \in \Pcal\big(\Scal(z)\big) \right\}$. Thus after conditioning, the normalized segments $\tilde{\uptheta}_z$ are Dirichlet distributed, independent of one another, and independent of the aggregation $\uptheta'$. 

PGR: discuss transform Jacobian and dimensionality? 

This principle holds for continuous-domain Dirichlet processes $\uptheta$ as well - the segments $\tilde{\uptheta}_z$ are now continuous-domain Dirichlet processes.








\section{Multinomial Distribution Properties}
\label{app:mult}

\subsection{Aggregation}

A characteristic of a Multinomial random process is that its aggregations are also Multinomial \cite{johnson}. Consider a random process $\bar{\nrm} \sim \Multi(N,\theta)$ over the set $\Ycal$. Define an arbitrary partition of $\Ycal$: $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$; the transformed random process $\nrm'(z) \equiv \sum_{y \in \Scal(z)} \bar{\nrm}(y)$ is distributed as $\nrm' \sim \Multi(N,\theta')$ with parameterizing function $\theta'(z) = \sum_{y \in \Scal(z)} \theta(y)$.

To prove this principle, define the subset $\tilde{\Ncal} = \big\{ \bar{n} \in \Ncal : \sum_{y \in \Scal(z)} \bar{n}(y) = n'(z), \quad \forall z \in \Zcal \big\} \subseteq \bar{\Ncal}$, where the original random process $\bar{\nrm} \in \bar{\Ncal}$. Next, observe
\begin{IEEEeqnarray}{rCl}
\Prm_{\mathrm{n}'}(n') & = & \sum_{\bar{n} \in \tilde{\Ncal}} \Prm_{\nbarrm}(\bar{n}) \\
& = & \Mcal(n') \prod_{z \in \Zcal} \sum_{\substack{n'(z) = \\ \sum_{y \in \Scal(z)} \bar{n}(y)}} \Mcal\big( \{ \bar{n}(y): y \in \Scal(z) \} \big) \prod_{y \in \Scal(z)} \theta(y)^{\bar{n}(y)} \nonumber \\
& = & \Mcal(n') \prod_{z \in \Zcal} \theta'(z)^{n'(z)} = \Multi(n' ; N,\theta') \nonumber \;,
\end{IEEEeqnarray}
where the multinomial theorem \cite{graham} has been used.




\subsection{Conditioned on its Aggregation}

If the multinomial random process $\bar{\nrm}$ is conditioned on its aggregation over the partition $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$, the distinct segements $\bar{\nrm}(y)$, $y \in \Scal(z)$ become independent multinomial random processes,
\begin{IEEEeqnarray}{rCl}
\Prm_{\bar{\nrm} | \nrm'}(\bar{n} | n') & = & \frac{\Mcal(\bar{n}) \prod_{y \in \Ycal} \theta(y)^{\bar{n}(y)}}{\Mcal(n') \prod_{z \in \Zcal} \theta'(z)^{n'(z)}} \\
& = & \prod_{z \in \Zcal} \Bigg[ \Mcal\Big( \big\{ \bar{n}(y) : y \in \Scal(z) \big\} \Big) \prod_{y \in \Scal(z)} \left(\frac{\theta(y)}{\theta'(z)}\right)^{\bar{n}(y)} \Bigg] \nonumber \\
& = & \prod_{z \in \Zcal} \Bigg[ \Mcal\Big( \big\{ \bar{n}(y) : y \in \Scal(z) \big\} \Big) \prod_{y \in \Scal(z)} \tilde{\theta}_z(y)^{\bar{n}(y)} \Bigg] \nonumber \\
& = & \prod_{z \in \Zcal} \Multi\Big( \big\{ \bar{n}(y) : y \in \Scal(z) \big\} ; n'(z) , \big\{ \tilde{\theta}_z(y) : y \in \Scal(z) \big\} \Big) \nonumber \;,
\end{IEEEeqnarray}
on the domain $\left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal} : \sum_{y \in \Scal(z)} \bar{n}(y) = n'(z), \quad \forall z \in \Zcal \right\}$. Observe that the segment over the set $\Scal(z)$ sums to $\nrm'(z)$ and is parameterized by the normalized segment $\tilde{\theta}_z \equiv \big\{ \theta(y)/\theta'(z) : y \in \Scal(z) \big\}$.






\section{Dirichlet-Multinomial random process conditioned on its aggregation} 
\label{app:DM_agg}

A defining characteristic of a Dirichlet-Multinomial random process is that its aggregations are also Dirichlet-Multinomial \cite{johnson}. Consider a DM random process $\bar{\nrm} \sim \DM(N,\alpha)$ over the set $\Ycal$. Define an arbitrary partition of $\Ycal$: $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$; the transformed random process $\nrm'(z) \equiv \sum_{y \in \Scal(z)} \bar{\nrm}(y)$ is neccessarily Dirichlet-Multinomial with parameterizing function $\alpha'(z) = \sum_{y \in \Scal(z)} \alpha(y)$.

It can be shown that conditioned on the aggregation $\nrm'$, the segments $\big\{ \bar{\nrm}(y) : y \in \Scal(z) \big\}$ of the original random process become independent Dirichlet-Multinomial random processes, such that
\begin{IEEEeqnarray}{rCl}
\Prm_{\bar{\nrm} | \nrm'}(\bar{n} | n') & = & \frac{\Mcal(\bar{n}) \beta(\alpha)^{-1} \beta(\alpha+\bar{n})}{\Mcal(n') \beta(\alpha')^{-1} \beta(\alpha'+n')} \\
& = & \left( \prod_{z \in \Zcal} \frac{\Gamma\big( \alpha'(z)+n'(z) \big)}{n'(z)! \Gamma\big( \alpha'(z) \big)} \right)^{-1} \left( \prod_{y \in \Ycal} \frac{\Gamma\big( \alpha(y)+\bar{n}(y) \big)}{\bar{n}(y)! \Gamma\big( \alpha(y) \big)} \right) \nonumber \\
& = & \prod_{z \in \Zcal} \left[ \frac{n'(z)! \Gamma\big( \alpha'(z) \big)}{\Gamma\big( \alpha'(z)+n'(z) \big)} \prod_{y \in \Scal(z)} \frac{\Gamma\big( \alpha(y)+\bar{n}(y) \big)}{\bar{n}(y)! \Gamma\big( \alpha(y) \big)} \right] \nonumber \\
& = & \prod_{z \in \Zcal} \DM\Big( \big\{ \bar{n}(y) : y \in \Scal(z) \big\} ; n'(z), \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big) \nonumber \;,
\end{IEEEeqnarray}
%\begin{IEEEeqnarray}{rCl}
%\Prm(\bar{\nrm} | \nrm') & = & \frac{\Prm(\bar{\nrm})}{\Prm(\nrm')} \Prm(\nrm' | \bar{\nrm}) \\ 
%& = & \frac{\Mcal(\bar{\nrm}) \beta(\alpha)^{-1} \beta(\alpha+\bar{\nrm})}{\Mcal(\nrm') \beta(\alpha')^{-1} \beta(\alpha'+\nrm')} \prod_{z \in \Zcal} \delta\left[ \nrm'(z),\sum_{y \in \Scal(z)} \bar{\nrm}(y) \right] \nonumber \\
%& = & \left( \prod_{z \in \Zcal} \frac{\Gamma\big( \alpha'(z)+\nrm'(z) \big)}{\nrm'(z)! \Gamma\big( \alpha'(z) \big)} \right)^{-1} \left( \prod_{y \in \Ycal} \frac{\Gamma\big( \alpha(y)+\bar{\nrm}(y) \big)}{\bar{\nrm}(y)! \Gamma\big( \alpha(y) \big)} \right) \nonumber \\
%&& \quad \prod_{z \in \Zcal} \delta\left[ \nrm'(z),\sum_{y \in \Scal(z)} \bar{\nrm}(y) \right] \nonumber \\
%& = & \prod_{z \in \Zcal} \left[ \delta\left[ \nrm'(z),\sum_{y \in \Scal(z)} \bar{\nrm}(y) \right] \frac{\nrm'(z)! \Gamma\big( \alpha'(z) \big)}{\Gamma\big( \alpha'(z)+\nrm'(z) \big)} \prod_{y \in \Scal(z)} \frac{\Gamma\big( \alpha(y)+\bar{\nrm}(y) \big)}{\bar{\nrm}(y)! \Gamma\big( \alpha(y) \big)} \right] \nonumber \\
%& = & \prod_{z \in \Zcal} \DM\Big( \big\{ \bar{\nrm}(y) : y \in \Scal(z) \big\} ; \nrm'(z), \big\{ \alpha(y) : y \in \Scal(z) \big\} \Big) \nonumber \;,
%\end{IEEEeqnarray}
on the domain $\left\{ \bar{n} \in {\Zbb_{\geq 0}}^{\Ycal} : \sum_{y \in \Scal(z)} \bar{n}(y) = n'(z), \quad \forall z \in \Zcal \right\}$. 







\section{First and Second moments of a Dirichlet Process} \label{app:E_DP}

In this section, it is shown that the expected value of a Dirichlet process $\uptheta \sim \DP(\alpha)$ is 
\begin{equation}
\mu_{\uptheta} = \frac{\alpha}{\alpha_0} \;,
\end{equation}
where $\alpha_0 = \int_{\Ycal} \alpha(y) \mathrm{d} y$.

The defining characteristic of Dirichlet processes is that their aggregations are also Dirichlet. Define the partition of $\Ycal$, $\big\{ \Scal(y),S^c(y) \big\}$ where $\Scal(y) = \{ t \in \Ycal : t \leq y \}$. The transform random variable $\uptheta'(y) \equiv \int_{-\infty}^y \uptheta(t) \mathrm{d} t$ is thus a Beta random variable with parameters $\lambda = \int_{-\infty}^y \alpha(t) \mathrm{d} t$ and $\lambda^c = \int_y^\infty \alpha(t) \mathrm{d} t$. Using the formula for the expected value of a beta random variable \cite{papoulis}, note that
\begin{IEEEeqnarray}{rCl}
\mu_{\uptheta'} & = & \frac{\lambda}{\lambda+\lambda^c} \\
& = & \frac{\int_{-\infty}^y \alpha(t) \mathrm{d} t}{\alpha_0} = \int_{-\infty}^y \mu_{\uptheta}(t) \mathrm{d} t \nonumber \;.
\end{IEEEeqnarray}
Differentiating with respect to $y$, we have the expected value of the DP.

Next, the correlation function is shown to be 
\begin{equation}
\Erm_{\uptheta}\big[ \uptheta(y_1)\uptheta(y_2) \big] = \frac{\alpha(y_1)\alpha(y_2) + \alpha(y_1)\delta(y_1-y_2)}{\alpha_0(\alpha_0+1)} \;.
\end{equation}
First, assume $y_2 \geq y_1$ and define a new partition of $\Ycal$, $\big\{ (-\infty,y_1], (y_1,y_2], (y_2,\infty) \big\}$. By the aggregation property, the random triplet $\left( \int_{-\infty}^{y_1} \uptheta(t) \mathrm{d} t, \int_{y_1}^{y_2} \uptheta(t) \mathrm{d} t, \int_{y_2}^{\infty} \uptheta(t) \mathrm{d} t \right)$ is Dirichlet with parameters $\left( \int_{-\infty}^{y_1} \alpha(t) \mathrm{d} t, \int_{y_1}^{y_2} \alpha(t) \mathrm{d} t, \int_{y_2}^{\infty} \alpha(t) \mathrm{d} t \right)$.

Define the function
\begin{IEEEeqnarray}{L}
g(t_1,t_2) = \Erm_{\uptheta}\left[ \int_{-\infty}^{y_1} \uptheta(t_1) \mathrm{d} t_1 \int_{-\infty}^{y_2} \uptheta(t_2) \mathrm{d} t_2 \right] \\
\quad = \Erm_{\uptheta}\left[ \left( \int_{-\infty}^{y_1} \uptheta(t_1) \mathrm{d} t_1 \right)^2 + \left( \int_{-\infty}^{y_1} \uptheta(t_1) \mathrm{d} t_1 \right) \left( \int_{y_1}^{y_2} \uptheta(t_2) \mathrm{d} t_2 \right) \right] \nonumber \\
\quad = \frac{\left( \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right) \left( 1 + \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right) + \left( \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right) \left( \int_{y_1}^{y_2} \alpha(t_2) \mathrm{d} t_2 \right)}{\alpha_0(\alpha_0+1)} \nonumber \\
\quad = \frac{\left( \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right) \left( \int_{-\infty}^{y_2} \alpha(t_2) \mathrm{d} t_2 \right) + \left( \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right)}{\alpha_0(\alpha_0+1)} \quad \forall y_2 \geq y_1 \nonumber \;.
\end{IEEEeqnarray}
Following the same steps provides the values of $g$ for $t_2 \leq t_1$; the combined formula can be given as
\begin{IEEEeqnarray}{L}
g(t_1,t_2) = \frac{\left( \int_{-\infty}^{y_1} \alpha(t_1) \mathrm{d} t_1 \right) \left( \int_{-\infty}^{y_2} \alpha(t_2) \mathrm{d} t_2 \right) + \left( \int_{-\infty}^{\min(y_1,y_2)} \alpha(t_1) \mathrm{d} t_1 \right)}{\alpha_0(\alpha_0+1)} \;. 
\end{IEEEeqnarray}
Finally,
\begin{IEEEeqnarray}{L}
\Erm_{\uptheta}\big[ \uptheta(y_1)\uptheta(y_2) \big] = \frac{\mathrm{d}^2}{\mathrm{d} t_1 \mathrm{d} t_2} g(t_1,t_2) \\
\quad = \frac{\frac{\mathrm{d}}{\mathrm{d} t_2} \left[ \alpha(y_1) \left( \int_{-\infty}^{y_2} \alpha(t_2) \mathrm{d} t_2 \right) + u(t_2-t_1) \alpha\big( \min(t_1,t_2) \big) \right]}{\alpha_0(\alpha_0+1)} \nonumber \\
\quad = \frac{\alpha(y_1)\alpha(y_2) + \alpha(y_1)\delta(y_1-y_2)}{\alpha_0(\alpha_0+1)} \nonumber \;.
\end{IEEEeqnarray}










\section{Proof: Continuous Model Posterior Distribution is Dirichlet Process} \label{app:DP_post}

In this section, it is shown that if the model $\uptheta \sim \DP(\alpha)$ is a Dirichlet process, then the model conditioned on the training data $\Drm$ is also a Dirichlet process with parameterizing function $\alpha + \sum_{n=1}^N \delta\big( \cdot - \Drm_n \big)$. 

The defining characteristic of Dirichlet processes is that their aggregations are also Dirichlet. Consider a DP over the set $\Ycal$. Define an arbitrary partition of $\Ycal$: $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$; the transformed random process $\uptheta'(z) \equiv \int_{\Scal(z)} \uptheta(y) \mathrm{d} y$ is neccessarily Dirichlet with parameterizing function $\alpha'(z) \equiv \int_{\Scal(z)} \alpha(y) \mathrm{d} y$.

To prove the hypothesis, it is required that
\begin{IEEEeqnarray}{rCl}
\uptheta' | \Drm & \sim & \Dir\big( \alpha' + \bar{N}(\Drm) \big) \;,
\end{IEEEeqnarray}
where $\bar{N}(z;\Drm) = \int_{\Scal(z)} \sum_{n=1}^N \delta\big( y - \Drm_n \big) \mathrm{d} y = \sum_{n=1}^N \chi\big( \Drm_n;\Scal(z) \big)$. $\chi$ is the indicator function
\begin{equation}
\chi(x;S) = \begin{cases} 1 & \mathrm{if} \ x \in S, \\ 0 & \mathrm{if} \ x \notin S.  \end{cases}
\end{equation}


PGR: SUFFICIENT STATISTIC FOR D!!!?

To prove the hypothesis, exploit the results of Appendix \ref{app:Dir_agg} to represent the training data distribution conditioned on the aggregation $\uptheta'$. The conditional distribution of interest is
\begin{IEEEeqnarray}{L}
\prm_{\Drm | \uptheta'}(D | \uptheta') = \Erm_{\uptheta | \uptheta'}\big[ \prm_{\Drm | \uptheta}(D | \uptheta) \big] = \Erm_{\uptheta | \uptheta'}\left[ \prod_{n=1}^N \uptheta\big( D_n \big) \right] \\
= \prod_{z \in \Zcal} \Erm_{\uptheta_z | \uptheta'(z)}\left[ \prod_{n=1}^N \uptheta_z\big( D_n \big)^{\chi\big( D_n;\Scal(z) \big)} \right] \nonumber \\
= \left( \prod_{z \in \Zcal} \prod_{n=1}^N \uptheta'(z)^{\chi\big( D_n;\Scal(z) \big)} \right) \prod_{z \in \Zcal} \Erm_{\tilde{\uptheta}_z}\left[ \prod_{n=1}^N \tilde{\uptheta}_z\big( D_n \big)^{\chi\big( D_n;\Scal(z) \big)} \right] \nonumber \\
= \left( \prod_{z \in \Zcal} \uptheta'(z)^{\bar{N}(z;D)} \right) \prod_{z \in \Zcal} \Erm_{\tilde{\uptheta}_z}\left[ \prod_{n=1}^N \tilde{\uptheta}_z\big( D_n \big)^{\chi\big( D_n;\Scal(z) \big)} \right] \nonumber \;,
\end{IEEEeqnarray}
Observe that the dependency on $\uptheta'$ is polynomial. The training data marginal distribution is
\begin{IEEEeqnarray}{rCl}
\prm_{\Drm}(D) & = & \Erm_{\uptheta'} \left[ \prod_{z \in \Zcal} \uptheta'(z)^{\bar{N}(z;D)} \right] \prod_{z \in \Zcal} \Erm_{\tilde{\uptheta}_z}\left[ \prod_{n=1}^N \tilde{\uptheta}_z\big( D_n \big)^{\chi\big( D_n;\Scal(z) \big)} \right] \\
& = & \frac{\beta\big( \alpha' + \bar{N}(D) \big)}{\beta(\alpha')} \prod_{z \in \Zcal} \Erm_{\tilde{\uptheta}_z}\left[ \prod_{n=1}^N \tilde{\uptheta}_z\big( D_n \big)^{\chi\big( D_n;\Scal(z) \big)} \right] \nonumber
\end{IEEEeqnarray}
and thus the distribution of interest is
\begin{IEEEeqnarray}{rCl}
\prm_{\uptheta' | \Drm}(\theta' | D) & = & \frac{\prod_{z \in \Zcal} \theta'(z)^{\alpha'(z)+\bar{N}(z;D)-1}}{\beta\big( \alpha' + \bar{N}(D) \big)} \\
& = & \Dir\left( \uptheta' ; \alpha' + \bar{N}(\cdot;\Drm) \right) \nonumber \;.
\end{IEEEeqnarray}
This proves the hypothesis.




\section{The Dirichlet-Multinomial Process} \label{app:DMP}

PGR: multinomial process too???

\subsection{Definition}

This section introduces a new random process, referred to as the Dirichlet-Multinomial process (DMP). It is the generalization of the Dirichlet-Multinomial distribution for i.i.d. samples drawn from a PDF; the underlying distribution is characterized by a Dirchlet process with parameter $\alpha$. The Dirichlet-Multinomial process assumes functions from the set $\left\{ \bar{n} \in {\Rbb_{\geq 0}}^{\Ycal} : \int_{\Ycal} \bar{n}(y) \mathrm{d} y = N \right\}$ and is parameterized by a function $\alpha : \Ycal \mapsto \Rbb^+$.

Analagous to the Dirichlet and Dirichlet-Multinomial distributions for countable spaces, the Dirichlet-Multinomial process inherits the aggregation property from the Dirichlet process prior. That is, for a Dirichlet-Multinomial process $\bar{\nrm} \in \left\{ \bar{n} \in {\Rbb_{\geq 0}}^{\Ycal} : \int_{\Ycal} \bar{n}(y) \mathrm{d} y = N \right\}$ and a partition of $\Ycal$, $\left\{ \ldots,\Scal(z),\ldots \right\}$, $z \in \Zcal$, the transformed random process $\nrm'(z) \equiv \int_{\Scal(z)} \bar{\nrm}(y) \mathrm{d} y$ is neccessarily Dirichlet-Multinomial with parameterizing function $\alpha'(z) \equiv \int_{\Scal(z)} \alpha(y) \mathrm{d} y$.

PGR: tilde not prime?

\subsection{Proof that $\sum_{n=1}^N \delta\big( y-\Drm_n \big)$ is a DMP}

Next, it is demonstrated that the random process $\bar{\nrm}(y) \equiv \bar{N}(y;\Drm) = \sum_{n=1}^N \delta\big( y-\Drm_n \big)$ is a DMP, given that $\prm_{\Drm|\uptheta}(D|\theta) = \prod_{n=1}^N \theta\big( D_n \big)$ and $\uptheta \sim \DP(\alpha)$. 

Observe that $\nrm'(z) \equiv \sum_{n=1}^N \chi\big( \Drm_n;\Scal(z) \big)$, where $\chi$ is the indicator function
\begin{equation}
\chi(x;S) = \begin{cases} 1 & \mathrm{if} \ x \in S, \\ 0 & \mathrm{if} \ x \notin S.  \end{cases}
\end{equation}
and note that $\Prm\Big( \chi\big( \Drm_n;\Scal(z) \big) = 1 \big| \uptheta \Big) = \int_{\Scal(z)} \uptheta(y) \mathrm{d} y$. As such, $\nrm'$ conditioned on the model $\uptheta$ is characterized by a multinomial distribution 
\begin{equation}
\Prm_{\nrm' | \uptheta}(n' | \theta) = \Mcal(n') \prod_{z \in \Zcal} \left( \int_{\Scal(z)} \theta(y) \mathrm{d} y \right)^{n'(z)} = \Multi\left( n' ; N,\theta'(z) \right) \;,
\end{equation}
where $\uptheta'(z) \equiv \int_{\Scal(z)} \uptheta(y) \mathrm{d} y$, $z \in \Zcal$.

By the aggregation property of the Dirichlet process $\uptheta$, the parameters of this multinomial distribution are characterized as $\uptheta' \sim \Dir\left( \alpha' \right)$, and thus $\tilde{\nrm}$ is drawn from a Dirichlet-Multinomial PMF with the same parameters $\alpha'$. As this holds for any partition of $\Ycal$, $\bar{\nrm}$ is a Dirichlet-Multinomial Process.


\subsection{Mean and Correlation Functions}

In this subsection the mean and correlation functions of a DMP are expressed. The mean function is
\begin{IEEEeqnarray}{rCl}
\mu_{\bar{\nrm}}(y) & = & \sum_{n=1}^N \Erm_{\Drm_n}\Big[\delta\big( y-\Drm_n \big) \Big] \\
& = & \sum_{n=1}^N \Prm_{\Drm_n}(y) \nonumber \\
& = & N \frac{\alpha(y)}{\alpha_0} \nonumber \;.
\end{IEEEeqnarray}

The correlation function is
\begin{IEEEeqnarray}{rCl}
\Erm_{\nbarrm}\big[ \bar{\nrm}(y) \bar{\nrm}(y') \big] & = & \sum_{n=1}^N \Erm_{\Drm_n}\Big[\delta\big( y-\Drm_n \big) \Big] \\
& = & \sum_{n=1}^N \sum_{n'=1}^N \Erm_{\Drm_n,\Drm_{n'}}\Big[ \delta\big( y-\Drm_n \big) \delta\big( y-\Drm_{n'} \big) \Big] \nonumber \\
& = & \sum_n \Erm_{\Drm_n}\Big[ \delta\big( y-\Drm_n \big) \delta\big( y'-\Drm_n \big) \Big] + \nonumber \\
&& \quad \sum_{n \neq n'} \Erm_{\Drm_n,\Drm_{n'}}\Big[ \delta\big( y-\Drm_n \big) \delta\big( y'-\Drm_{n'} \big) \Big] \nonumber \\
& = & \sum_n \int_{\Ycal} \frac{\alpha(\tilde{y})}{\alpha_0} \delta(y-\tilde{y}) \delta(y'-\tilde{y}) + \nonumber \\
&& \quad \sum_{n \neq n'} \int_{\Ycal} \int_{\Ycal} \frac{\alpha(\tilde{y}) \alpha(\tilde{y}') + \alpha(\tilde{y}) \delta(\tilde{y}-\tilde{y}')}{\alpha_0 (\alpha_0+1)} \delta(y-\tilde{y}) \delta(y-\tilde{y}') \nonumber \\
& = & N \frac{\alpha(y)}{\alpha_0} \delta(y-y') + N(N-1) \frac{\alpha(y) \alpha(y') + \alpha(y) \delta(y-y')}{\alpha_0 (\alpha_0+1)} \nonumber \\
& = & \frac{N}{\alpha_0 (\alpha_0+1)} \big[ (N-1)\alpha(y) \alpha(y') + (\alpha_0+N) \alpha(y) \delta(y-y') \big] \nonumber \;.
\end{IEEEeqnarray}



\subsection{Continuous aggregation}

If $\bar{\nrm}$ is a Dirichlet-Multinomial process over a Euclidean space $\Ycal$, then conditioning on its discrete aggregation $\nrm'$ produces independent Dirichlet-Multinomial processes $\big\{ \bar{\nrm}(y) : y \in \Scal(z) \big\} \sim \DMP\Big( \nrm'(z),\big\{ \alpha(y) : y \in \Scal(z) \big\} \Big)$ over the partition spaces $\Scal(z)$.

The previous result can be extended to conditioning on a continuous aggregation. Define $\bar{\nrm} \sim \DMP(N,\alpha)$ over the set $\Ycal \times \Xcal$ and the aggregation DMP $\nrm' = \int_{\Ycal} \bar{\nrm}(y,\cdot) \mathrm{d}y$ over set $\Xcal$ with parameterizing function $\alpha' = \int_{\Ycal} \alpha(y,\cdot) \mathrm{d}y$.

Use the aggregation propery to introduce a Dirichlet-Multinomial process $\tilde{\nrm}(y;k) = \int_{\Delta k}^{\Delta (k+1)} \bar{\nrm}(y,x) \mathrm{d}x$ with parameter $\tilde{\alpha}(y;k) = \int_{\Delta k}^{\Delta (k+1)} \alpha(y,x) \mathrm{d}x$. Additionally, introduce its own aggregation, a Dirichlet-Multinomial random process $\dot{n}(k) = \int_{\Ycal} \tilde{n}(y,k) \mathrm{d}y$ with parameter $\dot{\alpha}(k) = \int_{\Ycal} \tilde{\alpha}(y,k) \mathrm{d}y$. By the conditioning property for discrete aggregations demonstrated previously, $\tilde{\nrm}(\cdot,k) | \dot{\nrm}(k) \sim \DMP\big( \dot{\nrm}(k),\tilde{\alpha}(\cdot,k) \big)$ are independent DMP's.

Note that as $\Delta \to 0$, $\tilde{\nrm}(y,k) \approx \Delta \bar{\nrm}(y,\Delta k)$, $\tilde{\alpha}(y,k) \approx \Delta \alpha(y,\Delta k)$, and $\dot{\nrm}(k) \approx \Delta \nrm'(\Delta k)$. Letting $x \equiv \Delta k$, the statistics of the DMP conditioned on its continuous aggregation can be represented as
\begin{equation}
\Delta \bar{\nrm}(\cdot,x) | \Delta \nrm'(k) \sim \DMP\big( \Delta \nrm'(k), \Delta \alpha(\cdot,x) \big) \;.
\end{equation}







\chapter{}


PGR: Many sections redundant given Dirichlet perspective...

PGR: needs notation/formatting scrub






\section{Maximum \emph{a Posteriori} estimate of $\uptheta$ given $D$} \label{app:MAP_theta}

PGR: delete? been done...

PGR: REDO USING EASY LOG-LIKELIHOOD and LAGRANGE!!!!!!! Generalize for Dirichlet!!!

To determine the MAP estimate of the model PMF $\uptheta$ given the training data $D$, 

\begin{equation}
\hat{\theta}_{MAP}(D) = \argmax_{\theta \in \Theta} \Prm_{\uptheta | \Drm}(\theta | D) \;,
\end{equation}
we perform contrained optimization. Note that the set $\Theta = \left\{ \theta \in {\Rbb_{\geq 0}}^{M}: \sum_{m=1}^{M} \theta_m = 1 \right\}$ implies both equality and inequality constraints...







%\section{The Expected Value of $\bar{\nrm}_{\mathrm{max}}$} \label{app:E_N_max}
%
%\subsection{The CMF of $\bar{\nrm}_{\mathrm{max}}$}
%
%REAL PROOF???
%
%The cummulative mass function for $\bar{\nrm}_{\mathrm{max}} = \max_y \bar{\nrm}(y)$,
%
%\begin{IEEEeqnarray}{rCl}
%F_{\bar{\nrm}_{\mathrm{max}}}(n) & = & \Prm\left( \bar{\nrm}_{\mathrm{max}} \leq n \right) \\
%& = & \binom{N+M-1}{M-1}^{-1} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} \\
%&& \quad \binom{m(n+1)-N-1}{M-1} U\left(n+1-\frac{N+M}{m}\right) \;,
%\end{IEEEeqnarray}
%
%has been found via simulation. Although an exhaustive demonstration of conformity between the provided expression and the numerically determined CMF values has not been performed, the CMF has been confirmed for a variety of values $N$ and $M$.
%
%FIGURES???
%
%
%
%\subsection{In the limit $N \to \infty$}
%
%Having established the CMF for $\bar{\nrm}_{\mathrm{max}}$ and provided a general formula for the expected value in equation ???, we seek a more compact form to avoid the summation over $n=0,\ldots,N$. Although we do not provide the general expression, we do provide a compact formula for the expected value as $N \to \infty$.
%
%First, we note how the CMF of $\bar{\nrm}_{\mathrm{max}}$ simplifies in this limit. Below, observe how the binomial coefficients including $N$ reduce to a power term and that the argument of the step function simplifies, since the function invariant to scaling.
%
%\begin{IEEEeqnarray}{rCl}
%\lim_{N \to \infty} F_{\bar{\nrm}_{\mathrm{max}}}(n) & = & \lim_{N \to \infty} \binom{N+M-1}{M-1}^{-1} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} \\
%&& \quad \binom{m(n+1)-N-1}{M-1} U\left(n+1-\frac{N+M}{m}\right) \\
%& = & \lim_{N \to \infty} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} U\left(\frac{n}{N}+\frac{1}{N}-\frac{1}{m}-\frac{M}{Nm}\right) \\
%&& \quad \prod_{k=1}^M \frac{m(n+1)-N-k}{N+M-k} \\
%& = & \lim_{N \to \infty} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} U\left( \frac{n}{N}-\frac{1}{m} \right) \left( \frac{mn}{N} - 1 \right)^{M-1} \\
%\end{IEEEeqnarray}
%
%Consider the dependency of the above equation on $n$ as well as on the training set size $N$. We define a continuous function over the unit interval,
%
%\begin{equation}
%p(t) = \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} (mt - 1)^{M-1} U\left( t-\frac{1}{m} \right) \;,
%\end{equation}
%
%such that $p(n/N) = \lim_{N \to \infty} F_{\bar{\nrm}_{\mathrm{max}}}(n)$. This will be used in the following, where we use the CMF to determine the first moment. 
%
%THETA PDF???
%
%START from N - int F???
%
%It should be intuitive that as $N$ tends toward infinity, so does $\Erm_{\bar{n}} \left[ \bar{\nrm}_{\mathrm{max}} \right]$. With this in mind, and given the form of equation \eqref{risk_01_opt}, we proceed to determine the ``normalized'' value of the mean,
%
%\begin{IEEEeqnarray}{rCl}
%\lim_{N \to \infty} \frac{\Erm_{\bar{n}} \left[ \bar{\nrm}_{\mathrm{max}} \right]}{N} & = & \lim_{N \to \infty} N^{-1} \sum_{n=0}^N n 
%\left( F_{\bar{\nrm}_{\mathrm{max}}}(n) - F_{\bar{\nrm}_{\mathrm{max}}}(n-1) \right) \\
%& = & \lim_{N^{-1} \to 0} N^{-1} \sum_{n=1}^N \frac{n}{N} \left( \frac{p(n/N) - p(n/N - N^{-1})}{N^{-1}} \right) \\
%& = & \lim_{N^{-1} \to 0} N^{-1} \sum_{n=1}^N \frac{n}{N} \left. \frac{\mathrm{d} p(t)}{\mathrm{d} t} \right|_{t=n/N}  \\
%& \approx & \int_0^1  t \frac{\mathrm{d} p(t)}{\mathrm{d} t} \mathrm{d} t \;,
%\end{IEEEeqnarray}
%
%where we the sum is treated as a Riemann integral approximation. Performing integration by parts, we have,
%
%PGR: riemann reference
%
%\begin{IEEEeqnarray}{rCl}
%\lim_{N \to \infty} \frac{\Erm_{\bar{n}} \left[ \bar{\nrm}_{\mathrm{max}} \right]}{N} & = & p(1) - \int_0^1 p(t) \mathrm{d} t \\
%& = & p(1) - \frac{1}{M} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} m^{-1} (m-1)^M \;.
%\end{IEEEeqnarray}
%
%We evaluate $p(1)$, as,
%
%\begin{IEEEeqnarray}{rCl}
%p(1) & = & \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} (m - 1)^{M-1}  \\
%& = & \sum_{m=0}^M \binom{M}{m} (-1)^{M-m} (m - 1)^{M-1}  - \binom{M}{0} (-1)^M (-1)^{M-1} \\
%& = & 1 \;,
%\end{IEEEeqnarray}
%
%where we have used the identity $\sum_{m=0}^M \binom{M}{m} (-1)^{M-m}  P_{M-1}(m) = 0$, in which $P_{M-1}(m)$ is a polynomial of degree $M-1$ \cite{graham}.
%
%Next, we assess the second term in equation ???. We seek to re-use the previous identity. To this effect, we expand the final term,
%
%\begin{IEEEeqnarray}{rCl}
%m^{-1} (m-1)^M & = & \sum_{k=0}^M \binom{M}{k} m^{k-1} (-1)^{M-k} \\
%& = & (-1)^M m^{-1} + \sum_{k=0}^{M-1} \binom{M}{k+1} m^{k} (-1)^{M-1-k} \\
%\end{IEEEeqnarray}
%
%Substituting into the summation???, we first complete the alternating sum over the second term in the previous equation, another $M-1$ order polynomial. 
%
%\begin{IEEEeqnarray}{C}
%\frac{1}{M} \sum_{m=1}^M \binom{M}{m} (-1)^{M-m} m^{-1} (m-1)^M \\
%= \frac{1}{M} \sum_{m=1}^M \binom{M}{m} (-1)^{m} m^{-1} -  \left. \frac{1}{M} \binom{M}{m} (-1)^{M-m} \sum_{k=0}^{M-1} \binom{M}{k+1} m^{k} (-1)^{M-1-k} \right|_{m=0} \\
%= 1 + \frac{1}{M} \sum_{m=1}^M \binom{M}{m} (-1)^{m} m^{-1} \;.
%\end{IEEEeqnarray}
%
%Finally, we substitute back into equation ??? to find,
%
%\begin{IEEEeqnarray}{rCl}
%\lim_{N \to \infty} \frac{\Erm_{\bar{n}} \left[ \bar{\nrm}_{\mathrm{max}} \right]}{N} & = & - \frac{1}{M} \sum_{m=1}^M \binom{M}{m} (-1)^{m} m^{-1} \\
%& = & \frac{1}{M} \sum_{m=0}^{M-1} \binom{M}{m+1} (-1)^m (m+1)^{-1} \\
%& = & \sum_{m=0}^{M-1} \binom{M-1}{m} (-1)^m (m+1)^{-2} \;.
%\end{IEEEeqnarray}
%
%...??? 
%
%Next we use the finite difference identities \cite{graham},
%
%\begin{equation}
%\Delta^M f(k) = \sum_{m=0}^M (-1)^{M-m} \binom{M}{m} f(k+m) \;,
%\end{equation}
%
%\begin{equation}
%\Delta^M [u(k)v(k)] = \sum_{m=0}^M \binom{M}{m} \Delta^m u(k) \Delta^{M-m} v(k+m) \;,
%\end{equation}
%
%\begin{equation}
%\Delta^m k^{-1} = (-1)^m k^{-1} \binom{k+m}{m}^{-1} \;,
%\end{equation}
%
%to reform the alternating binomial summation,
%
%
%\begin{IEEEeqnarray}{rCl}
%\lim_{N \to \infty} \frac{\Erm_{\bar{n}} \left[ \bar{\nrm}_{\mathrm{max}} \right]}{N} & = & (-1)^{M-1} \left. \Delta^{M-1} u(k)^2 \right|_{k=1} \\
%& = & (-1)^{M-1}\sum_{m=0}^{M-1} \binom{M-1}{m} \left. \Delta^m k^{-1} \Delta^{M-m} (k+m)^{-1} \right|_{k=1} \\
%& = & \sum_{m=0}^{M-1} \binom{M-1}{m} (m+1)^{-1} \binom{M}{m+1}^{-1} (m+1)^{-1} \\
%& = & M^{-1} \sum_{m=0}^{M-1} (m+1)^{-1} \\
%& = & M^{-1} \sum_{m=1}^M m^{-1} \;.
%\end{IEEEeqnarray}
%
%Finally, we have a scaled harmonic summation based on $M$.
%
%
%
%\subsection{Maximum of subset of values of nbar}
%
%PGR: incomplete











%\bibliographystyle{plain}
%\bibliography{{../References/phd_bib}}
\printbibliography



\end{document}


























